{
  "_autogen_note": "This file is automatically generated by the RAID leaderboard submission script. Do not edit this file manually or include it in new submissions' PRs.",
  "_submission_hash": "a5a1dc7f138a0d152ff718ec5dc3976c1e36f4486cf60c43e2f200c4876b1e91",
  "_results_hash": "8c9f19f51cff20e511ccf659d13dea297c5c170acccdfaaa0cc02f512bc97282",
  "date_released": "2024-05-21",
  "detector_name": "Mage-baseline",
  "contact_info": "Email Address: yafuly@gmail.com",
  "website": null,
  "paper_link": "https://arxiv.org/abs/2305.13242",
  "huggingface_link": "https://huggingface.co/yaful/MAGE",
  "github_link": "https://github.com/yafuly/MAGE",
  "additional_metadata": null,
  "score_agg": {
    "all": {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8781846871170343
    },
    "no_adversarial": {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9261008903952206
    }
  },
  "scores": [
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.998909375
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990520833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9989807291666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990947916666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9968145833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9979546875
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990020833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9979333333333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        }
      },
      "auroc": 0.9984677083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991104166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9951822916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9971463541666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.964315625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9915239583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9779197916666669
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9817130208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9933531250000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 734,
          "fn": 66,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9875330729166667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9980166666666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9948854166666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9964510416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9987354166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9887874999999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9937614583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9983760416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9918364583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9951062500000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991895833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990114583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991005208333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9859802083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9917052083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9888427083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9925848958333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9953583333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        }
      },
      "auroc": 0.9939716145833334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9987614583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9773333333333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9880473958333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9002739583333332
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9937791666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.9470265625000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9495177083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.98555625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 731,
          "fn": 69,
          "accuracy": 0.91375
        },
        "0.01": {
          "tp": 657,
          "fn": 143,
          "accuracy": 0.82125
        }
      },
      "auroc": 0.9675369791666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9983927083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.99693125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9976619791666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991625000000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9906
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.99488125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9987776041666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.993765625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9962716145833335
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9740312500000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9740312500000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9639697916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9639697916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9690005208333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9690005208333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.888846875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.888846875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8410145833333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8410145833333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.8649307291666668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.8649307291666668
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9995739583333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9995739583333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990583333333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990583333333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9993161458333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9993161458333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9991874999999999
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9991874999999999
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.7996989583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.7996989583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8994432291666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8994432291666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9766489583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9766489583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.92183125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.92183125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9492401041666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9492401041666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2101,
          "fn": 99,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 2020,
          "fn": 180,
          "accuracy": 0.9181818181818182
        }
      },
      "auroc": 0.9846062499999999
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1177,
          "fn": 23,
          "accuracy": 0.9808333333333333
        },
        "0.01": {
          "tp": 1159,
          "fn": 41,
          "accuracy": 0.9658333333333333
        }
      },
      "auroc": 0.993732638888889
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3278,
          "fn": 122,
          "accuracy": 0.9641176470588235
        },
        "0.01": {
          "tp": 3179,
          "fn": 221,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9878273284313726
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1885,
          "fn": 315,
          "accuracy": 0.8568181818181818
        },
        "0.01": {
          "tp": 1634,
          "fn": 566,
          "accuracy": 0.7427272727272727
        }
      },
      "auroc": 0.9430123106060606
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1169,
          "fn": 31,
          "accuracy": 0.9741666666666666
        },
        "0.01": {
          "tp": 1142,
          "fn": 58,
          "accuracy": 0.9516666666666667
        }
      },
      "auroc": 0.9922017361111111
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3054,
          "fn": 346,
          "accuracy": 0.898235294117647
        },
        "0.01": {
          "tp": 2776,
          "fn": 624,
          "accuracy": 0.8164705882352942
        }
      },
      "auroc": 0.9603732843137254
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3986,
          "fn": 414,
          "accuracy": 0.9059090909090909
        },
        "0.01": {
          "tp": 3654,
          "fn": 746,
          "accuracy": 0.8304545454545454
        }
      },
      "auroc": 0.9638092803030303
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2346,
          "fn": 54,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 2301,
          "fn": 99,
          "accuracy": 0.95875
        }
      },
      "auroc": 0.9929671875
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6332,
          "fn": 468,
          "accuracy": 0.9311764705882353
        },
        "0.01": {
          "tp": 5955,
          "fn": 845,
          "accuracy": 0.8757352941176471
        }
      },
      "auroc": 0.974100306372549
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.998909375
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990520833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9989807291666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990947916666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9968145833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9979546875
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990020833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9979333333333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        }
      },
      "auroc": 0.9984677083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991104166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9951822916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9971463541666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.964315625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9915239583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9779197916666669
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9817130208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9933531250000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 734,
          "fn": 66,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9875330729166667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9980166666666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9948854166666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9964510416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9987354166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9887874999999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9937614583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9983760416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9918364583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9951062500000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991895833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990114583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991005208333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9859802083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9917052083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9888427083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9925848958333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9953583333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        }
      },
      "auroc": 0.9939716145833334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9987614583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9773333333333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9880473958333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9002739583333332
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9937791666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.9470265625000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9495177083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.98555625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 731,
          "fn": 69,
          "accuracy": 0.91375
        },
        "0.01": {
          "tp": 657,
          "fn": 143,
          "accuracy": 0.82125
        }
      },
      "auroc": 0.9675369791666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9983927083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.99693125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9976619791666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991625000000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9906
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.99488125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9987776041666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.993765625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9962716145833335
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9740312500000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9740312500000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9639697916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9639697916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9690005208333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9690005208333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.888846875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.888846875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8410145833333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8410145833333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.8649307291666668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.8649307291666668
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9995739583333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9995739583333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990583333333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990583333333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9993161458333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9993161458333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9991874999999999
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9991874999999999
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.7996989583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.7996989583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8994432291666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8994432291666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9766489583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9766489583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.92183125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.92183125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9492401041666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9492401041666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2101,
          "fn": 99,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 2020,
          "fn": 180,
          "accuracy": 0.9181818181818182
        }
      },
      "auroc": 0.9846062499999999
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1177,
          "fn": 23,
          "accuracy": 0.9808333333333333
        },
        "0.01": {
          "tp": 1159,
          "fn": 41,
          "accuracy": 0.9658333333333333
        }
      },
      "auroc": 0.993732638888889
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3278,
          "fn": 122,
          "accuracy": 0.9641176470588235
        },
        "0.01": {
          "tp": 3179,
          "fn": 221,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9878273284313726
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1885,
          "fn": 315,
          "accuracy": 0.8568181818181818
        },
        "0.01": {
          "tp": 1634,
          "fn": 566,
          "accuracy": 0.7427272727272727
        }
      },
      "auroc": 0.9430123106060606
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1169,
          "fn": 31,
          "accuracy": 0.9741666666666666
        },
        "0.01": {
          "tp": 1142,
          "fn": 58,
          "accuracy": 0.9516666666666667
        }
      },
      "auroc": 0.9922017361111111
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3054,
          "fn": 346,
          "accuracy": 0.898235294117647
        },
        "0.01": {
          "tp": 2776,
          "fn": 624,
          "accuracy": 0.8164705882352942
        }
      },
      "auroc": 0.9603732843137254
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3986,
          "fn": 414,
          "accuracy": 0.9059090909090909
        },
        "0.01": {
          "tp": 3654,
          "fn": 746,
          "accuracy": 0.8304545454545454
        }
      },
      "auroc": 0.9638092803030303
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2346,
          "fn": 54,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 2301,
          "fn": 99,
          "accuracy": 0.95875
        }
      },
      "auroc": 0.9929671875
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6332,
          "fn": 468,
          "accuracy": 0.9311764705882353
        },
        "0.01": {
          "tp": 5955,
          "fn": 845,
          "accuracy": 0.8757352941176471
        }
      },
      "auroc": 0.974100306372549
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.998965625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9882520833333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9936088541666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9966885416666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9723020833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9844953125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9978270833333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9802770833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        },
        "0.01": {
          "tp": 733,
          "fn": 67,
          "accuracy": 0.91625
        }
      },
      "auroc": 0.9890520833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9993937500000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9952020833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9972979166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.9374260416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9916229166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        }
      },
      "auroc": 0.9645244791666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9684098958333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9934125
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 693,
          "fn": 107,
          "accuracy": 0.86625
        }
      },
      "auroc": 0.9809111979166667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9948541666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9885020833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9916781250000002
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9926468749999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9865052083333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9895760416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.9937505208333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9875036458333335
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        },
        "0.01": {
          "tp": 739,
          "fn": 61,
          "accuracy": 0.92375
        }
      },
      "auroc": 0.9906270833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9994083333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9986354166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9990218750000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9592156250000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.99218125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9756984375
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9793119791666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9954083333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 768,
          "fn": 32,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 724,
          "fn": 76,
          "accuracy": 0.905
        }
      },
      "auroc": 0.98736015625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.998484375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9721
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9852921875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.85630625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9930885416666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        }
      },
      "auroc": 0.9246973958333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9273953125000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.9825942708333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 701,
          "fn": 99,
          "accuracy": 0.87625
        },
        "0.01": {
          "tp": 611,
          "fn": 189,
          "accuracy": 0.76375
        }
      },
      "auroc": 0.9549947916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9979364583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9919489583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9949427083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9921197916666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9716302083333335
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.981875
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9950281250000002
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9817895833333332
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 773,
          "fn": 27,
          "accuracy": 0.96625
        },
        "0.01": {
          "tp": 729,
          "fn": 71,
          "accuracy": 0.91125
        }
      },
      "auroc": 0.9884088541666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9690687499999999
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9690687499999999
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9552322916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9552322916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9621505208333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9621505208333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.8503041666666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.8503041666666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.7943645833333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.7943645833333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 131,
          "fn": 269,
          "accuracy": 0.3275
        }
      },
      "auroc": 0.822334375
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 131,
          "fn": 269,
          "accuracy": 0.3275
        }
      },
      "auroc": 0.822334375
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9952583333333332
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9952583333333332
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9820343749999999
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9820343749999999
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9886463541666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9886463541666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9945145833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9945145833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.7071770833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.7071770833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.8508458333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.8508458333333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9580395833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9580395833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.8983739583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.8983739583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9282067708333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9282067708333333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2064,
          "fn": 136,
          "accuracy": 0.9381818181818182
        },
        "0.01": {
          "tp": 1920,
          "fn": 280,
          "accuracy": 0.8727272727272727
        }
      },
      "auroc": 0.9778389204545455
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1167,
          "fn": 33,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 1108,
          "fn": 92,
          "accuracy": 0.9233333333333333
        }
      },
      "auroc": 0.9891067708333334
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3231,
          "fn": 169,
          "accuracy": 0.9502941176470588
        },
        "0.01": {
          "tp": 3028,
          "fn": 372,
          "accuracy": 0.8905882352941177
        }
      },
      "auroc": 0.9818158088235294
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1731,
          "fn": 469,
          "accuracy": 0.7868181818181819
        },
        "0.01": {
          "tp": 1390,
          "fn": 810,
          "accuracy": 0.6318181818181818
        }
      },
      "auroc": 0.9155986742424242
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1143,
          "fn": 57,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 1078,
          "fn": 122,
          "accuracy": 0.8983333333333333
        }
      },
      "auroc": 0.9845550347222223
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2874,
          "fn": 526,
          "accuracy": 0.8452941176470589
        },
        "0.01": {
          "tp": 2468,
          "fn": 932,
          "accuracy": 0.7258823529411764
        }
      },
      "auroc": 0.9399362132352942
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3795,
          "fn": 605,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 3310,
          "fn": 1090,
          "accuracy": 0.7522727272727273
        }
      },
      "auroc": 0.946718797348485
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2310,
          "fn": 90,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 2186,
          "fn": 214,
          "accuracy": 0.9108333333333334
        }
      },
      "auroc": 0.9868309027777777
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 6105,
          "fn": 695,
          "accuracy": 0.8977941176470589
        },
        "0.01": {
          "tp": 5496,
          "fn": 1304,
          "accuracy": 0.808235294117647
        }
      },
      "auroc": 0.9608760110294117
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9994447916666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9973322916666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9983885416666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9991697916666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9919093749999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9955395833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9993072916666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9946208333333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        }
      },
      "auroc": 0.9969640625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991177083333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9949489583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9970333333333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.94846875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9908947916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9696817708333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        }
      },
      "auroc": 0.9737932291666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9929218750000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 763,
          "fn": 37,
          "accuracy": 0.95375
        },
        "0.01": {
          "tp": 710,
          "fn": 90,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9833575520833333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9882447916666668
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.984734375
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9864895833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9875395833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9800354166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9837875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.9878921875000001
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9823848958333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 769,
          "fn": 31,
          "accuracy": 0.96125
        },
        "0.01": {
          "tp": 710,
          "fn": 90,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9851385416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99940625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.998028125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9987171875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9640260416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9903458333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9771859375
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9817161458333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9941869791666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 769,
          "fn": 31,
          "accuracy": 0.96125
        },
        "0.01": {
          "tp": 735,
          "fn": 65,
          "accuracy": 0.91875
        }
      },
      "auroc": 0.9879515625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.99788125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9708145833333335
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9843479166666668
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.863553125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9921677083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        }
      },
      "auroc": 0.9278604166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.9307171875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9814911458333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 713,
          "fn": 87,
          "accuracy": 0.89125
        },
        "0.01": {
          "tp": 625,
          "fn": 175,
          "accuracy": 0.78125
        }
      },
      "auroc": 0.9561041666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9974927083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9954260416666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.996459375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9914729166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9745416666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        }
      },
      "auroc": 0.9830072916666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9944828125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.9849838541666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 737,
          "fn": 63,
          "accuracy": 0.92125
        }
      },
      "auroc": 0.9897333333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9592645833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9592645833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.939671875
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.939671875
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.9494682291666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.9494682291666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.8549927083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.8549927083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.7907447916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.7907447916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 127,
          "fn": 273,
          "accuracy": 0.3175
        }
      },
      "auroc": 0.8228687499999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 127,
          "fn": 273,
          "accuracy": 0.3175
        }
      },
      "auroc": 0.8228687499999999
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.995284375
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.995284375
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9891770833333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9891770833333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9922307291666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9922307291666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9894458333333332
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9894458333333332
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.707190625
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.707190625
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.8483182291666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.8483182291666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9624645833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9624645833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.8909343750000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.8909343750000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9266994791666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9266994791666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2053,
          "fn": 147,
          "accuracy": 0.9331818181818182
        },
        "0.01": {
          "tp": 1899,
          "fn": 301,
          "accuracy": 0.8631818181818182
        }
      },
      "auroc": 0.9766399621212121
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1172,
          "fn": 28,
          "accuracy": 0.9766666666666667
        },
        "0.01": {
          "tp": 1127,
          "fn": 73,
          "accuracy": 0.9391666666666667
        }
      },
      "auroc": 0.9902140625
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3225,
          "fn": 175,
          "accuracy": 0.9485294117647058
        },
        "0.01": {
          "tp": 3026,
          "fn": 374,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9814308210784314
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1747,
          "fn": 453,
          "accuracy": 0.7940909090909091
        },
        "0.01": {
          "tp": 1431,
          "fn": 769,
          "accuracy": 0.6504545454545455
        }
      },
      "auroc": 0.9156317234848484
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1152,
          "fn": 48,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 1097,
          "fn": 103,
          "accuracy": 0.9141666666666667
        }
      },
      "auroc": 0.9866491319444445
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2899,
          "fn": 501,
          "accuracy": 0.8526470588235294
        },
        "0.01": {
          "tp": 2528,
          "fn": 872,
          "accuracy": 0.7435294117647059
        }
      },
      "auroc": 0.9406966911764706
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3800,
          "fn": 600,
          "accuracy": 0.8636363636363636
        },
        "0.01": {
          "tp": 3330,
          "fn": 1070,
          "accuracy": 0.7568181818181818
        }
      },
      "auroc": 0.9461358428030303
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2324,
          "fn": 76,
          "accuracy": 0.9683333333333334
        },
        "0.01": {
          "tp": 2224,
          "fn": 176,
          "accuracy": 0.9266666666666666
        }
      },
      "auroc": 0.9884315972222222
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6124,
          "fn": 676,
          "accuracy": 0.9005882352941177
        },
        "0.01": {
          "tp": 5554,
          "fn": 1246,
          "accuracy": 0.816764705882353
        }
      },
      "auroc": 0.961063756127451
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990822916666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9986833333333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9988828125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9989458333333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9953427083333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9971442708333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9990140625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9970130208333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9980135416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991354166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9952479166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9971916666666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9536874999999999
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9915333333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9726104166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9764114583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9933906250000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 715,
          "fn": 85,
          "accuracy": 0.89375
        }
      },
      "auroc": 0.9849010416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.99764375
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9942875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.995965625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9987229166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9878166666666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9932697916666668
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9981833333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9910520833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": {
          "tp": 768,
          "fn": 32,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9946177083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991916666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990697916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991307291666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.977859375
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9902656249999999
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9840625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9885255208333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9946677083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 779,
          "fn": 21,
          "accuracy": 0.97375
        },
        "0.01": {
          "tp": 753,
          "fn": 47,
          "accuracy": 0.94125
        }
      },
      "auroc": 0.9915966145833334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9987166666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.971590625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9851536458333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.887809375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9938343749999999
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        }
      },
      "auroc": 0.9408218750000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.9432630208333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9827125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 722,
          "fn": 78,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 640,
          "fn": 160,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9629877604166668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9983958333333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.995553125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9969744791666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9987197916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9892052083333335
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9939625000000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9985578125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9923791666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9954684895833334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9724395833333335
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9724395833333335
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9597354166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9597354166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9660875
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9660875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.88235625
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.88235625
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.8208447916666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.8208447916666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        }
      },
      "auroc": 0.8516005208333335
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        }
      },
      "auroc": 0.8516005208333335
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9995270833333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9995270833333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.99806875
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.99806875
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9987979166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9987979166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9992291666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9992291666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.7731552083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.7731552083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8861921875000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8861921875000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9724895833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9724895833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.90624375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.90624375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.9393666666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.9393666666666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2094,
          "fn": 106,
          "accuracy": 0.9518181818181818
        },
        "0.01": {
          "tp": 2003,
          "fn": 197,
          "accuracy": 0.9104545454545454
        }
      },
      "auroc": 0.9834733901515151
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1174,
          "fn": 26,
          "accuracy": 0.9783333333333334
        },
        "0.01": {
          "tp": 1148,
          "fn": 52,
          "accuracy": 0.9566666666666667
        }
      },
      "auroc": 0.9924053819444444
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3268,
          "fn": 132,
          "accuracy": 0.9611764705882353
        },
        "0.01": {
          "tp": 3151,
          "fn": 249,
          "accuracy": 0.9267647058823529
        }
      },
      "auroc": 0.9866258578431373
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1828,
          "fn": 372,
          "accuracy": 0.8309090909090909
        },
        "0.01": {
          "tp": 1561,
          "fn": 639,
          "accuracy": 0.7095454545454546
        }
      },
      "auroc": 0.9339811553030304
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1164,
          "fn": 36,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 1131,
          "fn": 69,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9913329861111111
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2992,
          "fn": 408,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 2692,
          "fn": 708,
          "accuracy": 0.7917647058823529
        }
      },
      "auroc": 0.9542229779411765
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3922,
          "fn": 478,
          "accuracy": 0.8913636363636364
        },
        "0.01": {
          "tp": 3564,
          "fn": 836,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9587272727272727
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2338,
          "fn": 62,
          "accuracy": 0.9741666666666666
        },
        "0.01": {
          "tp": 2279,
          "fn": 121,
          "accuracy": 0.9495833333333333
        }
      },
      "auroc": 0.9918691840277778
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6260,
          "fn": 540,
          "accuracy": 0.9205882352941176
        },
        "0.01": {
          "tp": 5843,
          "fn": 957,
          "accuracy": 0.8592647058823529
        }
      },
      "auroc": 0.9704244178921568
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9988322916666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.992665625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9957489583333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9985197916666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9803656250000001
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.9894427083333335
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9986760416666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.9865156250000001
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9925958333333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9981104166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9407864583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9694484375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.961853125
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8861947916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        }
      },
      "auroc": 0.9240239583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9799817708333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        },
        "0.01": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        }
      },
      "auroc": 0.9134906249999999
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 674,
          "fn": 126,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 572,
          "fn": 228,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9467361979166667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9790260416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9444125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9617192708333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9861208333333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9210729166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.953596875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        }
      },
      "auroc": 0.9825734375
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        }
      },
      "auroc": 0.9327427083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 702,
          "fn": 98,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 592,
          "fn": 208,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9576580729166666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.992921875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9944739583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9936979166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.961425
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9146364583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9380307291666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9771734375000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9545552083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 723,
          "fn": 77,
          "accuracy": 0.90375
        },
        "0.01": {
          "tp": 631,
          "fn": 169,
          "accuracy": 0.78875
        }
      },
      "auroc": 0.9658643229166668
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9936208333333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.909234375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9514276041666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.9065135416666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.76898125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        }
      },
      "auroc": 0.8377473958333332
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9500671875000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.8391078125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 566,
          "fn": 234,
          "accuracy": 0.7075
        },
        "0.01": {
          "tp": 459,
          "fn": 341,
          "accuracy": 0.57375
        }
      },
      "auroc": 0.8945875
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9968166666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9842739583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9905453125000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9926416666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9604531249999999
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9765473958333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9947291666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        }
      },
      "auroc": 0.9723635416666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        },
        "0.01": {
          "tp": 708,
          "fn": 92,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9835463541666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.954725
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.954725
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9526708333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9526708333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.9536979166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.9536979166666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.8951697916666668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.8951697916666668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.8840791666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.8840791666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8896244791666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8896244791666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9930427083333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9930427083333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.991378125
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.991378125
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9922104166666668
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9922104166666668
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9884760416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9884760416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9369354166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9369354166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9627057291666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9627057291666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9694625000000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9694625000000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9373489583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9373489583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9534057291666668
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9534057291666668
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2047,
          "fn": 153,
          "accuracy": 0.9304545454545454
        },
        "0.01": {
          "tp": 1859,
          "fn": 341,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9782003787878788
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1055,
          "fn": 145,
          "accuracy": 0.8791666666666667
        },
        "0.01": {
          "tp": 935,
          "fn": 265,
          "accuracy": 0.7791666666666667
        }
      },
      "auroc": 0.9609744791666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3102,
          "fn": 298,
          "accuracy": 0.9123529411764706
        },
        "0.01": {
          "tp": 2794,
          "fn": 606,
          "accuracy": 0.821764705882353
        }
      },
      "auroc": 0.9721206495098039
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1898,
          "fn": 302,
          "accuracy": 0.8627272727272727
        },
        "0.01": {
          "tp": 1607,
          "fn": 593,
          "accuracy": 0.7304545454545455
        }
      },
      "auroc": 0.9554078598484849
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 911,
          "fn": 289,
          "accuracy": 0.7591666666666667
        },
        "0.01": {
          "tp": 717,
          "fn": 483,
          "accuracy": 0.5975
        }
      },
      "auroc": 0.9052840277777778
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2809,
          "fn": 591,
          "accuracy": 0.8261764705882353
        },
        "0.01": {
          "tp": 2324,
          "fn": 1076,
          "accuracy": 0.6835294117647058
        }
      },
      "auroc": 0.9377170955882352
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3945,
          "fn": 455,
          "accuracy": 0.8965909090909091
        },
        "0.01": {
          "tp": 3466,
          "fn": 934,
          "accuracy": 0.7877272727272727
        }
      },
      "auroc": 0.9668041193181818
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1966,
          "fn": 434,
          "accuracy": 0.8191666666666667
        },
        "0.01": {
          "tp": 1652,
          "fn": 748,
          "accuracy": 0.6883333333333334
        }
      },
      "auroc": 0.9331292534722222
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 5911,
          "fn": 889,
          "accuracy": 0.8692647058823529
        },
        "0.01": {
          "tp": 5118,
          "fn": 1682,
          "accuracy": 0.7526470588235294
        }
      },
      "auroc": 0.9549188725490195
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9989239583333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990302083333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9989770833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990895833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9967645833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9979270833333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990067708333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9978973958333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        }
      },
      "auroc": 0.9984520833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991041666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9951854166666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9971447916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.964134375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.991234375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9776843749999999
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9816192708333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9932098958333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        },
        "0.01": {
          "tp": 732,
          "fn": 68,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9874145833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9980145833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9948864583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9964505208333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.99876875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9887927083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9937807291666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9983916666666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9918395833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.995115625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9992031250000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9992625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9992328125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9855
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.991671875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9885859375
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9923515625000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9954671875000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": {
          "tp": 760,
          "fn": 40,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9939093749999999
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.99865625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9769739583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9878151041666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.898340625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9939510416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        }
      },
      "auroc": 0.9461458333333335
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9484984375000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9854625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 731,
          "fn": 69,
          "accuracy": 0.91375
        },
        "0.01": {
          "tp": 655,
          "fn": 145,
          "accuracy": 0.81875
        }
      },
      "auroc": 0.9669804687500001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9984000000000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9969333333333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9976666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.99910625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9905958333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9948510416666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.998753125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9937645833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9962588541666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9736854166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9736854166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9634625000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9634625000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9685739583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9685739583333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.8874124999999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.8874124999999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.8385979166666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.8385979166666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8630052083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8630052083333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9995645833333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9995645833333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.999053125
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.999053125
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9993088541666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9993088541666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9991895833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9991895833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.798746875
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.798746875
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8989682291666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8989682291666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9752739583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9752739583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.919675
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.919675
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9474744791666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9474744791666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2099,
          "fn": 101,
          "accuracy": 0.9540909090909091
        },
        "0.01": {
          "tp": 2016,
          "fn": 184,
          "accuracy": 0.9163636363636364
        }
      },
      "auroc": 0.9843116477272728
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1178,
          "fn": 22,
          "accuracy": 0.9816666666666667
        },
        "0.01": {
          "tp": 1159,
          "fn": 41,
          "accuracy": 0.9658333333333333
        }
      },
      "auroc": 0.9937119791666666
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3277,
          "fn": 123,
          "accuracy": 0.9638235294117647
        },
        "0.01": {
          "tp": 3175,
          "fn": 225,
          "accuracy": 0.9338235294117647
        }
      },
      "auroc": 0.9876294117647058
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1882,
          "fn": 318,
          "accuracy": 0.8554545454545455
        },
        "0.01": {
          "tp": 1626,
          "fn": 574,
          "accuracy": 0.7390909090909091
        }
      },
      "auroc": 0.942225
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1168,
          "fn": 32,
          "accuracy": 0.9733333333333334
        },
        "0.01": {
          "tp": 1141,
          "fn": 59,
          "accuracy": 0.9508333333333333
        }
      },
      "auroc": 0.9921684027777778
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3050,
          "fn": 350,
          "accuracy": 0.8970588235294118
        },
        "0.01": {
          "tp": 2767,
          "fn": 633,
          "accuracy": 0.8138235294117647
        }
      },
      "auroc": 0.9598520833333333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3981,
          "fn": 419,
          "accuracy": 0.9047727272727273
        },
        "0.01": {
          "tp": 3642,
          "fn": 758,
          "accuracy": 0.8277272727272728
        }
      },
      "auroc": 0.9632683238636364
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2346,
          "fn": 54,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 2300,
          "fn": 100,
          "accuracy": 0.9583333333333334
        }
      },
      "auroc": 0.9929401909722222
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6327,
          "fn": 473,
          "accuracy": 0.9304411764705882
        },
        "0.01": {
          "tp": 5942,
          "fn": 858,
          "accuracy": 0.8738235294117647
        }
      },
      "auroc": 0.9737407475490196
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.998909375
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990520833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9989807291666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990947916666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9968145833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9979546875
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990020833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9979333333333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        }
      },
      "auroc": 0.9984677083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991104166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9951947916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9971526041666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9643197916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9916
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9779598958333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9817151041666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9933973958333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 734,
          "fn": 66,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9875562499999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9980166666666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9948854166666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9964510416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9987354166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9887874999999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9937614583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9983760416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9918364583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9951062500000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991895833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990114583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991005208333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9859802083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.99170625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9888432291666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9925848958333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9953588541666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        }
      },
      "auroc": 0.9939718750000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9987614583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9773333333333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9880473958333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9002739583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9937802083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.9470270833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9495177083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9855567708333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 731,
          "fn": 69,
          "accuracy": 0.91375
        },
        "0.01": {
          "tp": 657,
          "fn": 143,
          "accuracy": 0.82125
        }
      },
      "auroc": 0.9675372395833335
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9983927083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.99693125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9976619791666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991625000000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9906
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.99488125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9987776041666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.993765625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9962716145833335
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9740312500000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9740312500000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9639697916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9639697916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9690005208333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9690005208333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.888846875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.888846875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8410145833333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8410145833333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.8649307291666668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.8649307291666668
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9995739583333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9995739583333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990583333333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990583333333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9993161458333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9993161458333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9991874999999999
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9991874999999999
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.7996989583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.7996989583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8994432291666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8994432291666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9766489583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9766489583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.92183125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.92183125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9492401041666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9492401041666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2101,
          "fn": 99,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 2020,
          "fn": 180,
          "accuracy": 0.9181818181818182
        }
      },
      "auroc": 0.9846062499999999
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1177,
          "fn": 23,
          "accuracy": 0.9808333333333333
        },
        "0.01": {
          "tp": 1159,
          "fn": 41,
          "accuracy": 0.9658333333333333
        }
      },
      "auroc": 0.9937347222222223
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3278,
          "fn": 122,
          "accuracy": 0.9641176470588235
        },
        "0.01": {
          "tp": 3179,
          "fn": 221,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9878280637254901
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1885,
          "fn": 315,
          "accuracy": 0.8568181818181818
        },
        "0.01": {
          "tp": 1634,
          "fn": 566,
          "accuracy": 0.7427272727272727
        }
      },
      "auroc": 0.9430126893939393
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1169,
          "fn": 31,
          "accuracy": 0.9741666666666666
        },
        "0.01": {
          "tp": 1142,
          "fn": 58,
          "accuracy": 0.9516666666666667
        }
      },
      "auroc": 0.9922147569444445
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3054,
          "fn": 346,
          "accuracy": 0.898235294117647
        },
        "0.01": {
          "tp": 2776,
          "fn": 624,
          "accuracy": 0.8164705882352942
        }
      },
      "auroc": 0.9603781250000001
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3986,
          "fn": 414,
          "accuracy": 0.9059090909090909
        },
        "0.01": {
          "tp": 3654,
          "fn": 746,
          "accuracy": 0.8304545454545454
        }
      },
      "auroc": 0.9638094696969697
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2346,
          "fn": 54,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 2301,
          "fn": 99,
          "accuracy": 0.95875
        }
      },
      "auroc": 0.9929747395833333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6332,
          "fn": 468,
          "accuracy": 0.9311764705882353
        },
        "0.01": {
          "tp": 5955,
          "fn": 845,
          "accuracy": 0.8757352941176471
        }
      },
      "auroc": 0.974103094362745
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.7792489583333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5254218749999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 342,
          "accuracy": 0.145
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.6523354166666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.7550572916666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.42027083333333337
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.5876640625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        },
        "0.01": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        }
      },
      "auroc": 0.767153125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.47284635416666665
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 692,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 10,
          "fn": 790,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.6199997395833333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9955614583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9921385416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.99385
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.682575
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9818458333333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        }
      },
      "auroc": 0.8322104166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.8390682291666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9869921875000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 606,
          "fn": 194,
          "accuracy": 0.7575
        },
        "0.01": {
          "tp": 568,
          "fn": 232,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9130302083333335
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.7187677083333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.4185895833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        },
        "0.01": {
          "tp": 21,
          "fn": 379,
          "accuracy": 0.0525
        }
      },
      "auroc": 0.5686786458333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.6769520833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.5388416666666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        },
        "0.01": {
          "tp": 40,
          "fn": 360,
          "accuracy": 0.1
        }
      },
      "auroc": 0.607896875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        },
        "0.01": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        }
      },
      "auroc": 0.6978598958333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        },
        "0.01": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.478715625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 675,
          "accuracy": 0.15625
        },
        "0.01": {
          "tp": 61,
          "fn": 739,
          "accuracy": 0.07625
        }
      },
      "auroc": 0.5882877604166667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9993479166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.8491041666666668
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        }
      },
      "auroc": 0.9242260416666668
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.719565625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.5144333333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 358,
          "accuracy": 0.105
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6169994791666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8594567708333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.68176875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 465,
          "accuracy": 0.41875
        },
        "0.01": {
          "tp": 227,
          "fn": 573,
          "accuracy": 0.28375
        }
      },
      "auroc": 0.7706127604166666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9853666666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.5218239583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.7535953125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6056010416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.6135666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 324,
          "accuracy": 0.19
        },
        "0.01": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        }
      },
      "auroc": 0.6095838541666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        },
        "0.01": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        }
      },
      "auroc": 0.7954838541666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        },
        "0.01": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.5676953125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 528,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 217,
          "fn": 583,
          "accuracy": 0.27125
        }
      },
      "auroc": 0.6815895833333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.8202666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4579302083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 312,
          "accuracy": 0.22
        },
        "0.01": {
          "tp": 31,
          "fn": 369,
          "accuracy": 0.0775
        }
      },
      "auroc": 0.6390984375000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.6339197916666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2720197916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 377,
          "accuracy": 0.0575
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.45296979166666673
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 297,
          "accuracy": 0.2575
        },
        "0.01": {
          "tp": 37,
          "fn": 363,
          "accuracy": 0.0925
        }
      },
      "auroc": 0.7270932291666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.364975
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 689,
          "accuracy": 0.13875
        },
        "0.01": {
          "tp": 37,
          "fn": 763,
          "accuracy": 0.04625
        }
      },
      "auroc": 0.5460341145833333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.8862614583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.8862614583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.8137520833333332
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.8137520833333332
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 73,
          "fn": 327,
          "accuracy": 0.1825
        }
      },
      "auroc": 0.8500067708333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 73,
          "fn": 327,
          "accuracy": 0.1825
        }
      },
      "auroc": 0.8500067708333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.6453239583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.6453239583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.620590625
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.620590625
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        },
        "0.01": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        }
      },
      "auroc": 0.6329572916666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        },
        "0.01": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        }
      },
      "auroc": 0.6329572916666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.7129947916666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.7129947916666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.6170927083333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.6170927083333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        },
        "0.01": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        }
      },
      "auroc": 0.66504375
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        },
        "0.01": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        }
      },
      "auroc": 0.66504375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6493895833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6493895833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.34664062500000004
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.34664062500000004
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.49801510416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.49801510416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.7425645833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.7425645833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.6697510416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.6697510416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        },
        "0.01": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        }
      },
      "auroc": 0.7061578125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        },
        "0.01": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        }
      },
      "auroc": 0.7061578125
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1013,
          "fn": 1187,
          "accuracy": 0.46045454545454545
        },
        "0.01": {
          "tp": 695,
          "fn": 1505,
          "accuracy": 0.3159090909090909
        }
      },
      "auroc": 0.8122812500000001
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 883,
          "accuracy": 0.26416666666666666
        },
        "0.01": {
          "tp": 233,
          "fn": 967,
          "accuracy": 0.19416666666666665
        }
      },
      "auroc": 0.627501388888889
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1330,
          "fn": 2070,
          "accuracy": 0.3911764705882353
        },
        "0.01": {
          "tp": 928,
          "fn": 2472,
          "accuracy": 0.27294117647058824
        }
      },
      "auroc": 0.7470648284313726
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 1877,
          "accuracy": 0.14681818181818182
        },
        "0.01": {
          "tp": 59,
          "fn": 2141,
          "accuracy": 0.026818181818181817
        }
      },
      "auroc": 0.6492270833333333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 889,
          "accuracy": 0.25916666666666666
        },
        "0.01": {
          "tp": 251,
          "fn": 949,
          "accuracy": 0.20916666666666667
        }
      },
      "auroc": 0.5568296875
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 634,
          "fn": 2766,
          "accuracy": 0.1864705882352941
        },
        "0.01": {
          "tp": 310,
          "fn": 3090,
          "accuracy": 0.09117647058823529
        }
      },
      "auroc": 0.6166162377450981
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1336,
          "fn": 3064,
          "accuracy": 0.30363636363636365
        },
        "0.01": {
          "tp": 754,
          "fn": 3646,
          "accuracy": 0.17136363636363636
        }
      },
      "auroc": 0.7307541666666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 628,
          "fn": 1772,
          "accuracy": 0.26166666666666666
        },
        "0.01": {
          "tp": 484,
          "fn": 1916,
          "accuracy": 0.20166666666666666
        }
      },
      "auroc": 0.5921655381944444
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1964,
          "fn": 4836,
          "accuracy": 0.2888235294117647
        },
        "0.01": {
          "tp": 1238,
          "fn": 5562,
          "accuracy": 0.18205882352941177
        }
      },
      "auroc": 0.6818405330882353
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9970458333333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9922083333333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9946270833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9954333333333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9766510416666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9860421875000001
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9962395833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9844296875
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 746,
          "fn": 54,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.9903346354166667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9994072916666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9951864583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.997296875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9156854166666668
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9915239583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.9536046875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.9575463541666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9933552083333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 742,
          "fn": 58,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 690,
          "fn": 110,
          "accuracy": 0.8625
        }
      },
      "auroc": 0.9754507812500001
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9938677083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9939833333333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.9939255208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9948010416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9883958333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9915984375000001
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.994334375
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9911895833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 782,
          "fn": 18,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 742,
          "fn": 58,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9927619791666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9993333333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.997975
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9986541666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.934878125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.991840625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9633593750000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9671057291666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9949078125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 755,
          "fn": 45,
          "accuracy": 0.94375
        },
        "0.01": {
          "tp": 711,
          "fn": 89,
          "accuracy": 0.88875
        }
      },
      "auroc": 0.9810067708333332
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9965802083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.96735625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.9819682291666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.801159375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9946041666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.8978817708333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        },
        "0.01": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8988697916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9809802083333332
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 679,
          "fn": 121,
          "accuracy": 0.84875
        },
        "0.01": {
          "tp": 618,
          "fn": 182,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.939925
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9962916666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9941114583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9952015625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9915958333333335
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9828874999999999
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9872416666666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9939437500000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9884994791666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 779,
          "fn": 21,
          "accuracy": 0.97375
        },
        "0.01": {
          "tp": 751,
          "fn": 49,
          "accuracy": 0.93875
        }
      },
      "auroc": 0.9912216145833334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.939059375
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.939059375
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.916590625
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.916590625
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        }
      },
      "auroc": 0.927825
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        }
      },
      "auroc": 0.927825
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.7644947916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.7644947916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.6858989583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.6858989583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        }
      },
      "auroc": 0.7251968750000001
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        }
      },
      "auroc": 0.7251968750000001
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9962354166666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9962354166666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9874437500000001
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9874437500000001
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9918395833333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9918395833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9941510416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9941510416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.7099624999999999
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.7099624999999999
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        }
      },
      "auroc": 0.8520567708333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        }
      },
      "auroc": 0.8520567708333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9198125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9198125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.848540625
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.848540625
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8841765625
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8841765625
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1981,
          "fn": 219,
          "accuracy": 0.9004545454545455
        },
        "0.01": {
          "tp": 1845,
          "fn": 355,
          "accuracy": 0.8386363636363636
        }
      },
      "auroc": 0.9632981060606062
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1163,
          "fn": 37,
          "accuracy": 0.9691666666666666
        },
        "0.01": {
          "tp": 1125,
          "fn": 75,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9901368055555556
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3144,
          "fn": 256,
          "accuracy": 0.9247058823529412
        },
        "0.01": {
          "tp": 2970,
          "fn": 430,
          "accuracy": 0.8735294117647059
        }
      },
      "auroc": 0.9727705882352942
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1614,
          "fn": 586,
          "accuracy": 0.7336363636363636
        },
        "0.01": {
          "tp": 1306,
          "fn": 894,
          "accuracy": 0.5936363636363636
        }
      },
      "auroc": 0.8892717803030302
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1152,
          "fn": 48,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 1116,
          "fn": 84,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9876505208333333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2766,
          "fn": 634,
          "accuracy": 0.8135294117647058
        },
        "0.01": {
          "tp": 2422,
          "fn": 978,
          "accuracy": 0.7123529411764706
        }
      },
      "auroc": 0.9239936887254903
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3595,
          "fn": 805,
          "accuracy": 0.8170454545454545
        },
        "0.01": {
          "tp": 3151,
          "fn": 1249,
          "accuracy": 0.7161363636363637
        }
      },
      "auroc": 0.9262849431818181
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2315,
          "fn": 85,
          "accuracy": 0.9645833333333333
        },
        "0.01": {
          "tp": 2241,
          "fn": 159,
          "accuracy": 0.93375
        }
      },
      "auroc": 0.9888936631944445
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5910,
          "fn": 890,
          "accuracy": 0.8691176470588236
        },
        "0.01": {
          "tp": 5392,
          "fn": 1408,
          "accuracy": 0.7929411764705883
        }
      },
      "auroc": 0.9483821384803921
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9989802083333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9986145833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9987973958333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991791666666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9950552083333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9971171875
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990796875
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9968348958333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        }
      },
      "auroc": 0.9979572916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991177083333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9951916666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9971546875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9601229166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.991434375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9757786458333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9796203124999999
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9933130208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 772,
          "fn": 28,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 730,
          "fn": 70,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.9864666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.998096875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9948979166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9964973958333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9986385416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.988515625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9935770833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9983677083333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9917067708333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        },
        "0.01": {
          "tp": 773,
          "fn": 27,
          "accuracy": 0.96625
        }
      },
      "auroc": 0.9950372395833333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991885416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.999034375
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991114583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9839156250000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9916135416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9877645833333335
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.9915520833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9953239583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        },
        "0.01": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9934380208333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9987135416666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9761249999999999
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9874192708333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.8938510416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9943989583333332
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9441250000000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.9462822916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9852619791666668
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 725,
          "fn": 75,
          "accuracy": 0.90625
        },
        "0.01": {
          "tp": 649,
          "fn": 151,
          "accuracy": 0.81125
        }
      },
      "auroc": 0.9657721354166666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9983833333333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9965760416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9974796874999999
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990666666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9875645833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9933156249999999
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.998725
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9920703125000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        },
        "0.01": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.99539765625
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9723947916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9723947916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.96340625
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.96340625
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.9679005208333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.9679005208333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.8831416666666665
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.8831416666666665
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.834453125
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.834453125
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        }
      },
      "auroc": 0.8587973958333335
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        }
      },
      "auroc": 0.8587973958333335
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9995854166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9995854166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9987895833333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9987895833333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9991875
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9991875
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9988677083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9988677083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.7789552083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.7789552083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        }
      },
      "auroc": 0.8889114583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        }
      },
      "auroc": 0.8889114583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9742322916666668
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9742322916666668
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9153145833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9153145833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9447734374999999
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9447734374999999
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2094,
          "fn": 106,
          "accuracy": 0.9518181818181818
        },
        "0.01": {
          "tp": 2014,
          "fn": 186,
          "accuracy": 0.9154545454545454
        }
      },
      "auroc": 0.9837001893939394
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1177,
          "fn": 23,
          "accuracy": 0.9808333333333333
        },
        "0.01": {
          "tp": 1150,
          "fn": 50,
          "accuracy": 0.9583333333333334
        }
      },
      "auroc": 0.9934065972222222
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3271,
          "fn": 129,
          "accuracy": 0.9620588235294117
        },
        "0.01": {
          "tp": 3164,
          "fn": 236,
          "accuracy": 0.9305882352941176
        }
      },
      "auroc": 0.9871259803921568
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1857,
          "fn": 343,
          "accuracy": 0.8440909090909091
        },
        "0.01": {
          "tp": 1607,
          "fn": 593,
          "accuracy": 0.7304545454545455
        }
      },
      "auroc": 0.9386993371212122
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1167,
          "fn": 33,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 1136,
          "fn": 64,
          "accuracy": 0.9466666666666667
        }
      },
      "auroc": 0.9914303819444444
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3024,
          "fn": 376,
          "accuracy": 0.8894117647058823
        },
        "0.01": {
          "tp": 2743,
          "fn": 657,
          "accuracy": 0.8067647058823529
        }
      },
      "auroc": 0.957310294117647
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3951,
          "fn": 449,
          "accuracy": 0.8979545454545454
        },
        "0.01": {
          "tp": 3621,
          "fn": 779,
          "accuracy": 0.8229545454545455
        }
      },
      "auroc": 0.9611997632575758
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2344,
          "fn": 56,
          "accuracy": 0.9766666666666667
        },
        "0.01": {
          "tp": 2286,
          "fn": 114,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9924184895833333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6295,
          "fn": 505,
          "accuracy": 0.9257352941176471
        },
        "0.01": {
          "tp": 5907,
          "fn": 893,
          "accuracy": 0.8686764705882353
        }
      },
      "auroc": 0.972218137254902
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8996645833333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.903640625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9016526041666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9022770833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8956552083333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8989661458333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9009708333333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8996479166666668
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 468,
          "fn": 332,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9003093750000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9050208333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9016781250000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.9033494791666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.89350625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9120197916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9027630208333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.8992635416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9068489583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 487,
          "fn": 313,
          "accuracy": 0.60875
        },
        "0.01": {
          "tp": 5,
          "fn": 795,
          "accuracy": 0.00625
        }
      },
      "auroc": 0.90305625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8896229166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.881275
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8854489583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.888428125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8918239583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8901260416666668
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8890255208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8865494791666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 442,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8877875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.930190625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.8876020833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        }
      },
      "auroc": 0.9088963541666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9045635416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.8993500000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.9019567708333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9173770833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.8934760416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 490,
          "fn": 310,
          "accuracy": 0.6125
        },
        "0.01": {
          "tp": 8,
          "fn": 792,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9054265625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8914052083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8817062499999999
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8865557291666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.895190625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.8981354166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.8966630208333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8932979166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.8899208333333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 407,
          "fn": 393,
          "accuracy": 0.50875
        },
        "0.01": {
          "tp": 1,
          "fn": 799,
          "accuracy": 0.00125
        }
      },
      "auroc": 0.891609375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9075770833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8864447916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8970109374999999
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.898725
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.8866520833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.8926885416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.9031510416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.8865484374999999
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 419,
          "fn": 381,
          "accuracy": 0.52375
        },
        "0.01": {
          "tp": 2,
          "fn": 798,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.8948497395833332
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.866903125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.866903125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8770572916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8770572916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8719802083333335
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8719802083333335
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8814572916666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8814572916666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8893572916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8893572916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8854072916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8854072916666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8933895833333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8933895833333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8925770833333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8925770833333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8929833333333335
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8929833333333335
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8848750000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8848750000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8846260416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8846260416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8847505208333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8847505208333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9097062499999999
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9097062499999999
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9006885416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9006885416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9051973958333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9051973958333334
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1202,
          "fn": 998,
          "accuracy": 0.5463636363636364
        },
        "0.01": {
          "tp": 7,
          "fn": 2193,
          "accuracy": 0.003181818181818182
        }
      },
      "auroc": 0.896346590909091
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 578,
          "fn": 622,
          "accuracy": 0.4816666666666667
        },
        "0.01": {
          "tp": 1,
          "fn": 1199,
          "accuracy": 0.0008333333333333334
        }
      },
      "auroc": 0.8903911458333333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1780,
          "fn": 1620,
          "accuracy": 0.5235294117647059
        },
        "0.01": {
          "tp": 8,
          "fn": 3392,
          "accuracy": 0.002352941176470588
        }
      },
      "auroc": 0.894244669117647
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1141,
          "fn": 1059,
          "accuracy": 0.5186363636363637
        },
        "0.01": {
          "tp": 2,
          "fn": 2198,
          "accuracy": 0.0009090909090909091
        }
      },
      "auroc": 0.8933633522727272
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 656,
          "fn": 544,
          "accuracy": 0.5466666666666666
        },
        "0.01": {
          "tp": 6,
          "fn": 1194,
          "accuracy": 0.005
        }
      },
      "auroc": 0.8972727430555555
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1797,
          "fn": 1603,
          "accuracy": 0.5285294117647059
        },
        "0.01": {
          "tp": 8,
          "fn": 3392,
          "accuracy": 0.002352941176470588
        }
      },
      "auroc": 0.894743137254902
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2343,
          "fn": 2057,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 9,
          "fn": 4391,
          "accuracy": 0.0020454545454545456
        }
      },
      "auroc": 0.894854971590909
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1234,
          "fn": 1166,
          "accuracy": 0.5141666666666667
        },
        "0.01": {
          "tp": 7,
          "fn": 2393,
          "accuracy": 0.002916666666666667
        }
      },
      "auroc": 0.8938319444444445
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3577,
          "fn": 3223,
          "accuracy": 0.5260294117647059
        },
        "0.01": {
          "tp": 16,
          "fn": 6784,
          "accuracy": 0.002352941176470588
        }
      },
      "auroc": 0.8944939031862745
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2171,
          "fn": 229,
          "accuracy": 0.9045833333333333
        },
        "0.01": {
          "tp": 1998,
          "fn": 402,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9722430555555556
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2107,
          "fn": 293,
          "accuracy": 0.8779166666666667
        },
        "0.01": {
          "tp": 1935,
          "fn": 465,
          "accuracy": 0.80625
        }
      },
      "auroc": 0.9494171006944445
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4278,
          "fn": 522,
          "accuracy": 0.89125
        },
        "0.01": {
          "tp": 3933,
          "fn": 867,
          "accuracy": 0.819375
        }
      },
      "auroc": 0.9608300781249999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2166,
          "fn": 234,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 1989,
          "fn": 411,
          "accuracy": 0.82875
        }
      },
      "auroc": 0.9701370659722223
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2057,
          "fn": 343,
          "accuracy": 0.8570833333333333
        },
        "0.01": {
          "tp": 1829,
          "fn": 571,
          "accuracy": 0.7620833333333333
        }
      },
      "auroc": 0.9345633680555555
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4223,
          "fn": 577,
          "accuracy": 0.8797916666666666
        },
        "0.01": {
          "tp": 3818,
          "fn": 982,
          "accuracy": 0.7954166666666667
        }
      },
      "auroc": 0.9523502170138889
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4337,
          "fn": 463,
          "accuracy": 0.9035416666666667
        },
        "0.01": {
          "tp": 3987,
          "fn": 813,
          "accuracy": 0.830625
        }
      },
      "auroc": 0.9711900607638888
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4164,
          "fn": 636,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 3764,
          "fn": 1036,
          "accuracy": 0.7841666666666667
        }
      },
      "auroc": 0.941990234375
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8501,
          "fn": 1099,
          "accuracy": 0.8855208333333333
        },
        "0.01": {
          "tp": 7751,
          "fn": 1849,
          "accuracy": 0.8073958333333333
        }
      },
      "auroc": 0.9565901475694445
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2322,
          "fn": 78,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 2177,
          "fn": 223,
          "accuracy": 0.9070833333333334
        }
      },
      "auroc": 0.9909416666666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2242,
          "fn": 158,
          "accuracy": 0.9341666666666667
        },
        "0.01": {
          "tp": 2089,
          "fn": 311,
          "accuracy": 0.8704166666666666
        }
      },
      "auroc": 0.98259375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4564,
          "fn": 236,
          "accuracy": 0.9508333333333333
        },
        "0.01": {
          "tp": 4266,
          "fn": 534,
          "accuracy": 0.88875
        }
      },
      "auroc": 0.9867677083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1895,
          "fn": 505,
          "accuracy": 0.7895833333333333
        },
        "0.01": {
          "tp": 1340,
          "fn": 1060,
          "accuracy": 0.5583333333333333
        }
      },
      "auroc": 0.9258675347222223
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2206,
          "fn": 194,
          "accuracy": 0.9191666666666667
        },
        "0.01": {
          "tp": 2011,
          "fn": 389,
          "accuracy": 0.8379166666666666
        }
      },
      "auroc": 0.9752460069444444
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4101,
          "fn": 699,
          "accuracy": 0.854375
        },
        "0.01": {
          "tp": 3351,
          "fn": 1449,
          "accuracy": 0.698125
        }
      },
      "auroc": 0.9505567708333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4217,
          "fn": 583,
          "accuracy": 0.8785416666666667
        },
        "0.01": {
          "tp": 3517,
          "fn": 1283,
          "accuracy": 0.7327083333333333
        }
      },
      "auroc": 0.9584046006944444
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4448,
          "fn": 352,
          "accuracy": 0.9266666666666666
        },
        "0.01": {
          "tp": 4100,
          "fn": 700,
          "accuracy": 0.8541666666666666
        }
      },
      "auroc": 0.9789198784722222
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8665,
          "fn": 935,
          "accuracy": 0.9026041666666667
        },
        "0.01": {
          "tp": 7617,
          "fn": 1983,
          "accuracy": 0.7934375
        }
      },
      "auroc": 0.9686622395833333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2087,
          "fn": 313,
          "accuracy": 0.8695833333333334
        },
        "0.01": {
          "tp": 1908,
          "fn": 492,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9626823784722223
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2024,
          "fn": 376,
          "accuracy": 0.8433333333333334
        },
        "0.01": {
          "tp": 1841,
          "fn": 559,
          "accuracy": 0.7670833333333333
        }
      },
      "auroc": 0.9316854166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4111,
          "fn": 689,
          "accuracy": 0.8564583333333333
        },
        "0.01": {
          "tp": 3749,
          "fn": 1051,
          "accuracy": 0.7810416666666666
        }
      },
      "auroc": 0.9471838975694444
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2105,
          "fn": 295,
          "accuracy": 0.8770833333333333
        },
        "0.01": {
          "tp": 1902,
          "fn": 498,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9599020833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2029,
          "fn": 371,
          "accuracy": 0.8454166666666667
        },
        "0.01": {
          "tp": 1830,
          "fn": 570,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.9365135416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4134,
          "fn": 666,
          "accuracy": 0.86125
        },
        "0.01": {
          "tp": 3732,
          "fn": 1068,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.9482078125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4192,
          "fn": 608,
          "accuracy": 0.8733333333333333
        },
        "0.01": {
          "tp": 3810,
          "fn": 990,
          "accuracy": 0.79375
        }
      },
      "auroc": 0.9612922309027778
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4053,
          "fn": 747,
          "accuracy": 0.844375
        },
        "0.01": {
          "tp": 3671,
          "fn": 1129,
          "accuracy": 0.7647916666666666
        }
      },
      "auroc": 0.9340994791666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8245,
          "fn": 1355,
          "accuracy": 0.8588541666666667
        },
        "0.01": {
          "tp": 7481,
          "fn": 2119,
          "accuracy": 0.7792708333333334
        }
      },
      "auroc": 0.9476958550347221
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2354,
          "fn": 46,
          "accuracy": 0.9808333333333333
        },
        "0.01": {
          "tp": 2187,
          "fn": 213,
          "accuracy": 0.91125
        }
      },
      "auroc": 0.9929800347222222
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2173,
          "fn": 227,
          "accuracy": 0.9054166666666666
        },
        "0.01": {
          "tp": 2000,
          "fn": 400,
          "accuracy": 0.8333333333333334
        }
      },
      "auroc": 0.9766849826388889
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4527,
          "fn": 273,
          "accuracy": 0.943125
        },
        "0.01": {
          "tp": 4187,
          "fn": 613,
          "accuracy": 0.8722916666666667
        }
      },
      "auroc": 0.9848325086805555
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1994,
          "fn": 406,
          "accuracy": 0.8308333333333333
        },
        "0.01": {
          "tp": 1561,
          "fn": 839,
          "accuracy": 0.6504166666666666
        }
      },
      "auroc": 0.9457407986111112
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2029,
          "fn": 371,
          "accuracy": 0.8454166666666667
        },
        "0.01": {
          "tp": 1842,
          "fn": 558,
          "accuracy": 0.7675
        }
      },
      "auroc": 0.9376212673611111
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4023,
          "fn": 777,
          "accuracy": 0.838125
        },
        "0.01": {
          "tp": 3403,
          "fn": 1397,
          "accuracy": 0.7089583333333334
        }
      },
      "auroc": 0.941681032986111
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4348,
          "fn": 452,
          "accuracy": 0.9058333333333334
        },
        "0.01": {
          "tp": 3748,
          "fn": 1052,
          "accuracy": 0.7808333333333334
        }
      },
      "auroc": 0.9693604166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4202,
          "fn": 598,
          "accuracy": 0.8754166666666666
        },
        "0.01": {
          "tp": 3842,
          "fn": 958,
          "accuracy": 0.8004166666666667
        }
      },
      "auroc": 0.9571531249999999
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8550,
          "fn": 1050,
          "accuracy": 0.890625
        },
        "0.01": {
          "tp": 7590,
          "fn": 2010,
          "accuracy": 0.790625
        }
      },
      "auroc": 0.9632567708333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2293,
          "fn": 107,
          "accuracy": 0.9554166666666667
        },
        "0.01": {
          "tp": 2135,
          "fn": 265,
          "accuracy": 0.8895833333333333
        }
      },
      "auroc": 0.9879757812500001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1902,
          "fn": 498,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 1668,
          "fn": 732,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9233104166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4195,
          "fn": 605,
          "accuracy": 0.8739583333333333
        },
        "0.01": {
          "tp": 3803,
          "fn": 997,
          "accuracy": 0.7922916666666666
        }
      },
      "auroc": 0.9556430989583333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1492,
          "fn": 908,
          "accuracy": 0.6216666666666667
        },
        "0.01": {
          "tp": 805,
          "fn": 1595,
          "accuracy": 0.33541666666666664
        }
      },
      "auroc": 0.8590955729166667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2022,
          "fn": 378,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 1838,
          "fn": 562,
          "accuracy": 0.7658333333333334
        }
      },
      "auroc": 0.9353388888888888
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3514,
          "fn": 1286,
          "accuracy": 0.7320833333333333
        },
        "0.01": {
          "tp": 2643,
          "fn": 2157,
          "accuracy": 0.550625
        }
      },
      "auroc": 0.8972172309027777
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3785,
          "fn": 1015,
          "accuracy": 0.7885416666666667
        },
        "0.01": {
          "tp": 2940,
          "fn": 1860,
          "accuracy": 0.6125
        }
      },
      "auroc": 0.9235356770833333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3924,
          "fn": 876,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 3506,
          "fn": 1294,
          "accuracy": 0.7304166666666667
        }
      },
      "auroc": 0.9293246527777779
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7709,
          "fn": 1891,
          "accuracy": 0.8030208333333333
        },
        "0.01": {
          "tp": 6446,
          "fn": 3154,
          "accuracy": 0.6714583333333334
        }
      },
      "auroc": 0.9264301649305555
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 200,
          "accuracy": 0.9166666666666666
        },
        "0.01": {
          "tp": 2008,
          "fn": 392,
          "accuracy": 0.8366666666666667
        }
      },
      "auroc": 0.9755615451388889
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2064,
          "fn": 336,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 1930,
          "fn": 470,
          "accuracy": 0.8041666666666667
        }
      },
      "auroc": 0.9408326388888889
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4264,
          "fn": 536,
          "accuracy": 0.8883333333333333
        },
        "0.01": {
          "tp": 3938,
          "fn": 862,
          "accuracy": 0.8204166666666667
        }
      },
      "auroc": 0.9581970920138889
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2112,
          "fn": 288,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 1933,
          "fn": 467,
          "accuracy": 0.8054166666666667
        }
      },
      "auroc": 0.9579046006944444
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1988,
          "fn": 412,
          "accuracy": 0.8283333333333334
        },
        "0.01": {
          "tp": 1749,
          "fn": 651,
          "accuracy": 0.72875
        }
      },
      "auroc": 0.9156125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4100,
          "fn": 700,
          "accuracy": 0.8541666666666666
        },
        "0.01": {
          "tp": 3682,
          "fn": 1118,
          "accuracy": 0.7670833333333333
        }
      },
      "auroc": 0.9367585503472222
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4312,
          "fn": 488,
          "accuracy": 0.8983333333333333
        },
        "0.01": {
          "tp": 3941,
          "fn": 859,
          "accuracy": 0.8210416666666667
        }
      },
      "auroc": 0.9667330729166668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4052,
          "fn": 748,
          "accuracy": 0.8441666666666666
        },
        "0.01": {
          "tp": 3679,
          "fn": 1121,
          "accuracy": 0.7664583333333334
        }
      },
      "auroc": 0.9282225694444445
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8364,
          "fn": 1236,
          "accuracy": 0.87125
        },
        "0.01": {
          "tp": 7620,
          "fn": 1980,
          "accuracy": 0.79375
        }
      },
      "auroc": 0.9474778211805557
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1940,
          "fn": 460,
          "accuracy": 0.8083333333333333
        },
        "0.01": {
          "tp": 1477,
          "fn": 923,
          "accuracy": 0.6154166666666666
        }
      },
      "auroc": 0.9513246527777777
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1940,
          "fn": 460,
          "accuracy": 0.8083333333333333
        },
        "0.01": {
          "tp": 1477,
          "fn": 923,
          "accuracy": 0.6154166666666666
        }
      },
      "auroc": 0.9513246527777777
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1820,
          "fn": 580,
          "accuracy": 0.7583333333333333
        },
        "0.01": {
          "tp": 1356,
          "fn": 1044,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9361240451388888
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1820,
          "fn": 580,
          "accuracy": 0.7583333333333333
        },
        "0.01": {
          "tp": 1356,
          "fn": 1044,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9361240451388888
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3760,
          "fn": 1040,
          "accuracy": 0.7833333333333333
        },
        "0.01": {
          "tp": 2833,
          "fn": 1967,
          "accuracy": 0.5902083333333333
        }
      },
      "auroc": 0.9437243489583333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3760,
          "fn": 1040,
          "accuracy": 0.7833333333333333
        },
        "0.01": {
          "tp": 2833,
          "fn": 1967,
          "accuracy": 0.5902083333333333
        }
      },
      "auroc": 0.9437243489583333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1383,
          "fn": 1017,
          "accuracy": 0.57625
        },
        "0.01": {
          "tp": 911,
          "fn": 1489,
          "accuracy": 0.37958333333333333
        }
      },
      "auroc": 0.8509328125
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1383,
          "fn": 1017,
          "accuracy": 0.57625
        },
        "0.01": {
          "tp": 911,
          "fn": 1489,
          "accuracy": 0.37958333333333333
        }
      },
      "auroc": 0.8509328125
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1145,
          "fn": 1255,
          "accuracy": 0.47708333333333336
        },
        "0.01": {
          "tp": 620,
          "fn": 1780,
          "accuracy": 0.25833333333333336
        }
      },
      "auroc": 0.8068312499999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1145,
          "fn": 1255,
          "accuracy": 0.47708333333333336
        },
        "0.01": {
          "tp": 620,
          "fn": 1780,
          "accuracy": 0.25833333333333336
        }
      },
      "auroc": 0.8068312499999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2528,
          "fn": 2272,
          "accuracy": 0.5266666666666666
        },
        "0.01": {
          "tp": 1531,
          "fn": 3269,
          "accuracy": 0.31895833333333334
        }
      },
      "auroc": 0.82888203125
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2528,
          "fn": 2272,
          "accuracy": 0.5266666666666666
        },
        "0.01": {
          "tp": 1531,
          "fn": 3269,
          "accuracy": 0.31895833333333334
        }
      },
      "auroc": 0.82888203125
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2132,
          "fn": 268,
          "accuracy": 0.8883333333333333
        },
        "0.01": {
          "tp": 1959,
          "fn": 441,
          "accuracy": 0.81625
        }
      },
      "auroc": 0.9653003472222222
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2132,
          "fn": 268,
          "accuracy": 0.8883333333333333
        },
        "0.01": {
          "tp": 1959,
          "fn": 441,
          "accuracy": 0.81625
        }
      },
      "auroc": 0.9653003472222222
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2089,
          "fn": 311,
          "accuracy": 0.8704166666666666
        },
        "0.01": {
          "tp": 1899,
          "fn": 501,
          "accuracy": 0.79125
        }
      },
      "auroc": 0.9543991319444445
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2089,
          "fn": 311,
          "accuracy": 0.8704166666666666
        },
        "0.01": {
          "tp": 1899,
          "fn": 501,
          "accuracy": 0.79125
        }
      },
      "auroc": 0.9543991319444445
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4221,
          "fn": 579,
          "accuracy": 0.879375
        },
        "0.01": {
          "tp": 3858,
          "fn": 942,
          "accuracy": 0.80375
        }
      },
      "auroc": 0.9598497395833334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4221,
          "fn": 579,
          "accuracy": 0.879375
        },
        "0.01": {
          "tp": 3858,
          "fn": 942,
          "accuracy": 0.80375
        }
      },
      "auroc": 0.9598497395833334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2084,
          "fn": 316,
          "accuracy": 0.8683333333333333
        },
        "0.01": {
          "tp": 1930,
          "fn": 470,
          "accuracy": 0.8041666666666667
        }
      },
      "auroc": 0.9579750868055557
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2084,
          "fn": 316,
          "accuracy": 0.8683333333333333
        },
        "0.01": {
          "tp": 1930,
          "fn": 470,
          "accuracy": 0.8041666666666667
        }
      },
      "auroc": 0.9579750868055557
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1148,
          "fn": 1252,
          "accuracy": 0.47833333333333333
        },
        "0.01": {
          "tp": 722,
          "fn": 1678,
          "accuracy": 0.30083333333333334
        }
      },
      "auroc": 0.7535405381944444
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1148,
          "fn": 1252,
          "accuracy": 0.47833333333333333
        },
        "0.01": {
          "tp": 722,
          "fn": 1678,
          "accuracy": 0.30083333333333334
        }
      },
      "auroc": 0.7535405381944444
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3232,
          "fn": 1568,
          "accuracy": 0.6733333333333333
        },
        "0.01": {
          "tp": 2652,
          "fn": 2148,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.8557578124999999
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3232,
          "fn": 1568,
          "accuracy": 0.6733333333333333
        },
        "0.01": {
          "tp": 2652,
          "fn": 2148,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.8557578124999999
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1984,
          "fn": 416,
          "accuracy": 0.8266666666666667
        },
        "0.01": {
          "tp": 1628,
          "fn": 772,
          "accuracy": 0.6783333333333333
        }
      },
      "auroc": 0.9428327256944444
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1984,
          "fn": 416,
          "accuracy": 0.8266666666666667
        },
        "0.01": {
          "tp": 1628,
          "fn": 772,
          "accuracy": 0.6783333333333333
        }
      },
      "auroc": 0.9428327256944444
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1710,
          "fn": 690,
          "accuracy": 0.7125
        },
        "0.01": {
          "tp": 1364,
          "fn": 1036,
          "accuracy": 0.5683333333333334
        }
      },
      "auroc": 0.8876970486111111
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1710,
          "fn": 690,
          "accuracy": 0.7125
        },
        "0.01": {
          "tp": 1364,
          "fn": 1036,
          "accuracy": 0.5683333333333334
        }
      },
      "auroc": 0.8876970486111111
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3694,
          "fn": 1106,
          "accuracy": 0.7695833333333333
        },
        "0.01": {
          "tp": 2992,
          "fn": 1808,
          "accuracy": 0.6233333333333333
        }
      },
      "auroc": 0.9152648871527778
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3694,
          "fn": 1106,
          "accuracy": 0.7695833333333333
        },
        "0.01": {
          "tp": 2992,
          "fn": 1808,
          "accuracy": 0.6233333333333333
        }
      },
      "auroc": 0.9152648871527778
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 22950,
          "fn": 3450,
          "accuracy": 0.8693181818181818
        },
        "0.01": {
          "tp": 20318,
          "fn": 6082,
          "accuracy": 0.7696212121212122
        }
      },
      "auroc": 0.9591590988005052
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 12512,
          "fn": 1888,
          "accuracy": 0.8688888888888889
        },
        "0.01": {
          "tp": 11463,
          "fn": 2937,
          "accuracy": 0.7960416666666666
        }
      },
      "auroc": 0.9507540509259258
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 35462,
          "fn": 5338,
          "accuracy": 0.8691666666666666
        },
        "0.01": {
          "tp": 31781,
          "fn": 9019,
          "accuracy": 0.7789460784313725
        }
      },
      "auroc": 0.9561926113153594
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 19676,
          "fn": 6724,
          "accuracy": 0.7453030303030304
        },
        "0.01": {
          "tp": 15491,
          "fn": 10909,
          "accuracy": 0.586780303030303
        }
      },
      "auroc": 0.9052036063762626
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 12331,
          "fn": 2069,
          "accuracy": 0.8563194444444444
        },
        "0.01": {
          "tp": 11099,
          "fn": 3301,
          "accuracy": 0.7707638888888889
        }
      },
      "auroc": 0.9391492621527778
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 32007,
          "fn": 8793,
          "accuracy": 0.7844852941176471
        },
        "0.01": {
          "tp": 26590,
          "fn": 14210,
          "accuracy": 0.6517156862745098
        }
      },
      "auroc": 0.9171844260620915
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 42626,
          "fn": 10174,
          "accuracy": 0.807310606060606
        },
        "0.01": {
          "tp": 35809,
          "fn": 16991,
          "accuracy": 0.6782007575757576
        }
      },
      "auroc": 0.9321813525883837
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 24843,
          "fn": 3957,
          "accuracy": 0.8626041666666666
        },
        "0.01": {
          "tp": 22562,
          "fn": 6238,
          "accuracy": 0.7834027777777778
        }
      },
      "auroc": 0.9449516565393519
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 67469,
          "fn": 14131,
          "accuracy": 0.8268259803921568
        },
        "0.01": {
          "tp": 58371,
          "fn": 23229,
          "accuracy": 0.7153308823529412
        }
      },
      "auroc": 0.9366885186887255
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9823020833333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.982759375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.9825307291666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9828187500000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.9836145833333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.9832166666666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9825604166666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9831869791666668
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 6,
          "fn": 794,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9828736979166667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.985315625
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9850697916666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.9851927083333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9645260416666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.9812645833333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9728953124999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 57,
          "fn": 343,
          "accuracy": 0.1425
        }
      },
      "auroc": 0.9749208333333335
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9831671875000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 722,
          "fn": 78,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 61,
          "fn": 739,
          "accuracy": 0.07625
        }
      },
      "auroc": 0.9790440104166667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.98595
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.98401875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        }
      },
      "auroc": 0.984984375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.986428125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.986215625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.986321875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        }
      },
      "auroc": 0.9861890625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.9851171875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 45,
          "fn": 755,
          "accuracy": 0.05625
        }
      },
      "auroc": 0.985653125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9852666666666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.9866312500000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.9859489583333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9618145833333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.9800354166666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        }
      },
      "auroc": 0.970925
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        }
      },
      "auroc": 0.973540625
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        }
      },
      "auroc": 0.9833333333333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 735,
          "fn": 65,
          "accuracy": 0.91875
        },
        "0.01": {
          "tp": 117,
          "fn": 683,
          "accuracy": 0.14625
        }
      },
      "auroc": 0.9784369791666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.9862260416666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9823552083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 17,
          "fn": 383,
          "accuracy": 0.0425
        }
      },
      "auroc": 0.9842906250000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.9443999999999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9816989583333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": {
          "tp": 53,
          "fn": 347,
          "accuracy": 0.1325
        }
      },
      "auroc": 0.9630494791666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        }
      },
      "auroc": 0.9653130208333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9820270833333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 697,
          "fn": 103,
          "accuracy": 0.87125
        },
        "0.01": {
          "tp": 70,
          "fn": 730,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.9736700520833332
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9792458333333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9805208333333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9798833333333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9813562499999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.983578125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9824671875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9803010416666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9820494791666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 4,
          "fn": 796,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9811752604166667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9765604166666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9765604166666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9646239583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9646239583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9705921875
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9705921875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9606010416666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9606010416666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9039572916666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9039572916666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 90,
          "fn": 310,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9322791666666665
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 90,
          "fn": 310,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9322791666666665
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9818604166666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9818604166666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9834875
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9834875
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9826739583333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9826739583333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9824729166666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9824729166666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.958090625
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.958090625
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9702817708333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9702817708333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9784572916666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9784572916666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9650979166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9650979166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.9717776041666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.9717776041666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2101,
          "fn": 99,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 148,
          "fn": 2052,
          "accuracy": 0.06727272727272728
        }
      },
      "auroc": 0.9803871212121211
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1185,
          "fn": 15,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 37,
          "fn": 1163,
          "accuracy": 0.030833333333333334
        }
      },
      "auroc": 0.9835592013888889
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3286,
          "fn": 114,
          "accuracy": 0.9664705882352941
        },
        "0.01": {
          "tp": 185,
          "fn": 3215,
          "accuracy": 0.054411764705882354
        }
      },
      "auroc": 0.9815066789215686
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1775,
          "fn": 425,
          "accuracy": 0.8068181818181818
        },
        "0.01": {
          "tp": 306,
          "fn": 1894,
          "accuracy": 0.1390909090909091
        }
      },
      "auroc": 0.9633273674242424
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1166,
          "fn": 34,
          "accuracy": 0.9716666666666667
        },
        "0.01": {
          "tp": 60,
          "fn": 1140,
          "accuracy": 0.05
        }
      },
      "auroc": 0.9827345486111112
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2941,
          "fn": 459,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 366,
          "fn": 3034,
          "accuracy": 0.10764705882352942
        }
      },
      "auroc": 0.9701769607843138
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3876,
          "fn": 524,
          "accuracy": 0.8809090909090909
        },
        "0.01": {
          "tp": 454,
          "fn": 3946,
          "accuracy": 0.10318181818181818
        }
      },
      "auroc": 0.9718572443181819
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2351,
          "fn": 49,
          "accuracy": 0.9795833333333334
        },
        "0.01": {
          "tp": 97,
          "fn": 2303,
          "accuracy": 0.04041666666666666
        }
      },
      "auroc": 0.9831468750000001
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6227,
          "fn": 573,
          "accuracy": 0.9157352941176471
        },
        "0.01": {
          "tp": 551,
          "fn": 6249,
          "accuracy": 0.08102941176470588
        }
      },
      "auroc": 0.9758418198529413
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9823020833333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.982759375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.9825307291666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9828187500000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.9836145833333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.9832166666666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9825604166666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9831869791666668
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 6,
          "fn": 794,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9828736979166667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.985315625
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9850697916666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.9851927083333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9645260416666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.9812645833333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9728953124999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 57,
          "fn": 343,
          "accuracy": 0.1425
        }
      },
      "auroc": 0.9749208333333335
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9831671875000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 722,
          "fn": 78,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 61,
          "fn": 739,
          "accuracy": 0.07625
        }
      },
      "auroc": 0.9790440104166667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.98595
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.98401875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        }
      },
      "auroc": 0.984984375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.986428125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.986215625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.986321875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        }
      },
      "auroc": 0.9861890625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.9851171875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 45,
          "fn": 755,
          "accuracy": 0.05625
        }
      },
      "auroc": 0.985653125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9852666666666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.9866312500000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.9859489583333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9618145833333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.9800354166666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        }
      },
      "auroc": 0.970925
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        }
      },
      "auroc": 0.973540625
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        }
      },
      "auroc": 0.9833333333333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 735,
          "fn": 65,
          "accuracy": 0.91875
        },
        "0.01": {
          "tp": 117,
          "fn": 683,
          "accuracy": 0.14625
        }
      },
      "auroc": 0.9784369791666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.9862260416666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9823552083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 17,
          "fn": 383,
          "accuracy": 0.0425
        }
      },
      "auroc": 0.9842906250000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.9443999999999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9816989583333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": {
          "tp": 53,
          "fn": 347,
          "accuracy": 0.1325
        }
      },
      "auroc": 0.9630494791666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        }
      },
      "auroc": 0.9653130208333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9820270833333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 697,
          "fn": 103,
          "accuracy": 0.87125
        },
        "0.01": {
          "tp": 70,
          "fn": 730,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.9736700520833332
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9792458333333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9805208333333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9798833333333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9813562499999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.983578125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9824671875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9803010416666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9820494791666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 4,
          "fn": 796,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9811752604166667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9765604166666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9765604166666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9646239583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9646239583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9705921875
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9705921875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9606010416666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9606010416666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9039572916666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9039572916666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 90,
          "fn": 310,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9322791666666665
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 90,
          "fn": 310,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9322791666666665
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9818604166666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9818604166666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9834875
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9834875
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9826739583333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9826739583333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9824729166666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9824729166666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.958090625
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.958090625
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9702817708333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9702817708333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9784572916666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9784572916666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9650979166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9650979166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.9717776041666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.9717776041666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2101,
          "fn": 99,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 148,
          "fn": 2052,
          "accuracy": 0.06727272727272728
        }
      },
      "auroc": 0.9803871212121211
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1185,
          "fn": 15,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 37,
          "fn": 1163,
          "accuracy": 0.030833333333333334
        }
      },
      "auroc": 0.9835592013888889
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3286,
          "fn": 114,
          "accuracy": 0.9664705882352941
        },
        "0.01": {
          "tp": 185,
          "fn": 3215,
          "accuracy": 0.054411764705882354
        }
      },
      "auroc": 0.9815066789215686
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1775,
          "fn": 425,
          "accuracy": 0.8068181818181818
        },
        "0.01": {
          "tp": 306,
          "fn": 1894,
          "accuracy": 0.1390909090909091
        }
      },
      "auroc": 0.9633273674242424
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1166,
          "fn": 34,
          "accuracy": 0.9716666666666667
        },
        "0.01": {
          "tp": 60,
          "fn": 1140,
          "accuracy": 0.05
        }
      },
      "auroc": 0.9827345486111112
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2941,
          "fn": 459,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 366,
          "fn": 3034,
          "accuracy": 0.10764705882352942
        }
      },
      "auroc": 0.9701769607843138
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3876,
          "fn": 524,
          "accuracy": 0.8809090909090909
        },
        "0.01": {
          "tp": 454,
          "fn": 3946,
          "accuracy": 0.10318181818181818
        }
      },
      "auroc": 0.9718572443181819
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2351,
          "fn": 49,
          "accuracy": 0.9795833333333334
        },
        "0.01": {
          "tp": 97,
          "fn": 2303,
          "accuracy": 0.04041666666666666
        }
      },
      "auroc": 0.9831468750000001
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6227,
          "fn": 573,
          "accuracy": 0.9157352941176471
        },
        "0.01": {
          "tp": 551,
          "fn": 6249,
          "accuracy": 0.08102941176470588
        }
      },
      "auroc": 0.9758418198529413
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.992484375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.9937489583333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9931166666666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.9934385416666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.9865375000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        }
      },
      "auroc": 0.9899880208333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 96,
          "fn": 304,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9929614583333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.9901432291666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        },
        "0.01": {
          "tp": 249,
          "fn": 551,
          "accuracy": 0.31125
        }
      },
      "auroc": 0.99155234375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.99173125
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9856343750000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 44,
          "fn": 356,
          "accuracy": 0.11
        }
      },
      "auroc": 0.9886828125000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.919978125
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9828645833333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 21,
          "fn": 379,
          "accuracy": 0.0525
        }
      },
      "auroc": 0.9514213541666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 57,
          "fn": 343,
          "accuracy": 0.1425
        }
      },
      "auroc": 0.9558546875
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        }
      },
      "auroc": 0.9842494791666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 646,
          "fn": 154,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 65,
          "fn": 735,
          "accuracy": 0.08125
        }
      },
      "auroc": 0.9700520833333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.9929354166666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9865729166666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        }
      },
      "auroc": 0.9897541666666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.9930208333333335
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9882041666666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 123,
          "fn": 277,
          "accuracy": 0.3075
        }
      },
      "auroc": 0.9906125000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        }
      },
      "auroc": 0.9929781249999999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 99,
          "fn": 301,
          "accuracy": 0.2475
        }
      },
      "auroc": 0.9873885416666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 773,
          "fn": 27,
          "accuracy": 0.96625
        },
        "0.01": {
          "tp": 264,
          "fn": 536,
          "accuracy": 0.33
        }
      },
      "auroc": 0.9901833333333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.9881479166666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.9938895833333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        }
      },
      "auroc": 0.9910187500000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.9195979166666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.9816552083333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 80,
          "fn": 320,
          "accuracy": 0.2
        }
      },
      "auroc": 0.9506265625
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        }
      },
      "auroc": 0.9538729166666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        }
      },
      "auroc": 0.9877723958333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 666,
          "fn": 134,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 163,
          "fn": 637,
          "accuracy": 0.20375
        }
      },
      "auroc": 0.97082265625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.9918374999999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9831322916666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 73,
          "fn": 327,
          "accuracy": 0.1825
        }
      },
      "auroc": 0.9874848958333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.885359375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.98335625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        }
      },
      "auroc": 0.9343578125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 57,
          "fn": 343,
          "accuracy": 0.1425
        }
      },
      "auroc": 0.9385984375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.9832442708333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 625,
          "fn": 175,
          "accuracy": 0.78125
        },
        "0.01": {
          "tp": 107,
          "fn": 693,
          "accuracy": 0.13375
        }
      },
      "auroc": 0.9609213541666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.9904927083333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9915229166666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        }
      },
      "auroc": 0.9910078124999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.9929812499999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.989975
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9914781250000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        }
      },
      "auroc": 0.9917369791666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 100,
          "fn": 300,
          "accuracy": 0.25
        }
      },
      "auroc": 0.9907489583333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 181,
          "fn": 619,
          "accuracy": 0.22625
        }
      },
      "auroc": 0.9912429687500001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.964459375
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.964459375
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9347916666666668
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9347916666666668
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 107,
          "fn": 293,
          "accuracy": 0.2675
        }
      },
      "auroc": 0.9496255208333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 107,
          "fn": 293,
          "accuracy": 0.2675
        }
      },
      "auroc": 0.9496255208333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.905278125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.905278125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.8216916666666665
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.8216916666666665
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 283,
          "accuracy": 0.2925
        },
        "0.01": {
          "tp": 40,
          "fn": 360,
          "accuracy": 0.1
        }
      },
      "auroc": 0.8634848958333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 283,
          "accuracy": 0.2925
        },
        "0.01": {
          "tp": 40,
          "fn": 360,
          "accuracy": 0.1
        }
      },
      "auroc": 0.8634848958333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9923354166666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9923354166666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.99088125
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.99088125
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 119,
          "fn": 281,
          "accuracy": 0.2975
        }
      },
      "auroc": 0.9916083333333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 119,
          "fn": 281,
          "accuracy": 0.2975
        }
      },
      "auroc": 0.9916083333333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.9914697916666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.9914697916666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.8700114583333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.8700114583333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        }
      },
      "auroc": 0.930740625
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        }
      },
      "auroc": 0.930740625
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.97101875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.97101875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.9408000000000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.9408000000000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": {
          "tp": 117,
          "fn": 283,
          "accuracy": 0.2925
        }
      },
      "auroc": 0.9559093750000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": {
          "tp": 117,
          "fn": 283,
          "accuracy": 0.2925
        }
      },
      "auroc": 0.9559093750000001
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1962,
          "fn": 238,
          "accuracy": 0.8918181818181818
        },
        "0.01": {
          "tp": 479,
          "fn": 1721,
          "accuracy": 0.2177272727272727
        }
      },
      "auroc": 0.9792900568181817
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1173,
          "fn": 27,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 269,
          "fn": 931,
          "accuracy": 0.22416666666666665
        }
      },
      "auroc": 0.9890835069444445
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3135,
          "fn": 265,
          "accuracy": 0.9220588235294118
        },
        "0.01": {
          "tp": 748,
          "fn": 2652,
          "accuracy": 0.22
        }
      },
      "auroc": 0.982746568627451
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1320,
          "fn": 880,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 458,
          "fn": 1742,
          "accuracy": 0.2081818181818182
        }
      },
      "auroc": 0.9329592803030303
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1150,
          "fn": 50,
          "accuracy": 0.9583333333333334
        },
        "0.01": {
          "tp": 274,
          "fn": 926,
          "accuracy": 0.22833333333333333
        }
      },
      "auroc": 0.9854321180555555
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2470,
          "fn": 930,
          "accuracy": 0.7264705882352941
        },
        "0.01": {
          "tp": 732,
          "fn": 2668,
          "accuracy": 0.21529411764705883
        }
      },
      "auroc": 0.9514791053921569
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3282,
          "fn": 1118,
          "accuracy": 0.7459090909090909
        },
        "0.01": {
          "tp": 937,
          "fn": 3463,
          "accuracy": 0.21295454545454545
        }
      },
      "auroc": 0.956124668560606
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2323,
          "fn": 77,
          "accuracy": 0.9679166666666666
        },
        "0.01": {
          "tp": 543,
          "fn": 1857,
          "accuracy": 0.22625
        }
      },
      "auroc": 0.9872578125
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5605,
          "fn": 1195,
          "accuracy": 0.8242647058823529
        },
        "0.01": {
          "tp": 1480,
          "fn": 5320,
          "accuracy": 0.21764705882352942
        }
      },
      "auroc": 0.9671128370098039
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9882510416666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.9894364583333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        }
      },
      "auroc": 0.98884375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.9893510416666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.9841374999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        }
      },
      "auroc": 0.9867442708333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 38,
          "fn": 362,
          "accuracy": 0.095
        }
      },
      "auroc": 0.9888010416666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        }
      },
      "auroc": 0.9867869791666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 93,
          "fn": 707,
          "accuracy": 0.11625
        }
      },
      "auroc": 0.9877940104166667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.9879958333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9853468750000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        }
      },
      "auroc": 0.9866713541666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.9165125
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.9807
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        },
        "0.01": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9486062500000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": {
          "tp": 38,
          "fn": 362,
          "accuracy": 0.095
        }
      },
      "auroc": 0.9522541666666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9830234375000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 651,
          "fn": 149,
          "accuracy": 0.81375
        },
        "0.01": {
          "tp": 42,
          "fn": 758,
          "accuracy": 0.0525
        }
      },
      "auroc": 0.9676388020833333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9895552083333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.9851270833333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 93,
          "fn": 307,
          "accuracy": 0.2325
        }
      },
      "auroc": 0.9873411458333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.9869208333333332
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9834979166666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        }
      },
      "auroc": 0.9852093749999999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        }
      },
      "auroc": 0.9882380208333335
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 65,
          "fn": 335,
          "accuracy": 0.1625
        }
      },
      "auroc": 0.9843124999999999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 765,
          "fn": 35,
          "accuracy": 0.95625
        },
        "0.01": {
          "tp": 148,
          "fn": 652,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9862752604166667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9853427083333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9917947916666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.98856875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.91736875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9753697916666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 71,
          "fn": 329,
          "accuracy": 0.1775
        }
      },
      "auroc": 0.9463692708333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 37,
          "fn": 363,
          "accuracy": 0.0925
        }
      },
      "auroc": 0.9513557291666668
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 90,
          "fn": 310,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9835822916666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 648,
          "fn": 152,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 127,
          "fn": 673,
          "accuracy": 0.15875
        }
      },
      "auroc": 0.9674690104166667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.9888
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9793520833333332
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        }
      },
      "auroc": 0.9840760416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.8918385416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.9795385416666668
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        }
      },
      "auroc": 0.9356885416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.9403192708333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        }
      },
      "auroc": 0.9794453125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 617,
          "fn": 183,
          "accuracy": 0.77125
        },
        "0.01": {
          "tp": 75,
          "fn": 725,
          "accuracy": 0.09375
        }
      },
      "auroc": 0.9598822916666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9859135416666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9864916666666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9862026041666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.98748125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9850479166666668
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.9862645833333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 23,
          "fn": 377,
          "accuracy": 0.0575
        }
      },
      "auroc": 0.9866973958333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 38,
          "fn": 362,
          "accuracy": 0.095
        }
      },
      "auroc": 0.9857697916666668
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 61,
          "fn": 739,
          "accuracy": 0.07625
        }
      },
      "auroc": 0.9862335937500002
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.9505458333333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.9505458333333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.9262520833333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.9262520833333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        }
      },
      "auroc": 0.9383989583333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        }
      },
      "auroc": 0.9383989583333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.9015312499999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.9015312499999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.8397989583333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.8397989583333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 260,
          "accuracy": 0.35
        },
        "0.01": {
          "tp": 42,
          "fn": 358,
          "accuracy": 0.105
        }
      },
      "auroc": 0.8706651041666668
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 260,
          "accuracy": 0.35
        },
        "0.01": {
          "tp": 42,
          "fn": 358,
          "accuracy": 0.105
        }
      },
      "auroc": 0.8706651041666668
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.9877020833333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.9877020833333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9878197916666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9878197916666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.9877609375
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.9877609375
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.9879479166666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.9879479166666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.8995354166666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.8995354166666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        }
      },
      "auroc": 0.9437416666666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        }
      },
      "auroc": 0.9437416666666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.96683125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.96683125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9350083333333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9350083333333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 75,
          "fn": 325,
          "accuracy": 0.1875
        }
      },
      "auroc": 0.9509197916666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 75,
          "fn": 325,
          "accuracy": 0.1875
        }
      },
      "auroc": 0.9509197916666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1973,
          "fn": 227,
          "accuracy": 0.8968181818181818
        },
        "0.01": {
          "tp": 249,
          "fn": 1951,
          "accuracy": 0.11318181818181818
        }
      },
      "auroc": 0.9745833333333334
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1166,
          "fn": 34,
          "accuracy": 0.9716666666666667
        },
        "0.01": {
          "tp": 155,
          "fn": 1045,
          "accuracy": 0.12916666666666668
        }
      },
      "auroc": 0.9862581597222222
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3139,
          "fn": 261,
          "accuracy": 0.923235294117647
        },
        "0.01": {
          "tp": 404,
          "fn": 2996,
          "accuracy": 0.1188235294117647
        }
      },
      "auroc": 0.9787038602941176
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1356,
          "fn": 844,
          "accuracy": 0.6163636363636363
        },
        "0.01": {
          "tp": 246,
          "fn": 1954,
          "accuracy": 0.11181818181818182
        }
      },
      "auroc": 0.9343534090909091
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1145,
          "fn": 55,
          "accuracy": 0.9541666666666667
        },
        "0.01": {
          "tp": 145,
          "fn": 1055,
          "accuracy": 0.12083333333333333
        }
      },
      "auroc": 0.9813819444444445
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2501,
          "fn": 899,
          "accuracy": 0.7355882352941177
        },
        "0.01": {
          "tp": 391,
          "fn": 3009,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9509517156862746
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3329,
          "fn": 1071,
          "accuracy": 0.7565909090909091
        },
        "0.01": {
          "tp": 495,
          "fn": 3905,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.9544683712121212
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2311,
          "fn": 89,
          "accuracy": 0.9629166666666666
        },
        "0.01": {
          "tp": 300,
          "fn": 2100,
          "accuracy": 0.125
        }
      },
      "auroc": 0.9838200520833333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 5640,
          "fn": 1160,
          "accuracy": 0.8294117647058824
        },
        "0.01": {
          "tp": 795,
          "fn": 6005,
          "accuracy": 0.11691176470588235
        }
      },
      "auroc": 0.9648277879901961
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9835197916666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9844229166666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9839713541666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9842354166666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9844510416666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9843432291666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9838776041666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        }
      },
      "auroc": 0.9844369791666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": {
          "tp": 14,
          "fn": 786,
          "accuracy": 0.0175
        }
      },
      "auroc": 0.9841572916666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.9858843749999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9852197916666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9855520833333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.9460354166666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9813239583333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": {
          "tp": 37,
          "fn": 363,
          "accuracy": 0.0925
        }
      },
      "auroc": 0.9636796875000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 39,
          "fn": 361,
          "accuracy": 0.0975
        }
      },
      "auroc": 0.9659598958333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.983271875
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 691,
          "fn": 109,
          "accuracy": 0.86375
        },
        "0.01": {
          "tp": 43,
          "fn": 757,
          "accuracy": 0.05375
        }
      },
      "auroc": 0.9746158854166667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.987221875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.9830802083333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        }
      },
      "auroc": 0.9851510416666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.987903125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.9859822916666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9869427083333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 29,
          "fn": 371,
          "accuracy": 0.0725
        }
      },
      "auroc": 0.9875625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 37,
          "fn": 363,
          "accuracy": 0.0925
        }
      },
      "auroc": 0.98453125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 66,
          "fn": 734,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.986046875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.9854791666666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.9896291666666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.9875541666666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9409145833333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.9775375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        }
      },
      "auroc": 0.9592260416666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 57,
          "fn": 343,
          "accuracy": 0.1425
        }
      },
      "auroc": 0.963196875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        }
      },
      "auroc": 0.9835833333333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 695,
          "fn": 105,
          "accuracy": 0.86875
        },
        "0.01": {
          "tp": 112,
          "fn": 688,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9733901041666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.9867812499999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9808416666666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9838114583333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.915646875
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9794968749999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 38,
          "fn": 362,
          "accuracy": 0.095
        }
      },
      "auroc": 0.9475718750000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 36,
          "fn": 364,
          "accuracy": 0.09
        }
      },
      "auroc": 0.9512140625000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        }
      },
      "auroc": 0.9801692708333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 654,
          "fn": 146,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 70,
          "fn": 730,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.9656916666666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9807375
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9820708333333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.9814041666666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9830302083333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9854375000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9842338541666668
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9818838541666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.9837541666666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 15,
          "fn": 785,
          "accuracy": 0.01875
        }
      },
      "auroc": 0.9828190104166667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.9717083333333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.9717083333333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9574427083333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9574427083333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 100,
          "fn": 300,
          "accuracy": 0.25
        }
      },
      "auroc": 0.9645755208333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 100,
          "fn": 300,
          "accuracy": 0.25
        }
      },
      "auroc": 0.9645755208333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9447645833333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9447645833333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.8794677083333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.8794677083333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        }
      },
      "auroc": 0.9121161458333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        }
      },
      "auroc": 0.9121161458333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.9832041666666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.9832041666666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.9850958333333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.9850958333333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        }
      },
      "auroc": 0.98415
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        }
      },
      "auroc": 0.98415
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9838510416666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9838510416666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9449052083333332
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9449052083333332
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        }
      },
      "auroc": 0.9643781250000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        }
      },
      "auroc": 0.9643781250000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.974928125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.974928125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.9538093750000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.9538093750000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        }
      },
      "auroc": 0.96436875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        }
      },
      "auroc": 0.96436875
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2068,
          "fn": 132,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 148,
          "fn": 2052,
          "accuracy": 0.06727272727272728
        }
      },
      "auroc": 0.9789163825757576
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1178,
          "fn": 22,
          "accuracy": 0.9816666666666667
        },
        "0.01": {
          "tp": 75,
          "fn": 1125,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.9842107638888888
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3246,
          "fn": 154,
          "accuracy": 0.9547058823529412
        },
        "0.01": {
          "tp": 223,
          "fn": 3177,
          "accuracy": 0.06558823529411764
        }
      },
      "auroc": 0.980784987745098
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1625,
          "fn": 575,
          "accuracy": 0.7386363636363636
        },
        "0.01": {
          "tp": 240,
          "fn": 1960,
          "accuracy": 0.10909090909090909
        }
      },
      "auroc": 0.9525896780303031
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1152,
          "fn": 48,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 81,
          "fn": 1119,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.9823715277777778
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2777,
          "fn": 623,
          "accuracy": 0.816764705882353
        },
        "0.01": {
          "tp": 321,
          "fn": 3079,
          "accuracy": 0.09441176470588235
        }
      },
      "auroc": 0.9631009191176471
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3693,
          "fn": 707,
          "accuracy": 0.8393181818181819
        },
        "0.01": {
          "tp": 388,
          "fn": 4012,
          "accuracy": 0.08818181818181818
        }
      },
      "auroc": 0.9657530303030304
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2330,
          "fn": 70,
          "accuracy": 0.9708333333333333
        },
        "0.01": {
          "tp": 156,
          "fn": 2244,
          "accuracy": 0.065
        }
      },
      "auroc": 0.9832911458333333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6023,
          "fn": 777,
          "accuracy": 0.8857352941176471
        },
        "0.01": {
          "tp": 544,
          "fn": 6256,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9719429534313725
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9899604166666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.9877427083333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        }
      },
      "auroc": 0.9888515625000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9886750000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.9729927083333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        }
      },
      "auroc": 0.9808338541666668
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9893177083333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 52,
          "fn": 348,
          "accuracy": 0.13
        }
      },
      "auroc": 0.9803677083333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        },
        "0.01": {
          "tp": 106,
          "fn": 694,
          "accuracy": 0.1325
        }
      },
      "auroc": 0.9848427083333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.9880895833333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.8902260416666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": {
          "tp": 60,
          "fn": 340,
          "accuracy": 0.15
        }
      },
      "auroc": 0.9391578125
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.9266864583333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.812984375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        },
        "0.01": {
          "tp": 40,
          "fn": 360,
          "accuracy": 0.1
        }
      },
      "auroc": 0.8698354166666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9573880208333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 253,
          "accuracy": 0.3675
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.8516052083333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 419,
          "fn": 381,
          "accuracy": 0.52375
        },
        "0.01": {
          "tp": 100,
          "fn": 700,
          "accuracy": 0.125
        }
      },
      "auroc": 0.9044966145833333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9800322916666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.948728125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9643802083333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.9855812500000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.906678125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        }
      },
      "auroc": 0.9461296875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        }
      },
      "auroc": 0.9828067708333332
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 67,
          "fn": 333,
          "accuracy": 0.1675
        }
      },
      "auroc": 0.9277031250000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 582,
          "fn": 218,
          "accuracy": 0.7275
        },
        "0.01": {
          "tp": 164,
          "fn": 636,
          "accuracy": 0.205
        }
      },
      "auroc": 0.9552549479166668
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.9842875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.9884354166666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 75,
          "fn": 325,
          "accuracy": 0.1875
        }
      },
      "auroc": 0.9863614583333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9282239583333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.8463166666666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        },
        "0.01": {
          "tp": 53,
          "fn": 347,
          "accuracy": 0.1325
        }
      },
      "auroc": 0.8872703125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        },
        "0.01": {
          "tp": 44,
          "fn": 356,
          "accuracy": 0.11
        }
      },
      "auroc": 0.9562557291666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        }
      },
      "auroc": 0.9173760416666668
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 535,
          "fn": 265,
          "accuracy": 0.66875
        },
        "0.01": {
          "tp": 128,
          "fn": 672,
          "accuracy": 0.16
        }
      },
      "auroc": 0.9368158854166667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.9876625000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.940478125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9640703125000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.9055947916666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.7568333333333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.8312140625000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        }
      },
      "auroc": 0.9466286458333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        },
        "0.01": {
          "tp": 72,
          "fn": 328,
          "accuracy": 0.18
        }
      },
      "auroc": 0.8486557291666668
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 429,
          "fn": 371,
          "accuracy": 0.53625
        },
        "0.01": {
          "tp": 127,
          "fn": 673,
          "accuracy": 0.15875
        }
      },
      "auroc": 0.8976421875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.9872281249999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.987415625
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 49,
          "fn": 351,
          "accuracy": 0.1225
        }
      },
      "auroc": 0.987321875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9852770833333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.9741864583333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        }
      },
      "auroc": 0.9797317708333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 36,
          "fn": 364,
          "accuracy": 0.09
        }
      },
      "auroc": 0.9862526041666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        }
      },
      "auroc": 0.9808010416666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 744,
          "fn": 56,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 97,
          "fn": 703,
          "accuracy": 0.12125
        }
      },
      "auroc": 0.9835268229166666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.9329833333333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.9329833333333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.9351739583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.9351739583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": {
          "tp": 75,
          "fn": 325,
          "accuracy": 0.1875
        }
      },
      "auroc": 0.9340786458333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": {
          "tp": 75,
          "fn": 325,
          "accuracy": 0.1875
        }
      },
      "auroc": 0.9340786458333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9277260416666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9277260416666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9071072916666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9071072916666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 58,
          "fn": 342,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9174166666666668
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 58,
          "fn": 342,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9174166666666668
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9858583333333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9858583333333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9838541666666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9838541666666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        }
      },
      "auroc": 0.9848562500000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        }
      },
      "auroc": 0.9848562500000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9813729166666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9813729166666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9650645833333332
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9650645833333332
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        }
      },
      "auroc": 0.97321875
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        }
      },
      "auroc": 0.97321875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.968803125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.968803125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.9598614583333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.9598614583333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.9643322916666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.9643322916666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1918,
          "fn": 282,
          "accuracy": 0.8718181818181818
        },
        "0.01": {
          "tp": 329,
          "fn": 1871,
          "accuracy": 0.14954545454545454
        }
      },
      "auroc": 0.9740003787878788
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 926,
          "fn": 274,
          "accuracy": 0.7716666666666666
        },
        "0.01": {
          "tp": 245,
          "fn": 955,
          "accuracy": 0.20416666666666666
        }
      },
      "auroc": 0.9571710069444443
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2844,
          "fn": 556,
          "accuracy": 0.8364705882352941
        },
        "0.01": {
          "tp": 574,
          "fn": 2826,
          "accuracy": 0.1688235294117647
        }
      },
      "auroc": 0.9680606004901962
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1464,
          "fn": 736,
          "accuracy": 0.6654545454545454
        },
        "0.01": {
          "tp": 329,
          "fn": 1871,
          "accuracy": 0.14954545454545454
        }
      },
      "auroc": 0.9519181818181818
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 587,
          "fn": 613,
          "accuracy": 0.4891666666666667
        },
        "0.01": {
          "tp": 145,
          "fn": 1055,
          "accuracy": 0.12083333333333333
        }
      },
      "auroc": 0.8783319444444444
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2051,
          "fn": 1349,
          "accuracy": 0.6032352941176471
        },
        "0.01": {
          "tp": 474,
          "fn": 2926,
          "accuracy": 0.13941176470588235
        }
      },
      "auroc": 0.925946568627451
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3382,
          "fn": 1018,
          "accuracy": 0.7686363636363637
        },
        "0.01": {
          "tp": 658,
          "fn": 3742,
          "accuracy": 0.14954545454545454
        }
      },
      "auroc": 0.9629592803030304
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1513,
          "fn": 887,
          "accuracy": 0.6304166666666666
        },
        "0.01": {
          "tp": 390,
          "fn": 2010,
          "accuracy": 0.1625
        }
      },
      "auroc": 0.9177514756944445
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4895,
          "fn": 1905,
          "accuracy": 0.7198529411764706
        },
        "0.01": {
          "tp": 1048,
          "fn": 5752,
          "accuracy": 0.15411764705882353
        }
      },
      "auroc": 0.9470035845588235
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9823020833333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9827854166666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.98254375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9828364583333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.98365625
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.9832463541666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9825692708333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9832208333333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 6,
          "fn": 794,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9828950520833334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.9854968749999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9850479166666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9852723958333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.9637010416666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9813729166666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 58,
          "fn": 342,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9725369791666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        }
      },
      "auroc": 0.9745989583333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9832104166666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 717,
          "fn": 83,
          "accuracy": 0.89625
        },
        "0.01": {
          "tp": 64,
          "fn": 736,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9789046874999999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9859760416666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.9840291666666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        }
      },
      "auroc": 0.9850026041666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9864541666666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9862375000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.9863458333333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        }
      },
      "auroc": 0.9862151041666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.9851333333333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 45,
          "fn": 755,
          "accuracy": 0.05625
        }
      },
      "auroc": 0.98567421875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9848854166666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9869166666666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 17,
          "fn": 383,
          "accuracy": 0.0425
        }
      },
      "auroc": 0.9859010416666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.9604739583333335
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.9799729166666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 99,
          "fn": 301,
          "accuracy": 0.2475
        }
      },
      "auroc": 0.9702234375000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        }
      },
      "auroc": 0.9726796875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.9834447916666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 728,
          "fn": 72,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 116,
          "fn": 684,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9780622395833334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.9865635416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.9827541666666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 21,
          "fn": 379,
          "accuracy": 0.0525
        }
      },
      "auroc": 0.9846588541666668
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.942696875
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9818906250000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 52,
          "fn": 348,
          "accuracy": 0.13
        }
      },
      "auroc": 0.96229375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.9646302083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 23,
          "fn": 377,
          "accuracy": 0.0575
        }
      },
      "auroc": 0.9823223958333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 695,
          "fn": 105,
          "accuracy": 0.86875
        },
        "0.01": {
          "tp": 73,
          "fn": 727,
          "accuracy": 0.09125
        }
      },
      "auroc": 0.9734763020833335
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.979271875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9805208333333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9798963541666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9813562499999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.983578125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9824671875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9803140625
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9820494791666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 4,
          "fn": 796,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9811817708333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9759208333333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9759208333333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9644197916666668
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9644197916666668
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        }
      },
      "auroc": 0.9701703125
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        }
      },
      "auroc": 0.9701703125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.9596864583333332
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.9596864583333332
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9033041666666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9033041666666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.9314953125000001
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.9314953125000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.98188125
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.98188125
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.983509375
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.983509375
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9826953125
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9826953125
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.98248125
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.98248125
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.958034375
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.958034375
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9702578125000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9702578125000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.978553125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.978553125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9653020833333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9653020833333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.9719276041666668
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.9719276041666668
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2094,
          "fn": 106,
          "accuracy": 0.9518181818181818
        },
        "0.01": {
          "tp": 152,
          "fn": 2048,
          "accuracy": 0.06909090909090909
        }
      },
      "auroc": 0.9802744318181819
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1186,
          "fn": 14,
          "accuracy": 0.9883333333333333
        },
        "0.01": {
          "tp": 40,
          "fn": 1160,
          "accuracy": 0.03333333333333333
        }
      },
      "auroc": 0.9836756944444445
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3280,
          "fn": 120,
          "accuracy": 0.9647058823529412
        },
        "0.01": {
          "tp": 192,
          "fn": 3208,
          "accuracy": 0.05647058823529412
        }
      },
      "auroc": 0.9814748774509804
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1763,
          "fn": 437,
          "accuracy": 0.8013636363636364
        },
        "0.01": {
          "tp": 308,
          "fn": 1892,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9629171401515151
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1166,
          "fn": 34,
          "accuracy": 0.9716666666666667
        },
        "0.01": {
          "tp": 58,
          "fn": 1142,
          "accuracy": 0.04833333333333333
        }
      },
      "auroc": 0.9827847222222222
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2929,
          "fn": 471,
          "accuracy": 0.8614705882352941
        },
        "0.01": {
          "tp": 366,
          "fn": 3034,
          "accuracy": 0.10764705882352942
        }
      },
      "auroc": 0.9699292279411766
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3857,
          "fn": 543,
          "accuracy": 0.8765909090909091
        },
        "0.01": {
          "tp": 460,
          "fn": 3940,
          "accuracy": 0.10454545454545454
        }
      },
      "auroc": 0.9715957859848484
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2352,
          "fn": 48,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 98,
          "fn": 2302,
          "accuracy": 0.04083333333333333
        }
      },
      "auroc": 0.9832302083333333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6209,
          "fn": 591,
          "accuracy": 0.9130882352941176
        },
        "0.01": {
          "tp": 558,
          "fn": 6242,
          "accuracy": 0.08205882352941177
        }
      },
      "auroc": 0.9757020526960783
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9823020833333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.982759375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.9825307291666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9828187500000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.9836145833333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.9832166666666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9825604166666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9831869791666668
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 6,
          "fn": 794,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9828736979166667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.985315625
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9846791666666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.9849973958333335
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9644843750000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.9811614583333332
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9728229166666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 57,
          "fn": 343,
          "accuracy": 0.1425
        }
      },
      "auroc": 0.9749
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9829203125000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 722,
          "fn": 78,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 61,
          "fn": 739,
          "accuracy": 0.07625
        }
      },
      "auroc": 0.97891015625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.98595
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.98401875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        }
      },
      "auroc": 0.984984375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.986428125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.986215625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.986321875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        }
      },
      "auroc": 0.9861890625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.9851171875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 45,
          "fn": 755,
          "accuracy": 0.05625
        }
      },
      "auroc": 0.985653125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9852666666666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.9866312500000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.9859489583333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9617885416666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9804458333333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 103,
          "fn": 297,
          "accuracy": 0.2575
        }
      },
      "auroc": 0.9711171875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        }
      },
      "auroc": 0.9735276041666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.9835385416666668
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 735,
          "fn": 65,
          "accuracy": 0.91875
        },
        "0.01": {
          "tp": 118,
          "fn": 682,
          "accuracy": 0.1475
        }
      },
      "auroc": 0.9785330729166667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.9862260416666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9823843750000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 17,
          "fn": 383,
          "accuracy": 0.0425
        }
      },
      "auroc": 0.9843052083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.9442875
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.9808125000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        }
      },
      "auroc": 0.96255
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        }
      },
      "auroc": 0.9652567708333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        }
      },
      "auroc": 0.9815984375000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 695,
          "fn": 105,
          "accuracy": 0.86875
        },
        "0.01": {
          "tp": 72,
          "fn": 728,
          "accuracy": 0.09
        }
      },
      "auroc": 0.9734276041666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9792458333333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9805208333333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9798833333333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9813562499999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.983578125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9824671875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9803010416666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9820494791666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 4,
          "fn": 796,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9811752604166667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9765604166666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9765604166666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9646239583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9646239583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9705921875
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9705921875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9606010416666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9606010416666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9039572916666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9039572916666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 90,
          "fn": 310,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9322791666666665
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 90,
          "fn": 310,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9322791666666665
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9818604166666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9818604166666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9834875
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9834875
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9826739583333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9826739583333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9824729166666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9824729166666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.958090625
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.958090625
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9702817708333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9702817708333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9784572916666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9784572916666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9650979166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9650979166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.9717776041666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.9717776041666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2101,
          "fn": 99,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 148,
          "fn": 2052,
          "accuracy": 0.06727272727272728
        }
      },
      "auroc": 0.9803871212121211
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1185,
          "fn": 15,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 37,
          "fn": 1163,
          "accuracy": 0.030833333333333334
        }
      },
      "auroc": 0.9834989583333333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3286,
          "fn": 114,
          "accuracy": 0.9664705882352941
        },
        "0.01": {
          "tp": 185,
          "fn": 3215,
          "accuracy": 0.054411764705882354
        }
      },
      "auroc": 0.9814854166666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1774,
          "fn": 426,
          "accuracy": 0.8063636363636364
        },
        "0.01": {
          "tp": 306,
          "fn": 1894,
          "accuracy": 0.1390909090909091
        }
      },
      "auroc": 0.9633109848484849
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1165,
          "fn": 35,
          "accuracy": 0.9708333333333333
        },
        "0.01": {
          "tp": 63,
          "fn": 1137,
          "accuracy": 0.0525
        }
      },
      "auroc": 0.9826380208333334
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2939,
          "fn": 461,
          "accuracy": 0.8644117647058823
        },
        "0.01": {
          "tp": 369,
          "fn": 3031,
          "accuracy": 0.10852941176470589
        }
      },
      "auroc": 0.9701322916666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3875,
          "fn": 525,
          "accuracy": 0.8806818181818182
        },
        "0.01": {
          "tp": 454,
          "fn": 3946,
          "accuracy": 0.10318181818181818
        }
      },
      "auroc": 0.971849053030303
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2350,
          "fn": 50,
          "accuracy": 0.9791666666666666
        },
        "0.01": {
          "tp": 100,
          "fn": 2300,
          "accuracy": 0.041666666666666664
        }
      },
      "auroc": 0.9830684895833335
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6225,
          "fn": 575,
          "accuracy": 0.9154411764705882
        },
        "0.01": {
          "tp": 554,
          "fn": 6246,
          "accuracy": 0.08147058823529411
        }
      },
      "auroc": 0.9758088541666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7468895833333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5658177083333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6563536458333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7223958333333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4827489583333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6025723958333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7346427083333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5242833333333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 799,
          "accuracy": 0.00125
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6294630208333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9671947916666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.9902166666666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 49,
          "fn": 351,
          "accuracy": 0.1225
        }
      },
      "auroc": 0.9787057291666665
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6962854166666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9821854166666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        }
      },
      "auroc": 0.8392354166666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 276,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        }
      },
      "auroc": 0.8317401041666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 49,
          "fn": 351,
          "accuracy": 0.1225
        }
      },
      "auroc": 0.9862010416666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 506,
          "fn": 294,
          "accuracy": 0.6325
        },
        "0.01": {
          "tp": 77,
          "fn": 723,
          "accuracy": 0.09625
        }
      },
      "auroc": 0.9089705729166667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.612734375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.522375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.5675546875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5810989583333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.599978125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 371,
          "accuracy": 0.0725
        },
        "0.01": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        }
      },
      "auroc": 0.5905385416666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5969166666666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        },
        "0.01": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        }
      },
      "auroc": 0.5611765624999999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 765,
          "accuracy": 0.04375
        },
        "0.01": {
          "tp": 16,
          "fn": 784,
          "accuracy": 0.02
        }
      },
      "auroc": 0.5790466145833333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.9900687499999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.7699739583333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.8800213541666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6837916666666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.5048052083333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.5942984375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.8369302083333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.6373895833333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 587,
          "accuracy": 0.26625
        },
        "0.01": {
          "tp": 28,
          "fn": 772,
          "accuracy": 0.035
        }
      },
      "auroc": 0.7371598958333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.9714552083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.6679875
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        }
      },
      "auroc": 0.8197213541666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6442958333333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.6943677083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.6693317708333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.8078755208333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.6811776041666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 610,
          "accuracy": 0.2375
        },
        "0.01": {
          "tp": 74,
          "fn": 726,
          "accuracy": 0.0925
        }
      },
      "auroc": 0.7445265624999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7278625
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.563559375
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6457109375
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6514281250000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.40975520833333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5305916666666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6896453125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.48665729166666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5881513020833333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7490635416666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7490635416666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6820802083333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6820802083333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.715571875
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.715571875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6364416666666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6364416666666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5792677083333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5792677083333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6078546874999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6078546874999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6039406249999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6039406249999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5492708333333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5492708333333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5766057291666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5766057291666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6187989583333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6187989583333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.3850541666666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.3850541666666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5019265625
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5019265625
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6933572916666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6933572916666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6354697916666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6354697916666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6644135416666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6644135416666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 458,
          "fn": 1742,
          "accuracy": 0.2081818181818182
        },
        "0.01": {
          "tp": 101,
          "fn": 2099,
          "accuracy": 0.045909090909090906
        }
      },
      "auroc": 0.7561642992424242
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 957,
          "accuracy": 0.2025
        },
        "0.01": {
          "tp": 37,
          "fn": 1163,
          "accuracy": 0.030833333333333334
        }
      },
      "auroc": 0.6799883680555556
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 701,
          "fn": 2699,
          "accuracy": 0.2061764705882353
        },
        "0.01": {
          "tp": 138,
          "fn": 3262,
          "accuracy": 0.04058823529411765
        }
      },
      "auroc": 0.7292786764705882
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 2197,
          "accuracy": 0.0013636363636363637
        },
        "0.01": {
          "tp": 0,
          "fn": 2200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6191307765151515
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 950,
          "accuracy": 0.20833333333333334
        },
        "0.01": {
          "tp": 58,
          "fn": 1142,
          "accuracy": 0.04833333333333333
        }
      },
      "auroc": 0.6123067708333334
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 253,
          "fn": 3147,
          "accuracy": 0.07441176470588236
        },
        "0.01": {
          "tp": 58,
          "fn": 3342,
          "accuracy": 0.017058823529411765
        }
      },
      "auroc": 0.6167223039215686
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 461,
          "fn": 3939,
          "accuracy": 0.10477272727272727
        },
        "0.01": {
          "tp": 101,
          "fn": 4299,
          "accuracy": 0.022954545454545453
        }
      },
      "auroc": 0.6876475378787879
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 493,
          "fn": 1907,
          "accuracy": 0.20541666666666666
        },
        "0.01": {
          "tp": 95,
          "fn": 2305,
          "accuracy": 0.03958333333333333
        }
      },
      "auroc": 0.6461475694444444
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 954,
          "fn": 5846,
          "accuracy": 0.14029411764705882
        },
        "0.01": {
          "tp": 196,
          "fn": 6604,
          "accuracy": 0.028823529411764706
        }
      },
      "auroc": 0.6730004901960784
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.9879739583333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.987440625
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 36,
          "fn": 364,
          "accuracy": 0.09
        }
      },
      "auroc": 0.9877072916666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9890281249999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.9869114583333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        }
      },
      "auroc": 0.9879697916666668
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        }
      },
      "auroc": 0.9885010416666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 49,
          "fn": 351,
          "accuracy": 0.1225
        }
      },
      "auroc": 0.9871760416666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": {
          "tp": 97,
          "fn": 703,
          "accuracy": 0.12125
        }
      },
      "auroc": 0.9878385416666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.988925
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9850697916666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9869973958333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.8819989583333332
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.9812645833333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        }
      },
      "auroc": 0.9316317708333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 44,
          "fn": 356,
          "accuracy": 0.11
        }
      },
      "auroc": 0.9354619791666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9831671875000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 634,
          "fn": 166,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 48,
          "fn": 752,
          "accuracy": 0.06
        }
      },
      "auroc": 0.9593145833333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.9884552083333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9843302083333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 65,
          "fn": 335,
          "accuracy": 0.1625
        }
      },
      "auroc": 0.9863927083333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.9881104166666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.9870166666666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 42,
          "fn": 358,
          "accuracy": 0.105
        }
      },
      "auroc": 0.9875635416666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 58,
          "fn": 342,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9882828125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 49,
          "fn": 351,
          "accuracy": 0.1225
        }
      },
      "auroc": 0.9856734375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 772,
          "fn": 28,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 107,
          "fn": 693,
          "accuracy": 0.13375
        }
      },
      "auroc": 0.986978125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9864958333333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.98860625
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.9875510416666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.8800781249999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.9791541666666668
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 41,
          "fn": 359,
          "accuracy": 0.1025
        }
      },
      "auroc": 0.9296161458333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        }
      },
      "auroc": 0.9332869791666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9838802083333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 641,
          "fn": 159,
          "accuracy": 0.80125
        },
        "0.01": {
          "tp": 76,
          "fn": 724,
          "accuracy": 0.095
        }
      },
      "auroc": 0.95858359375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9898885416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.97935625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 52,
          "fn": 348,
          "accuracy": 0.13
        }
      },
      "auroc": 0.9846223958333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.8316979166666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.9813177083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        }
      },
      "auroc": 0.9065078125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.9107932291666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.9803369791666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 604,
          "fn": 196,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 70,
          "fn": 730,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.9455651041666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.983759375
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9844979166666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        }
      },
      "auroc": 0.9841286458333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9866479166666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.9860520833333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.9863500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        }
      },
      "auroc": 0.9852036458333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        }
      },
      "auroc": 0.9852750000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 44,
          "fn": 756,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9852393229166667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9354875
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9354875
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.9091364583333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.9091364583333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9223119791666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9223119791666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.8478937500000001
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.8478937500000001
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.7448302083333332
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.7448302083333332
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        },
        "0.01": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        }
      },
      "auroc": 0.7963619791666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        },
        "0.01": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        }
      },
      "auroc": 0.7963619791666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9865833333333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9865833333333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9852145833333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9852145833333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.9858989583333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.9858989583333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.9803239583333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.9803239583333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8090614583333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8090614583333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        }
      },
      "auroc": 0.8946927083333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        }
      },
      "auroc": 0.8946927083333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.953065625
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.953065625
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.9215864583333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.9215864583333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.9373260416666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.9373260416666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1919,
          "fn": 281,
          "accuracy": 0.8722727272727273
        },
        "0.01": {
          "tp": 229,
          "fn": 1971,
          "accuracy": 0.1040909090909091
        }
      },
      "auroc": 0.9662592803030303
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1171,
          "fn": 29,
          "accuracy": 0.9758333333333333
        },
        "0.01": {
          "tp": 99,
          "fn": 1101,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.9848835069444445
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3090,
          "fn": 310,
          "accuracy": 0.9088235294117647
        },
        "0.01": {
          "tp": 328,
          "fn": 3072,
          "accuracy": 0.09647058823529411
        }
      },
      "auroc": 0.9728325367647059
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1217,
          "fn": 983,
          "accuracy": 0.5531818181818182
        },
        "0.01": {
          "tp": 204,
          "fn": 1996,
          "accuracy": 0.09272727272727273
        }
      },
      "auroc": 0.9024900568181818
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1165,
          "fn": 35,
          "accuracy": 0.9708333333333333
        },
        "0.01": {
          "tp": 100,
          "fn": 1100,
          "accuracy": 0.08333333333333333
        }
      },
      "auroc": 0.9836194444444445
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2382,
          "fn": 1018,
          "accuracy": 0.7005882352941176
        },
        "0.01": {
          "tp": 304,
          "fn": 3096,
          "accuracy": 0.08941176470588236
        }
      },
      "auroc": 0.9311239583333333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3136,
          "fn": 1264,
          "accuracy": 0.7127272727272728
        },
        "0.01": {
          "tp": 433,
          "fn": 3967,
          "accuracy": 0.0984090909090909
        }
      },
      "auroc": 0.934374668560606
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2336,
          "fn": 64,
          "accuracy": 0.9733333333333334
        },
        "0.01": {
          "tp": 199,
          "fn": 2201,
          "accuracy": 0.08291666666666667
        }
      },
      "auroc": 0.9842514756944445
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5472,
          "fn": 1328,
          "accuracy": 0.8047058823529412
        },
        "0.01": {
          "tp": 632,
          "fn": 6168,
          "accuracy": 0.09294117647058824
        }
      },
      "auroc": 0.9519782475490197
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9832385416666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9837604166666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9834994791666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9838177083333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9841885416666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        }
      },
      "auroc": 0.9840031250000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.983528125
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        }
      },
      "auroc": 0.9839744791666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 11,
          "fn": 789,
          "accuracy": 0.01375
        }
      },
      "auroc": 0.9837513020833334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9858239583333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9851041666666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        }
      },
      "auroc": 0.9854640625000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.9610000000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9812770833333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.9711385416666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9734119791666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9831906250000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 715,
          "fn": 85,
          "accuracy": 0.89375
        },
        "0.01": {
          "tp": 57,
          "fn": 743,
          "accuracy": 0.07125
        }
      },
      "auroc": 0.9783013020833333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.98679375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.98383125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9853125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.98733125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.986425
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        }
      },
      "auroc": 0.986878125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        }
      },
      "auroc": 0.9870625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 40,
          "fn": 360,
          "accuracy": 0.1
        }
      },
      "auroc": 0.985128125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 60,
          "fn": 740,
          "accuracy": 0.075
        }
      },
      "auroc": 0.9860953125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9853302083333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.9873614583333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        }
      },
      "auroc": 0.9863458333333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9572125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.9802989583333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 99,
          "fn": 301,
          "accuracy": 0.2475
        }
      },
      "auroc": 0.9687557291666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        }
      },
      "auroc": 0.9712713541666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 40,
          "fn": 360,
          "accuracy": 0.1
        }
      },
      "auroc": 0.9838302083333335
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 725,
          "fn": 75,
          "accuracy": 0.90625
        },
        "0.01": {
          "tp": 117,
          "fn": 683,
          "accuracy": 0.14625
        }
      },
      "auroc": 0.9775507812500002
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.98650625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.9829749999999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        }
      },
      "auroc": 0.9847406249999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9381635416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9810458333333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        }
      },
      "auroc": 0.9596046875000002
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": {
          "tp": 42,
          "fn": 358,
          "accuracy": 0.105
        }
      },
      "auroc": 0.9623348958333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.9820104166666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 687,
          "fn": 113,
          "accuracy": 0.85875
        },
        "0.01": {
          "tp": 67,
          "fn": 733,
          "accuracy": 0.08375
        }
      },
      "auroc": 0.97217265625
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9802552083333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9818885416666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.981071875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9823125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.9845531249999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9834328125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.9812838541666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.9832208333333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 6,
          "fn": 794,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9822523437499999
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9755489583333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9755489583333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9621770833333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9621770833333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 100,
          "fn": 300,
          "accuracy": 0.25
        }
      },
      "auroc": 0.9688630208333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 100,
          "fn": 300,
          "accuracy": 0.25
        }
      },
      "auroc": 0.9688630208333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.9544447916666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.9544447916666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.8938906249999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.8938906249999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9241677083333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9241677083333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9827520833333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9827520833333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.984521875
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.984521875
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9836369791666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9836369791666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9833468750000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9833468750000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.9468541666666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.9468541666666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        }
      },
      "auroc": 0.9651005208333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        }
      },
      "auroc": 0.9651005208333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.9758093750000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.9758093750000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9639145833333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9639145833333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.9698619791666668
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.9698619791666668
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2093,
          "fn": 107,
          "accuracy": 0.9513636363636364
        },
        "0.01": {
          "tp": 155,
          "fn": 2045,
          "accuracy": 0.07045454545454545
        }
      },
      "auroc": 0.9799863636363636
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1183,
          "fn": 17,
          "accuracy": 0.9858333333333333
        },
        "0.01": {
          "tp": 55,
          "fn": 1145,
          "accuracy": 0.04583333333333333
        }
      },
      "auroc": 0.9841534722222223
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3276,
          "fn": 124,
          "accuracy": 0.9635294117647059
        },
        "0.01": {
          "tp": 210,
          "fn": 3190,
          "accuracy": 0.061764705882352944
        }
      },
      "auroc": 0.9814571078431372
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1725,
          "fn": 475,
          "accuracy": 0.7840909090909091
        },
        "0.01": {
          "tp": 295,
          "fn": 1905,
          "accuracy": 0.1340909090909091
        }
      },
      "auroc": 0.9601087121212122
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1166,
          "fn": 34,
          "accuracy": 0.9716666666666667
        },
        "0.01": {
          "tp": 66,
          "fn": 1134,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9829647569444444
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2891,
          "fn": 509,
          "accuracy": 0.8502941176470589
        },
        "0.01": {
          "tp": 361,
          "fn": 3039,
          "accuracy": 0.10617647058823529
        }
      },
      "auroc": 0.9681755514705882
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3818,
          "fn": 582,
          "accuracy": 0.8677272727272727
        },
        "0.01": {
          "tp": 450,
          "fn": 3950,
          "accuracy": 0.10227272727272728
        }
      },
      "auroc": 0.9700475378787878
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2349,
          "fn": 51,
          "accuracy": 0.97875
        },
        "0.01": {
          "tp": 121,
          "fn": 2279,
          "accuracy": 0.050416666666666665
        }
      },
      "auroc": 0.9835591145833333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6167,
          "fn": 633,
          "accuracy": 0.9069117647058823
        },
        "0.01": {
          "tp": 571,
          "fn": 6229,
          "accuracy": 0.08397058823529412
        }
      },
      "auroc": 0.9748163296568628
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8194635416666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8185229166666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8189932291666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.81485625
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.81530625
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8150812500000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8171598958333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8169145833333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8170372395833334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7740947916666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8389697916666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8065322916666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7935604166666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8376572916666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8156088541666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7838276041666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8383135416666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8110705729166666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8017354166666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8028072916666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8022713541666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8047552083333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8125385416666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8086468750000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8032453125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8076729166666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8054591145833333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7941291666666668
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8195885416666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8068588541666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8063489583333332
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8160614583333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8112052083333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8002390625000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.817825
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.80903203125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7606364583333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7717083333333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7661723958333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.794275
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.81281875
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.803546875
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7774557291666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7922635416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7848596354166667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8193375
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8193458333333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8193416666666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8218760416666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8208864583333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.82138125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8206067708333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8201161458333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8203614583333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7623666666666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7623666666666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7613635416666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7613635416666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7618651041666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7618651041666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.774534375
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.774534375
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7854822916666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7854822916666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7800083333333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7800083333333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7945114583333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7945114583333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.791915625
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.791915625
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7932135416666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7932135416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7846010416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7846010416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7917635416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7917635416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7881822916666668
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7881822916666668
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8076458333333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8076458333333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.805171875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.805171875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8064088541666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8064088541666666
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 2200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 2200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7902778409090909
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8118237847222223
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 3400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 3400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7978822916666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 2200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 2200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.797397159090909
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8192114583333333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 3400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 3400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8050963235294117
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 4400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 4400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7938375
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 2400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 2400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8155176215277778
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 6800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 6800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8014893075980392
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1999,
          "fn": 401,
          "accuracy": 0.8329166666666666
        },
        "0.01": {
          "tp": 104,
          "fn": 2296,
          "accuracy": 0.043333333333333335
        }
      },
      "auroc": 0.9517491319444444
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1989,
          "fn": 411,
          "accuracy": 0.82875
        },
        "0.01": {
          "tp": 127,
          "fn": 2273,
          "accuracy": 0.05291666666666667
        }
      },
      "auroc": 0.9368296875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3988,
          "fn": 812,
          "accuracy": 0.8308333333333333
        },
        "0.01": {
          "tp": 231,
          "fn": 4569,
          "accuracy": 0.048125
        }
      },
      "auroc": 0.9442894097222223
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1996,
          "fn": 404,
          "accuracy": 0.8316666666666667
        },
        "0.01": {
          "tp": 138,
          "fn": 2262,
          "accuracy": 0.0575
        }
      },
      "auroc": 0.9497575520833332
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1929,
          "fn": 471,
          "accuracy": 0.80375
        },
        "0.01": {
          "tp": 225,
          "fn": 2175,
          "accuracy": 0.09375
        }
      },
      "auroc": 0.927647829861111
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3925,
          "fn": 875,
          "accuracy": 0.8177083333333334
        },
        "0.01": {
          "tp": 363,
          "fn": 4437,
          "accuracy": 0.075625
        }
      },
      "auroc": 0.9387026909722223
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3995,
          "fn": 805,
          "accuracy": 0.8322916666666667
        },
        "0.01": {
          "tp": 242,
          "fn": 4558,
          "accuracy": 0.050416666666666665
        }
      },
      "auroc": 0.950753342013889
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3918,
          "fn": 882,
          "accuracy": 0.81625
        },
        "0.01": {
          "tp": 352,
          "fn": 4448,
          "accuracy": 0.07333333333333333
        }
      },
      "auroc": 0.9322387586805555
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7913,
          "fn": 1687,
          "accuracy": 0.8242708333333333
        },
        "0.01": {
          "tp": 594,
          "fn": 9006,
          "accuracy": 0.061875
        }
      },
      "auroc": 0.9414960503472221
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2111,
          "fn": 289,
          "accuracy": 0.8795833333333334
        },
        "0.01": {
          "tp": 178,
          "fn": 2222,
          "accuracy": 0.07416666666666667
        }
      },
      "auroc": 0.9675986111111111
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2082,
          "fn": 318,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 57,
          "fn": 2343,
          "accuracy": 0.02375
        }
      },
      "auroc": 0.9654711805555556
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4193,
          "fn": 607,
          "accuracy": 0.8735416666666667
        },
        "0.01": {
          "tp": 235,
          "fn": 4565,
          "accuracy": 0.04895833333333333
        }
      },
      "auroc": 0.9665348958333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 960,
          "fn": 1440,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 360,
          "fn": 2040,
          "accuracy": 0.15
        }
      },
      "auroc": 0.9082745659722223
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1992,
          "fn": 408,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 84,
          "fn": 2316,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9554434027777777
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2952,
          "fn": 1848,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 444,
          "fn": 4356,
          "accuracy": 0.0925
        }
      },
      "auroc": 0.931858984375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3071,
          "fn": 1729,
          "accuracy": 0.6397916666666666
        },
        "0.01": {
          "tp": 538,
          "fn": 4262,
          "accuracy": 0.11208333333333333
        }
      },
      "auroc": 0.9379365885416666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4074,
          "fn": 726,
          "accuracy": 0.84875
        },
        "0.01": {
          "tp": 141,
          "fn": 4659,
          "accuracy": 0.029375
        }
      },
      "auroc": 0.9604572916666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7145,
          "fn": 2455,
          "accuracy": 0.7442708333333333
        },
        "0.01": {
          "tp": 679,
          "fn": 8921,
          "accuracy": 0.07072916666666666
        }
      },
      "auroc": 0.9491969401041666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1958,
          "fn": 442,
          "accuracy": 0.8158333333333333
        },
        "0.01": {
          "tp": 260,
          "fn": 2140,
          "accuracy": 0.10833333333333334
        }
      },
      "auroc": 0.9402741319444444
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1873,
          "fn": 527,
          "accuracy": 0.7804166666666666
        },
        "0.01": {
          "tp": 269,
          "fn": 2131,
          "accuracy": 0.11208333333333333
        }
      },
      "auroc": 0.9277447916666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3831,
          "fn": 969,
          "accuracy": 0.798125
        },
        "0.01": {
          "tp": 529,
          "fn": 4271,
          "accuracy": 0.11020833333333334
        }
      },
      "auroc": 0.9340094618055556
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1948,
          "fn": 452,
          "accuracy": 0.8116666666666666
        },
        "0.01": {
          "tp": 264,
          "fn": 2136,
          "accuracy": 0.11
        }
      },
      "auroc": 0.9383717013888889
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1892,
          "fn": 508,
          "accuracy": 0.7883333333333333
        },
        "0.01": {
          "tp": 212,
          "fn": 2188,
          "accuracy": 0.08833333333333333
        }
      },
      "auroc": 0.9329337673611111
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3840,
          "fn": 960,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 476,
          "fn": 4324,
          "accuracy": 0.09916666666666667
        }
      },
      "auroc": 0.9356527343750001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3906,
          "fn": 894,
          "accuracy": 0.81375
        },
        "0.01": {
          "tp": 524,
          "fn": 4276,
          "accuracy": 0.10916666666666666
        }
      },
      "auroc": 0.9393229166666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3765,
          "fn": 1035,
          "accuracy": 0.784375
        },
        "0.01": {
          "tp": 481,
          "fn": 4319,
          "accuracy": 0.10020833333333333
        }
      },
      "auroc": 0.930339279513889
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7671,
          "fn": 1929,
          "accuracy": 0.7990625
        },
        "0.01": {
          "tp": 1005,
          "fn": 8595,
          "accuracy": 0.1046875
        }
      },
      "auroc": 0.9348310980902778
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2183,
          "fn": 217,
          "accuracy": 0.9095833333333333
        },
        "0.01": {
          "tp": 68,
          "fn": 2332,
          "accuracy": 0.028333333333333332
        }
      },
      "auroc": 0.9699972222222222
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1993,
          "fn": 407,
          "accuracy": 0.8304166666666667
        },
        "0.01": {
          "tp": 323,
          "fn": 2077,
          "accuracy": 0.13458333333333333
        }
      },
      "auroc": 0.956340798611111
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4176,
          "fn": 624,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 391,
          "fn": 4409,
          "accuracy": 0.08145833333333333
        }
      },
      "auroc": 0.9631690104166668
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1166,
          "fn": 1234,
          "accuracy": 0.48583333333333334
        },
        "0.01": {
          "tp": 562,
          "fn": 1838,
          "accuracy": 0.23416666666666666
        }
      },
      "auroc": 0.9066190104166667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1714,
          "fn": 686,
          "accuracy": 0.7141666666666666
        },
        "0.01": {
          "tp": 266,
          "fn": 2134,
          "accuracy": 0.11083333333333334
        }
      },
      "auroc": 0.9151407118055557
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2880,
          "fn": 1920,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 828,
          "fn": 3972,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.9108798611111111
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3349,
          "fn": 1451,
          "accuracy": 0.6977083333333334
        },
        "0.01": {
          "tp": 630,
          "fn": 4170,
          "accuracy": 0.13125
        }
      },
      "auroc": 0.9383081163194444
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3707,
          "fn": 1093,
          "accuracy": 0.7722916666666667
        },
        "0.01": {
          "tp": 589,
          "fn": 4211,
          "accuracy": 0.12270833333333334
        }
      },
      "auroc": 0.9357407552083332
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7056,
          "fn": 2544,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 1219,
          "fn": 8381,
          "accuracy": 0.12697916666666667
        }
      },
      "auroc": 0.9370244357638889
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2124,
          "fn": 276,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 222,
          "fn": 2178,
          "accuracy": 0.0925
        }
      },
      "auroc": 0.96740078125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1850,
          "fn": 550,
          "accuracy": 0.7708333333333334
        },
        "0.01": {
          "tp": 233,
          "fn": 2167,
          "accuracy": 0.09708333333333333
        }
      },
      "auroc": 0.9346400173611111
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3974,
          "fn": 826,
          "accuracy": 0.8279166666666666
        },
        "0.01": {
          "tp": 455,
          "fn": 4345,
          "accuracy": 0.09479166666666666
        }
      },
      "auroc": 0.9510203993055557
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 814,
          "fn": 1586,
          "accuracy": 0.33916666666666667
        },
        "0.01": {
          "tp": 281,
          "fn": 2119,
          "accuracy": 0.11708333333333333
        }
      },
      "auroc": 0.8818880208333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1802,
          "fn": 598,
          "accuracy": 0.7508333333333334
        },
        "0.01": {
          "tp": 139,
          "fn": 2261,
          "accuracy": 0.057916666666666665
        }
      },
      "auroc": 0.9245730034722223
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2616,
          "fn": 2184,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 420,
          "fn": 4380,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.9032305121527777
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2938,
          "fn": 1862,
          "accuracy": 0.6120833333333333
        },
        "0.01": {
          "tp": 503,
          "fn": 4297,
          "accuracy": 0.10479166666666667
        }
      },
      "auroc": 0.9246444010416666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3652,
          "fn": 1148,
          "accuracy": 0.7608333333333334
        },
        "0.01": {
          "tp": 372,
          "fn": 4428,
          "accuracy": 0.0775
        }
      },
      "auroc": 0.9296065104166666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6590,
          "fn": 3010,
          "accuracy": 0.6864583333333333
        },
        "0.01": {
          "tp": 875,
          "fn": 8725,
          "accuracy": 0.09114583333333333
        }
      },
      "auroc": 0.9271254557291667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1987,
          "fn": 413,
          "accuracy": 0.8279166666666666
        },
        "0.01": {
          "tp": 53,
          "fn": 2347,
          "accuracy": 0.022083333333333333
        }
      },
      "auroc": 0.9477163194444445
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1994,
          "fn": 406,
          "accuracy": 0.8308333333333333
        },
        "0.01": {
          "tp": 77,
          "fn": 2323,
          "accuracy": 0.03208333333333333
        }
      },
      "auroc": 0.9349063368055556
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3981,
          "fn": 819,
          "accuracy": 0.829375
        },
        "0.01": {
          "tp": 130,
          "fn": 4670,
          "accuracy": 0.027083333333333334
        }
      },
      "auroc": 0.9413113281250001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1988,
          "fn": 412,
          "accuracy": 0.8283333333333334
        },
        "0.01": {
          "tp": 106,
          "fn": 2294,
          "accuracy": 0.04416666666666667
        }
      },
      "auroc": 0.94303828125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1949,
          "fn": 451,
          "accuracy": 0.8120833333333334
        },
        "0.01": {
          "tp": 184,
          "fn": 2216,
          "accuracy": 0.07666666666666666
        }
      },
      "auroc": 0.9225171875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3937,
          "fn": 863,
          "accuracy": 0.8202083333333333
        },
        "0.01": {
          "tp": 290,
          "fn": 4510,
          "accuracy": 0.06041666666666667
        }
      },
      "auroc": 0.9327777343750001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3975,
          "fn": 825,
          "accuracy": 0.828125
        },
        "0.01": {
          "tp": 159,
          "fn": 4641,
          "accuracy": 0.033125
        }
      },
      "auroc": 0.9453773003472223
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3943,
          "fn": 857,
          "accuracy": 0.8214583333333333
        },
        "0.01": {
          "tp": 261,
          "fn": 4539,
          "accuracy": 0.054375
        }
      },
      "auroc": 0.9287117621527777
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7918,
          "fn": 1682,
          "accuracy": 0.8247916666666667
        },
        "0.01": {
          "tp": 420,
          "fn": 9180,
          "accuracy": 0.04375
        }
      },
      "auroc": 0.9370445312500001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1488,
          "fn": 912,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 501,
          "fn": 1899,
          "accuracy": 0.20875
        }
      },
      "auroc": 0.9289804687500001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1488,
          "fn": 912,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 501,
          "fn": 1899,
          "accuracy": 0.20875
        }
      },
      "auroc": 0.9289804687500001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1312,
          "fn": 1088,
          "accuracy": 0.5466666666666666
        },
        "0.01": {
          "tp": 466,
          "fn": 1934,
          "accuracy": 0.19416666666666665
        }
      },
      "auroc": 0.9105591145833333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1312,
          "fn": 1088,
          "accuracy": 0.5466666666666666
        },
        "0.01": {
          "tp": 466,
          "fn": 1934,
          "accuracy": 0.19416666666666665
        }
      },
      "auroc": 0.9105591145833333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2800,
          "fn": 2000,
          "accuracy": 0.5833333333333334
        },
        "0.01": {
          "tp": 967,
          "fn": 3833,
          "accuracy": 0.20145833333333332
        }
      },
      "auroc": 0.9197697916666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2800,
          "fn": 2000,
          "accuracy": 0.5833333333333334
        },
        "0.01": {
          "tp": 967,
          "fn": 3833,
          "accuracy": 0.20145833333333332
        }
      },
      "auroc": 0.9197697916666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1196,
          "fn": 1204,
          "accuracy": 0.49833333333333335
        },
        "0.01": {
          "tp": 404,
          "fn": 1996,
          "accuracy": 0.16833333333333333
        }
      },
      "auroc": 0.8945086805555555
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1196,
          "fn": 1204,
          "accuracy": 0.49833333333333335
        },
        "0.01": {
          "tp": 404,
          "fn": 1996,
          "accuracy": 0.16833333333333333
        }
      },
      "auroc": 0.8945086805555555
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 802,
          "fn": 1598,
          "accuracy": 0.33416666666666667
        },
        "0.01": {
          "tp": 272,
          "fn": 2128,
          "accuracy": 0.11333333333333333
        }
      },
      "auroc": 0.8388927083333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 802,
          "fn": 1598,
          "accuracy": 0.33416666666666667
        },
        "0.01": {
          "tp": 272,
          "fn": 2128,
          "accuracy": 0.11333333333333333
        }
      },
      "auroc": 0.8388927083333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1998,
          "fn": 2802,
          "accuracy": 0.41625
        },
        "0.01": {
          "tp": 676,
          "fn": 4124,
          "accuracy": 0.14083333333333334
        }
      },
      "auroc": 0.8667006944444444
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1998,
          "fn": 2802,
          "accuracy": 0.41625
        },
        "0.01": {
          "tp": 676,
          "fn": 4124,
          "accuracy": 0.14083333333333334
        }
      },
      "auroc": 0.8667006944444444
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1984,
          "fn": 416,
          "accuracy": 0.8266666666666667
        },
        "0.01": {
          "tp": 114,
          "fn": 2286,
          "accuracy": 0.0475
        }
      },
      "auroc": 0.9370291666666668
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1984,
          "fn": 416,
          "accuracy": 0.8266666666666667
        },
        "0.01": {
          "tp": 114,
          "fn": 2286,
          "accuracy": 0.0475
        }
      },
      "auroc": 0.9370291666666668
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1951,
          "fn": 449,
          "accuracy": 0.8129166666666666
        },
        "0.01": {
          "tp": 133,
          "fn": 2267,
          "accuracy": 0.05541666666666667
        }
      },
      "auroc": 0.9327121527777777
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1951,
          "fn": 449,
          "accuracy": 0.8129166666666666
        },
        "0.01": {
          "tp": 133,
          "fn": 2267,
          "accuracy": 0.05541666666666667
        }
      },
      "auroc": 0.9327121527777777
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3935,
          "fn": 865,
          "accuracy": 0.8197916666666667
        },
        "0.01": {
          "tp": 247,
          "fn": 4553,
          "accuracy": 0.051458333333333335
        }
      },
      "auroc": 0.9348706597222222
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3935,
          "fn": 865,
          "accuracy": 0.8197916666666667
        },
        "0.01": {
          "tp": 247,
          "fn": 4553,
          "accuracy": 0.051458333333333335
        }
      },
      "auroc": 0.9348706597222222
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1960,
          "fn": 440,
          "accuracy": 0.8166666666666667
        },
        "0.01": {
          "tp": 107,
          "fn": 2293,
          "accuracy": 0.044583333333333336
        }
      },
      "auroc": 0.9368010416666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1960,
          "fn": 440,
          "accuracy": 0.8166666666666667
        },
        "0.01": {
          "tp": 107,
          "fn": 2293,
          "accuracy": 0.044583333333333336
        }
      },
      "auroc": 0.9368010416666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1290,
          "fn": 1110,
          "accuracy": 0.5375
        },
        "0.01": {
          "tp": 72,
          "fn": 2328,
          "accuracy": 0.03
        }
      },
      "auroc": 0.8703796874999999
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1290,
          "fn": 1110,
          "accuracy": 0.5375
        },
        "0.01": {
          "tp": 72,
          "fn": 2328,
          "accuracy": 0.03
        }
      },
      "auroc": 0.8703796874999999
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3250,
          "fn": 1550,
          "accuracy": 0.6770833333333334
        },
        "0.01": {
          "tp": 179,
          "fn": 4621,
          "accuracy": 0.03729166666666667
        }
      },
      "auroc": 0.9035903645833334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3250,
          "fn": 1550,
          "accuracy": 0.6770833333333334
        },
        "0.01": {
          "tp": 179,
          "fn": 4621,
          "accuracy": 0.03729166666666667
        }
      },
      "auroc": 0.9035903645833334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1798,
          "fn": 602,
          "accuracy": 0.7491666666666666
        },
        "0.01": {
          "tp": 275,
          "fn": 2125,
          "accuracy": 0.11458333333333333
        }
      },
      "auroc": 0.9354486979166666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1798,
          "fn": 602,
          "accuracy": 0.7491666666666666
        },
        "0.01": {
          "tp": 275,
          "fn": 2125,
          "accuracy": 0.11458333333333333
        }
      },
      "auroc": 0.9354486979166666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1570,
          "fn": 830,
          "accuracy": 0.6541666666666667
        },
        "0.01": {
          "tp": 344,
          "fn": 2056,
          "accuracy": 0.14333333333333334
        }
      },
      "auroc": 0.9146848090277778
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1570,
          "fn": 830,
          "accuracy": 0.6541666666666667
        },
        "0.01": {
          "tp": 344,
          "fn": 2056,
          "accuracy": 0.14333333333333334
        }
      },
      "auroc": 0.9146848090277778
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3368,
          "fn": 1432,
          "accuracy": 0.7016666666666667
        },
        "0.01": {
          "tp": 619,
          "fn": 4181,
          "accuracy": 0.12895833333333334
        }
      },
      "auroc": 0.9250667534722221
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3368,
          "fn": 1432,
          "accuracy": 0.7016666666666667
        },
        "0.01": {
          "tp": 619,
          "fn": 4181,
          "accuracy": 0.12895833333333334
        }
      },
      "auroc": 0.9250667534722221
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 20788,
          "fn": 5612,
          "accuracy": 0.7874242424242425
        },
        "0.01": {
          "tp": 2286,
          "fn": 24114,
          "accuracy": 0.0865909090909091
        }
      },
      "auroc": 0.9434094775883839
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 11781,
          "fn": 2619,
          "accuracy": 0.818125
        },
        "0.01": {
          "tp": 1086,
          "fn": 13314,
          "accuracy": 0.07541666666666667
        }
      },
      "auroc": 0.9426554687500001
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 32569,
          "fn": 8231,
          "accuracy": 0.7982598039215686
        },
        "0.01": {
          "tp": 3372,
          "fn": 37428,
          "accuracy": 0.0826470588235294
        }
      },
      "auroc": 0.9431433568218954
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15797,
          "fn": 10603,
          "accuracy": 0.5983712121212121
        },
        "0.01": {
          "tp": 2998,
          "fn": 23402,
          "accuracy": 0.11356060606060606
        }
      },
      "auroc": 0.9086525094696971
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 11278,
          "fn": 3122,
          "accuracy": 0.7831944444444444
        },
        "0.01": {
          "tp": 1110,
          "fn": 13290,
          "accuracy": 0.07708333333333334
        }
      },
      "auroc": 0.9297093171296296
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 27075,
          "fn": 13725,
          "accuracy": 0.6636029411764706
        },
        "0.01": {
          "tp": 4108,
          "fn": 36692,
          "accuracy": 0.10068627450980393
        }
      },
      "auroc": 0.9160843239379085
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 36585,
          "fn": 16215,
          "accuracy": 0.6928977272727272
        },
        "0.01": {
          "tp": 5284,
          "fn": 47516,
          "accuracy": 0.10007575757575758
        }
      },
      "auroc": 0.9260309935290404
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 23059,
          "fn": 5741,
          "accuracy": 0.8006597222222223
        },
        "0.01": {
          "tp": 2196,
          "fn": 26604,
          "accuracy": 0.07625
        }
      },
      "auroc": 0.9361823929398149
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 59644,
          "fn": 21956,
          "accuracy": 0.7309313725490196
        },
        "0.01": {
          "tp": 7480,
          "fn": 74120,
          "accuracy": 0.09166666666666666
        }
      },
      "auroc": 0.929613840379902
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9719822916666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9738281249999999
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9729052083333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9729531249999999
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9757135416666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9743333333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9724677083333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9747708333333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9736192708333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9761218749999999
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9725947916666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9743583333333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.918509375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9659927083333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": null
      },
      "auroc": 0.9422510416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9473156250000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9692937500000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 696,
          "fn": 104,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9583046874999999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9745427083333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.97424375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9743932291666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9758729166666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9714541666666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9736635416666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9752078125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9728489583333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9740283854166667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9762499999999998
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9749677083333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9756088541666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.9144343749999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9765989583333332
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.9455166666666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": null
      },
      "auroc": 0.9453421875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9757833333333332
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 703,
          "fn": 97,
          "accuracy": 0.87875
        },
        "0.01": null
      },
      "auroc": 0.9605627604166667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9761875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9725416666666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9743645833333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.8634145833333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9770864583333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        },
        "0.01": null
      },
      "auroc": 0.9202505208333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9198010416666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9748140625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 671,
          "fn": 129,
          "accuracy": 0.83875
        },
        "0.01": null
      },
      "auroc": 0.9473075520833333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9699520833333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9718687500000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9709104166666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.972859375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.972196875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9725281250000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9714057291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9720328125
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": null
      },
      "auroc": 0.9717192708333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9613708333333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9613708333333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.94865625
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.94865625
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9550135416666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9550135416666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.9287229166666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.9287229166666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.880015625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.880015625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.9043692708333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.9043692708333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.966359375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.966359375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9682833333333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9682833333333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9673213541666666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9673213541666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9716499999999999
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9716499999999999
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9572395833333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9572395833333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9644447916666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9644447916666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.936865625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.936865625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.9175416666666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.9175416666666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.9272036458333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.9272036458333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2045,
          "fn": 155,
          "accuracy": 0.9295454545454546
        },
        "0.01": null
      },
      "auroc": 0.964545928030303
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1167,
          "fn": 33,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9733407986111111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3212,
          "fn": 188,
          "accuracy": 0.9447058823529412
        },
        "0.01": null
      },
      "auroc": 0.9676500000000001
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1671,
          "fn": 529,
          "accuracy": 0.7595454545454545
        },
        "0.01": null
      },
      "auroc": 0.9354345643939393
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1160,
          "fn": 40,
          "accuracy": 0.9666666666666667
        },
        "0.01": null
      },
      "auroc": 0.9731737847222224
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2831,
          "fn": 569,
          "accuracy": 0.8326470588235294
        },
        "0.01": null
      },
      "auroc": 0.9487542892156864
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3716,
          "fn": 684,
          "accuracy": 0.8445454545454546
        },
        "0.01": null
      },
      "auroc": 0.9499902462121212
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2327,
          "fn": 73,
          "accuracy": 0.9695833333333334
        },
        "0.01": null
      },
      "auroc": 0.9732572916666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6043,
          "fn": 757,
          "accuracy": 0.8886764705882353
        },
        "0.01": null
      },
      "auroc": 0.958202144607843
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9719822916666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9738281249999999
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9729052083333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9729531249999999
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9757135416666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9743333333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9724677083333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9747708333333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9736192708333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9761218749999999
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9725947916666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9743583333333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.918509375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9659927083333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": null
      },
      "auroc": 0.9422510416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9473156250000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9692937500000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 696,
          "fn": 104,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9583046874999999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9745427083333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.97424375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9743932291666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9758729166666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9714541666666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9736635416666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9752078125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9728489583333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9740283854166667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9762499999999998
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9749677083333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9756088541666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.9144343749999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9765989583333332
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.9455166666666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": null
      },
      "auroc": 0.9453421875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9757833333333332
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 703,
          "fn": 97,
          "accuracy": 0.87875
        },
        "0.01": null
      },
      "auroc": 0.9605627604166667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9761875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9725416666666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9743645833333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.8634145833333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9770864583333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        },
        "0.01": null
      },
      "auroc": 0.9202505208333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9198010416666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9748140625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 671,
          "fn": 129,
          "accuracy": 0.83875
        },
        "0.01": null
      },
      "auroc": 0.9473075520833333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9699520833333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9718687500000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9709104166666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.972859375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.972196875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9725281250000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9714057291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9720328125
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": null
      },
      "auroc": 0.9717192708333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9613708333333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9613708333333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.94865625
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.94865625
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9550135416666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9550135416666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.9287229166666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.9287229166666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.880015625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.880015625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.9043692708333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.9043692708333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.966359375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.966359375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9682833333333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9682833333333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9673213541666666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9673213541666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9716499999999999
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9716499999999999
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9572395833333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9572395833333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9644447916666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9644447916666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.936865625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.936865625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.9175416666666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.9175416666666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.9272036458333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.9272036458333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2045,
          "fn": 155,
          "accuracy": 0.9295454545454546
        },
        "0.01": null
      },
      "auroc": 0.964545928030303
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1167,
          "fn": 33,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9733407986111111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3212,
          "fn": 188,
          "accuracy": 0.9447058823529412
        },
        "0.01": null
      },
      "auroc": 0.9676500000000001
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1671,
          "fn": 529,
          "accuracy": 0.7595454545454545
        },
        "0.01": null
      },
      "auroc": 0.9354345643939393
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1160,
          "fn": 40,
          "accuracy": 0.9666666666666667
        },
        "0.01": null
      },
      "auroc": 0.9731737847222224
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2831,
          "fn": 569,
          "accuracy": 0.8326470588235294
        },
        "0.01": null
      },
      "auroc": 0.9487542892156864
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3716,
          "fn": 684,
          "accuracy": 0.8445454545454546
        },
        "0.01": null
      },
      "auroc": 0.9499902462121212
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2327,
          "fn": 73,
          "accuracy": 0.9695833333333334
        },
        "0.01": null
      },
      "auroc": 0.9732572916666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6043,
          "fn": 757,
          "accuracy": 0.8886764705882353
        },
        "0.01": null
      },
      "auroc": 0.958202144607843
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9875354166666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9881885416666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9878619791666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9878885416666665
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9780958333333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9829921875000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9877119791666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9831421874999999
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 782,
          "fn": 18,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9854270833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.983253125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9739677083333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9786104166666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        },
        "0.01": null
      },
      "auroc": 0.8153427083333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.966796875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": null
      },
      "auroc": 0.8910697916666668
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.8992979166666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9703822916666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 639,
          "fn": 161,
          "accuracy": 0.79875
        },
        "0.01": null
      },
      "auroc": 0.9348401041666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9826489583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.97963125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9811401041666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9781541666666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9719354166666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9750447916666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9804015625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9757833333333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 749,
          "fn": 51,
          "accuracy": 0.93625
        },
        "0.01": null
      },
      "auroc": 0.9780924479166666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9813083333333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9716802083333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9764942708333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": null
      },
      "auroc": 0.8111572916666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9747583333333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": null
      },
      "auroc": 0.8929578125000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": null
      },
      "auroc": 0.8962328125000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9732192708333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 608,
          "fn": 192,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9347260416666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9815677083333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.9536697916666668
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9676187500000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.7346239583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.972553125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.8535885416666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        },
        "0.01": null
      },
      "auroc": 0.8580958333333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": null
      },
      "auroc": 0.9631114583333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 548,
          "fn": 252,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.9106036458333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9830260416666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9834427083333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.983234375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9814083333333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9796552083333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9805317708333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9822171875000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9815489583333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 771,
          "fn": 29,
          "accuracy": 0.96375
        },
        "0.01": null
      },
      "auroc": 0.9818830729166667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.9194697916666668
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.9194697916666668
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": null
      },
      "auroc": 0.88469375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": null
      },
      "auroc": 0.88469375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": null
      },
      "auroc": 0.9020817708333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": null
      },
      "auroc": 0.9020817708333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        },
        "0.01": null
      },
      "auroc": 0.8078416666666668
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        },
        "0.01": null
      },
      "auroc": 0.8078416666666668
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": null
      },
      "auroc": 0.7224208333333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": null
      },
      "auroc": 0.7224208333333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 330,
          "accuracy": 0.175
        },
        "0.01": null
      },
      "auroc": 0.76513125
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 330,
          "accuracy": 0.175
        },
        "0.01": null
      },
      "auroc": 0.76513125
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9846010416666666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9846010416666666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9856427083333332
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9856427083333332
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9851218749999999
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9851218749999999
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9817770833333332
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9817770833333332
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": null
      },
      "auroc": 0.836278125
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": null
      },
      "auroc": 0.836278125
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": null
      },
      "auroc": 0.9090276041666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": null
      },
      "auroc": 0.9090276041666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.8515552083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.8515552083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": null
      },
      "auroc": 0.8076572916666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": null
      },
      "auroc": 0.8076572916666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        },
        "0.01": null
      },
      "auroc": 0.8296062499999999
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        },
        "0.01": null
      },
      "auroc": 0.8296062499999999
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1827,
          "fn": 373,
          "accuracy": 0.8304545454545454
        },
        "0.01": null
      },
      "auroc": 0.9495076704545455
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1112,
          "fn": 88,
          "accuracy": 0.9266666666666666
        },
        "0.01": null
      },
      "auroc": 0.9750967013888889
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2939,
          "fn": 461,
          "accuracy": 0.8644117647058823
        },
        "0.01": null
      },
      "auroc": 0.9585390931372548
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1145,
          "fn": 1055,
          "accuracy": 0.5204545454545455
        },
        "0.01": null
      },
      "auroc": 0.8677516098484849
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1110,
          "fn": 90,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9739657986111111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2255,
          "fn": 1145,
          "accuracy": 0.663235294117647
        },
        "0.01": null
      },
      "auroc": 0.9052389705882353
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2972,
          "fn": 1428,
          "accuracy": 0.6754545454545454
        },
        "0.01": null
      },
      "auroc": 0.9086296401515153
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2222,
          "fn": 178,
          "accuracy": 0.9258333333333333
        },
        "0.01": null
      },
      "auroc": 0.9745312500000001
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5194,
          "fn": 1606,
          "accuracy": 0.7638235294117647
        },
        "0.01": null
      },
      "auroc": 0.9318890318627451
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9806906249999999
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9840010416666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9823458333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9817125
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9763916666666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9790520833333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9812015624999999
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9801963541666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.9806989583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.97813125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9714229166666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9747770833333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": null
      },
      "auroc": 0.8088583333333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9651895833333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": null
      },
      "auroc": 0.8870239583333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.8934947916666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9683062500000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 640,
          "fn": 160,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9309005208333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9762458333333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9695989583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9729223958333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9668010416666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9652239583333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9660125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9715234375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9674114583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 731,
          "fn": 69,
          "accuracy": 0.91375
        },
        "0.01": null
      },
      "auroc": 0.9694674479166667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9776302083333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9640843749999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9708572916666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": null
      },
      "auroc": 0.7770979166666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9636427083333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        },
        "0.01": null
      },
      "auroc": 0.8703703125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": null
      },
      "auroc": 0.8773640625000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.9638635416666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 596,
          "fn": 204,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.9206138020833334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9785958333333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9276125000000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.9531041666666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        },
        "0.01": null
      },
      "auroc": 0.6999708333333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.965059375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.8325151041666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        },
        "0.01": null
      },
      "auroc": 0.8392833333333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9463359375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 546,
          "fn": 254,
          "accuracy": 0.6825
        },
        "0.01": null
      },
      "auroc": 0.8928096354166666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9754239583333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9786270833333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9770255208333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9759041666666668
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9723583333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9741312499999999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9756640625000002
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9754927083333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 767,
          "fn": 33,
          "accuracy": 0.95875
        },
        "0.01": null
      },
      "auroc": 0.9755783854166666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.8928427083333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.8928427083333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": null
      },
      "auroc": 0.8515885416666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": null
      },
      "auroc": 0.8515885416666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": null
      },
      "auroc": 0.872215625
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": null
      },
      "auroc": 0.872215625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": null
      },
      "auroc": 0.7956156249999999
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": null
      },
      "auroc": 0.7956156249999999
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": null
      },
      "auroc": 0.7141697916666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": null
      },
      "auroc": 0.7141697916666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        },
        "0.01": null
      },
      "auroc": 0.7548927083333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        },
        "0.01": null
      },
      "auroc": 0.7548927083333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9750687499999999
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9750687499999999
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9779583333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9779583333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9765135416666666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9765135416666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9773020833333332
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9773020833333332
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.8594260416666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.8594260416666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.9183640625
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.9183640625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": null
      },
      "auroc": 0.8254104166666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": null
      },
      "auroc": 0.8254104166666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": null
      },
      "auroc": 0.7993958333333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": null
      },
      "auroc": 0.7993958333333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        },
        "0.01": null
      },
      "auroc": 0.812403125
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        },
        "0.01": null
      },
      "auroc": 0.812403125
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1834,
          "fn": 366,
          "accuracy": 0.8336363636363636
        },
        "0.01": null
      },
      "auroc": 0.9393597537878787
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1090,
          "fn": 110,
          "accuracy": 0.9083333333333333
        },
        "0.01": null
      },
      "auroc": 0.9658911458333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2924,
          "fn": 476,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9487237745098039
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1164,
          "fn": 1036,
          "accuracy": 0.5290909090909091
        },
        "0.01": null
      },
      "auroc": 0.8557166666666667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1103,
          "fn": 97,
          "accuracy": 0.9191666666666667
        },
        "0.01": null
      },
      "auroc": 0.9679776041666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2267,
          "fn": 1133,
          "accuracy": 0.6667647058823529
        },
        "0.01": null
      },
      "auroc": 0.8953381740196078
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2998,
          "fn": 1402,
          "accuracy": 0.6813636363636364
        },
        "0.01": null
      },
      "auroc": 0.8975382102272728
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2193,
          "fn": 207,
          "accuracy": 0.91375
        },
        "0.01": null
      },
      "auroc": 0.9669343749999999
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 5191,
          "fn": 1609,
          "accuracy": 0.7633823529411765
        },
        "0.01": null
      },
      "auroc": 0.9220309742647059
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9748541666666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9784822916666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9766682291666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9764072916666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9773583333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9768828125
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9756307291666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9779203125
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9767755208333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9763562499999999
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.972884375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9746203125000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.8795802083333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9656645833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": null
      },
      "auroc": 0.9226223958333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9279682291666668
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9692744791666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 671,
          "fn": 129,
          "accuracy": 0.83875
        },
        "0.01": null
      },
      "auroc": 0.9486213541666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.97785625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9761385416666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9769973958333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9790406250000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9713177083333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9751791666666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9784484375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9737281249999999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 784,
          "fn": 16,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.97608828125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9765145833333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9760031250000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9762588541666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.8696427083333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9741114583333332
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        },
        "0.01": null
      },
      "auroc": 0.9218770833333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9230786458333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9750572916666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 663,
          "fn": 137,
          "accuracy": 0.82875
        },
        "0.01": null
      },
      "auroc": 0.9490679687500001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9770083333333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9637541666666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.97038125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": null
      },
      "auroc": 0.8184375000000002
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9750531250000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": null
      },
      "auroc": 0.8967453125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": null
      },
      "auroc": 0.8977229166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9694036458333335
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 623,
          "fn": 177,
          "accuracy": 0.77875
        },
        "0.01": null
      },
      "auroc": 0.93356328125
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9726375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9759249999999999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.97428125
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.974721875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9740229166666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9743723958333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9736796875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9749739583333332
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9743268229166666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.9505708333333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.9505708333333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.9314593750000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.9314593750000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.9410151041666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.9410151041666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.8913572916666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.8913572916666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        },
        "0.01": null
      },
      "auroc": 0.8219937499999999
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        },
        "0.01": null
      },
      "auroc": 0.8219937499999999
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        },
        "0.01": null
      },
      "auroc": 0.8566755208333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        },
        "0.01": null
      },
      "auroc": 0.8566755208333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9702458333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9702458333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.972465625
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.972465625
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9713557291666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9713557291666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9752677083333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9752677083333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9280770833333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9280770833333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9516723958333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9516723958333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9078739583333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9078739583333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.8840333333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.8840333333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.8959536458333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.8959536458333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1974,
          "fn": 226,
          "accuracy": 0.8972727272727272
        },
        "0.01": null
      },
      "auroc": 0.9591402462121212
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1149,
          "fn": 51,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9738645833333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3123,
          "fn": 277,
          "accuracy": 0.9185294117647059
        },
        "0.01": null
      },
      "auroc": 0.9643370710784314
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1506,
          "fn": 694,
          "accuracy": 0.6845454545454546
        },
        "0.01": null
      },
      "auroc": 0.9123508522727273
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1139,
          "fn": 61,
          "accuracy": 0.9491666666666667
        },
        "0.01": null
      },
      "auroc": 0.9729213541666667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2645,
          "fn": 755,
          "accuracy": 0.7779411764705882
        },
        "0.01": null
      },
      "auroc": 0.9337286764705882
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3480,
          "fn": 920,
          "accuracy": 0.7909090909090909
        },
        "0.01": null
      },
      "auroc": 0.9357455492424243
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2288,
          "fn": 112,
          "accuracy": 0.9533333333333334
        },
        "0.01": null
      },
      "auroc": 0.97339296875
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5768,
          "fn": 1032,
          "accuracy": 0.8482352941176471
        },
        "0.01": null
      },
      "auroc": 0.9490328737745097
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9798760416666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9802635416666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9800697916666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9771291666666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9699322916666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9735307291666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9785026041666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9750979166666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 765,
          "fn": 35,
          "accuracy": 0.95625
        },
        "0.01": null
      },
      "auroc": 0.9768002604166667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9769958333333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.9149510416666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": null
      },
      "auroc": 0.9459734375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": null
      },
      "auroc": 0.8965270833333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        },
        "0.01": null
      },
      "auroc": 0.80055625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        },
        "0.01": null
      },
      "auroc": 0.8485416666666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9367614583333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        },
        "0.01": null
      },
      "auroc": 0.8577536458333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 492,
          "fn": 308,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.8972575520833334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9710343750000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9572645833333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9641494791666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9717239583333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9156416666666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9436828125000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9713791666666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.936453125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 650,
          "fn": 150,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.9539161458333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9691947916666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9641375000000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9666661458333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.9161281250000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": null
      },
      "auroc": 0.832446875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": null
      },
      "auroc": 0.8742875000000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.9426614583333335
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.8982921875000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 542,
          "fn": 258,
          "accuracy": 0.6775
        },
        "0.01": null
      },
      "auroc": 0.9204768229166667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.978321875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.9337802083333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": null
      },
      "auroc": 0.9560510416666668
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": null
      },
      "auroc": 0.8936385416666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        },
        "0.01": null
      },
      "auroc": 0.7861520833333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        },
        "0.01": null
      },
      "auroc": 0.8398953125000002
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.9359802083333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": null
      },
      "auroc": 0.8599661458333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 477,
          "fn": 323,
          "accuracy": 0.59625
        },
        "0.01": null
      },
      "auroc": 0.8979731770833334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9745083333333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9746812499999999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9745947916666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.972828125
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9527979166666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9628130208333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9736682291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9637395833333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 734,
          "fn": 66,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.96870390625
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.95290625
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.95290625
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.9275010416666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.9275010416666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": null
      },
      "auroc": 0.9402036458333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": null
      },
      "auroc": 0.9402036458333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.9280104166666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.9280104166666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.927740625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.927740625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.9278755208333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.9278755208333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9759958333333332
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9759958333333332
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9756270833333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9756270833333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9758114583333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9758114583333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9792302083333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9792302083333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9796187500000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9796187500000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9794244791666668
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9794244791666668
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.9220677083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.9220677083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.9029552083333332
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.9029552083333332
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        },
        "0.01": null
      },
      "auroc": 0.9125114583333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        },
        "0.01": null
      },
      "auroc": 0.9125114583333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1978,
          "fn": 222,
          "accuracy": 0.899090909090909
        },
        "0.01": null
      },
      "auroc": 0.9643765151515151
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 968,
          "fn": 232,
          "accuracy": 0.8066666666666666
        },
        "0.01": null
      },
      "auroc": 0.9541796874999999
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2946,
          "fn": 454,
          "accuracy": 0.8664705882352941
        },
        "0.01": null
      },
      "auroc": 0.9607776348039215
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1637,
          "fn": 563,
          "accuracy": 0.7440909090909091
        },
        "0.01": null
      },
      "auroc": 0.9401288825757576
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 659,
          "fn": 541,
          "accuracy": 0.5491666666666667
        },
        "0.01": null
      },
      "auroc": 0.8762545138888889
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2296,
          "fn": 1104,
          "accuracy": 0.6752941176470588
        },
        "0.01": null
      },
      "auroc": 0.9175849877450981
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3615,
          "fn": 785,
          "accuracy": 0.821590909090909
        },
        "0.01": null
      },
      "auroc": 0.9522526988636364
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1627,
          "fn": 773,
          "accuracy": 0.6779166666666666
        },
        "0.01": null
      },
      "auroc": 0.9152171006944445
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 5242,
          "fn": 1558,
          "accuracy": 0.7708823529411765
        },
        "0.01": null
      },
      "auroc": 0.9391813112745098
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9724666666666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9742072916666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9733369791666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9735052083333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9762197916666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9748625
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9729859375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9752135416666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9740997395833333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9768281249999999
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9726875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9747578125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.9095322916666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9662322916666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        },
        "0.01": null
      },
      "auroc": 0.9378822916666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9431802083333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9694598958333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 688,
          "fn": 112,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9563200520833333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9747625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9744395833333332
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9746010416666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.97608125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9711708333333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9736260416666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.975421875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9728052083333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        },
        "0.01": null
      },
      "auroc": 0.9741135416666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9767375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.975021875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9758796875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": null
      },
      "auroc": 0.908315625
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9762166666666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": null
      },
      "auroc": 0.9422661458333335
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": null
      },
      "auroc": 0.9425265624999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9756192708333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 695,
          "fn": 105,
          "accuracy": 0.86875
        },
        "0.01": null
      },
      "auroc": 0.9590729166666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.97778125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9716697916666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9747255208333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": null
      },
      "auroc": 0.8535552083333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9770229166666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": null
      },
      "auroc": 0.9152890625000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        },
        "0.01": null
      },
      "auroc": 0.9156682291666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9743463541666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 663,
          "fn": 137,
          "accuracy": 0.82875
        },
        "0.01": null
      },
      "auroc": 0.9450072916666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9703697916666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9722427083333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.97130625
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.973521875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9723208333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9729213541666668
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9719458333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9722817708333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9721138020833334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9589354166666668
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9589354166666668
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9437552083333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9437552083333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9513453125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9513453125
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.9188666666666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.9188666666666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": null
      },
      "auroc": 0.8710072916666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": null
      },
      "auroc": 0.8710072916666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": null
      },
      "auroc": 0.8949369791666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": null
      },
      "auroc": 0.8949369791666666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9667604166666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9667604166666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9686739583333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9686739583333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9677171875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9677171875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9722625
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9722625
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9554947916666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9554947916666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9638786458333335
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9638786458333335
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9296531250000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9296531250000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.9098125
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.9098125
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        },
        "0.01": null
      },
      "auroc": 0.9197328125
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        },
        "0.01": null
      },
      "auroc": 0.9197328125
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2033,
          "fn": 167,
          "accuracy": 0.9240909090909091
        },
        "0.01": null
      },
      "auroc": 0.9632203598484849
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1166,
          "fn": 34,
          "accuracy": 0.9716666666666667
        },
        "0.01": null
      },
      "auroc": 0.973378125
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3199,
          "fn": 201,
          "accuracy": 0.9408823529411765
        },
        "0.01": null
      },
      "auroc": 0.9668054534313725
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1634,
          "fn": 566,
          "accuracy": 0.7427272727272727
        },
        "0.01": null
      },
      "auroc": 0.9312050189393939
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1157,
          "fn": 43,
          "accuracy": 0.9641666666666666
        },
        "0.01": null
      },
      "auroc": 0.9731972222222222
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2791,
          "fn": 609,
          "accuracy": 0.8208823529411765
        },
        "0.01": null
      },
      "auroc": 0.9460257965686275
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3667,
          "fn": 733,
          "accuracy": 0.8334090909090909
        },
        "0.01": null
      },
      "auroc": 0.9472126893939394
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2323,
          "fn": 77,
          "accuracy": 0.9679166666666666
        },
        "0.01": null
      },
      "auroc": 0.973287673611111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5990,
          "fn": 810,
          "accuracy": 0.8808823529411764
        },
        "0.01": null
      },
      "auroc": 0.9564156250000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9719822916666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9738281249999999
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9729052083333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9729531249999999
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9757135416666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9743333333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9724677083333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9747708333333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9736192708333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9761218749999999
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9722968750000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.974209375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.918415625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9656791666666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": null
      },
      "auroc": 0.9420473958333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.94726875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9689880208333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 696,
          "fn": 104,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9581283854166667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9745427083333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.974209375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9743760416666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9758729166666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9713635416666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9736182291666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9752078125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9727864583333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9739971354166667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9762499999999998
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9749677083333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9756088541666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.9144343749999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9766593749999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.945546875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": null
      },
      "auroc": 0.9453421875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9758135416666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 703,
          "fn": 97,
          "accuracy": 0.87875
        },
        "0.01": null
      },
      "auroc": 0.9605778645833334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9761875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9725416666666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9743645833333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.8634197916666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9771489583333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        },
        "0.01": null
      },
      "auroc": 0.920284375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9198036458333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9748453125000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 671,
          "fn": 129,
          "accuracy": 0.83875
        },
        "0.01": null
      },
      "auroc": 0.9473244791666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9699520833333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9718687500000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9709104166666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.972859375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.972196875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9725281250000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9714057291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9720328125
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": null
      },
      "auroc": 0.9717192708333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9613708333333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9613708333333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.9486333333333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.9486333333333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9550020833333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9550020833333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.9287229166666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.9287229166666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.880015625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.880015625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.9043692708333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.9043692708333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.966359375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.966359375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9682833333333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9682833333333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9673213541666666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9673213541666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9716499999999999
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9716499999999999
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9572395833333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9572395833333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9644447916666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9644447916666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.936865625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.936865625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.9175416666666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.9175416666666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.9272036458333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.9272036458333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2045,
          "fn": 155,
          "accuracy": 0.9295454545454546
        },
        "0.01": null
      },
      "auroc": 0.964545928030303
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1167,
          "fn": 33,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9732854166666667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3212,
          "fn": 188,
          "accuracy": 0.9447058823529412
        },
        "0.01": null
      },
      "auroc": 0.9676304534313727
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1671,
          "fn": 529,
          "accuracy": 0.7595454545454545
        },
        "0.01": null
      },
      "auroc": 0.9354244318181818
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1160,
          "fn": 40,
          "accuracy": 0.9666666666666667
        },
        "0.01": null
      },
      "auroc": 0.9731269097222223
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2831,
          "fn": 569,
          "accuracy": 0.8326470588235294
        },
        "0.01": null
      },
      "auroc": 0.9487311887254903
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3716,
          "fn": 684,
          "accuracy": 0.8445454545454546
        },
        "0.01": null
      },
      "auroc": 0.9499851799242424
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2327,
          "fn": 73,
          "accuracy": 0.9695833333333334
        },
        "0.01": null
      },
      "auroc": 0.9732061631944444
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6043,
          "fn": 757,
          "accuracy": 0.8886764705882353
        },
        "0.01": null
      },
      "auroc": 0.9581808210784313
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7808260416666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.5332458333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.6570359374999999
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.7589541666666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.40742708333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.583190625
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.7698901041666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4703364583333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 798,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.62011328125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9834572916666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9593572916666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9714072916666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.5078364583333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9550541666666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        },
        "0.01": null
      },
      "auroc": 0.7314453125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": null
      },
      "auroc": 0.745646875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.9572057291666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 520,
          "fn": 280,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.8514263020833333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.6350354166666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.313871875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": null
      },
      "auroc": 0.4744536458333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.5641947916666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": null
      },
      "auroc": 0.4573375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        },
        "0.01": null
      },
      "auroc": 0.5107661458333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": null
      },
      "auroc": 0.5996151041666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        },
        "0.01": null
      },
      "auroc": 0.3856046875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 780,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.4926098958333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9823520833333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.5647458333333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": null
      },
      "auroc": 0.7735489583333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.46597083333333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.3128166666666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": null
      },
      "auroc": 0.38939375000000004
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": null
      },
      "auroc": 0.7241614583333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.43878125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 602,
          "accuracy": 0.2475
        },
        "0.01": null
      },
      "auroc": 0.5814713541666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9611520833333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.31337083333333327
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        },
        "0.01": null
      },
      "auroc": 0.6372614583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.41959375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.4342125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.42690312500000005
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        },
        "0.01": null
      },
      "auroc": 0.6903729166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        },
        "0.01": null
      },
      "auroc": 0.37379166666666663
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 623,
          "accuracy": 0.22125
        },
        "0.01": null
      },
      "auroc": 0.5320822916666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": null
      },
      "auroc": 0.7392177083333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4291270833333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        },
        "0.01": null
      },
      "auroc": 0.5841723958333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.6027177083333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.26858020833333335
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.43564895833333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        },
        "0.01": null
      },
      "auroc": 0.6709677083333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.34885364583333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 790,
          "accuracy": 0.0125
        },
        "0.01": null
      },
      "auroc": 0.5099106770833334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.6460416666666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.6460416666666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.547534375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.547534375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": null
      },
      "auroc": 0.5967880208333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": null
      },
      "auroc": 0.5967880208333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4576416666666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4576416666666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4178104166666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4178104166666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4377260416666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4377260416666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.6468562500000001
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.6468562500000001
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.5553197916666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.5553197916666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.6010880208333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.6010880208333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.6573833333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.6573833333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4045770833333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4045770833333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.5309802083333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.5309802083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.5233937500000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.5233937500000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.49573229166666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.49573229166666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": null
      },
      "auroc": 0.5095630208333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": null
      },
      "auroc": 0.5095630208333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 573,
          "fn": 1627,
          "accuracy": 0.26045454545454544
        },
        "0.01": null
      },
      "auroc": 0.7284870265151515
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 1036,
          "accuracy": 0.13666666666666666
        },
        "0.01": null
      },
      "auroc": 0.518953125
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 737,
          "fn": 2663,
          "accuracy": 0.21676470588235294
        },
        "0.01": null
      },
      "auroc": 0.6545338848039215
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 2192,
          "accuracy": 0.0036363636363636364
        },
        "0.01": null
      },
      "auroc": 0.5218401515151515
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 1010,
          "accuracy": 0.15833333333333333
        },
        "0.01": null
      },
      "auroc": 0.4725713541666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 3202,
          "accuracy": 0.05823529411764706
        },
        "0.01": null
      },
      "auroc": 0.5044511642156863
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 581,
          "fn": 3819,
          "accuracy": 0.13204545454545455
        },
        "0.01": null
      },
      "auroc": 0.6251635890151515
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 2046,
          "accuracy": 0.1475
        },
        "0.01": null
      },
      "auroc": 0.4957622395833333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 935,
          "fn": 5865,
          "accuracy": 0.1375
        },
        "0.01": null
      },
      "auroc": 0.5794925245098039
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9786489583333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9803020833333332
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9794755208333332
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.98005625
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9794052083333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9797307291666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9793526041666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9798536458333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": null
      },
      "auroc": 0.979603125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9782458333333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9725947916666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9754203125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": null
      },
      "auroc": 0.7998197916666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9659927083333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.88290625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.8890328125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9692937500000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 642,
          "fn": 158,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.9291632812499999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9784729166666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9754135416666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9769432291666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9750239583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9717625000000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9733932291666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9767484375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9735880208333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9751682291666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9781760416666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9755052083333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9768406250000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        },
        "0.01": null
      },
      "auroc": 0.7728739583333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9758
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": null
      },
      "auroc": 0.8743369791666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": null
      },
      "auroc": 0.875525
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9756526041666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 637,
          "fn": 163,
          "accuracy": 0.79625
        },
        "0.01": null
      },
      "auroc": 0.9255888020833334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9779322916666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9700375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9739848958333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        },
        "0.01": null
      },
      "auroc": 0.7050666666666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.976340625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": null
      },
      "auroc": 0.8407036458333332
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.8414994791666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9731890625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 593,
          "fn": 207,
          "accuracy": 0.74125
        },
        "0.01": null
      },
      "auroc": 0.9073442708333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9735385416666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.975553125
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9745458333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.973565625
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9735635416666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9735645833333335
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9735520833333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9745583333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.9740552083333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9163333333333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9163333333333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": null
      },
      "auroc": 0.88183125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": null
      },
      "auroc": 0.88183125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.8990822916666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.8990822916666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": null
      },
      "auroc": 0.7692395833333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": null
      },
      "auroc": 0.7692395833333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": null
      },
      "auroc": 0.7022458333333332
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": null
      },
      "auroc": 0.7022458333333332
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 321,
          "accuracy": 0.1975
        },
        "0.01": null
      },
      "auroc": 0.7357427083333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 321,
          "accuracy": 0.1975
        },
        "0.01": null
      },
      "auroc": 0.7357427083333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9710875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9710875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.97425
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.97425
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.97266875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.97266875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.97264375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.97264375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": null
      },
      "auroc": 0.8312843750000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": null
      },
      "auroc": 0.8312843750000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        },
        "0.01": null
      },
      "auroc": 0.9019640625
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        },
        "0.01": null
      },
      "auroc": 0.9019640625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.8506260416666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.8506260416666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": null
      },
      "auroc": 0.7904135416666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": null
      },
      "auroc": 0.7904135416666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": null
      },
      "auroc": 0.8205197916666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": null
      },
      "auroc": 0.8205197916666667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1863,
          "fn": 337,
          "accuracy": 0.8468181818181818
        },
        "0.01": null
      },
      "auroc": 0.9404495265151516
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1162,
          "fn": 38,
          "accuracy": 0.9683333333333334
        },
        "0.01": null
      },
      "auroc": 0.9749010416666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3025,
          "fn": 375,
          "accuracy": 0.8897058823529411
        },
        "0.01": null
      },
      "auroc": 0.9526088848039216
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1195,
          "fn": 1005,
          "accuracy": 0.5431818181818182
        },
        "0.01": null
      },
      "auroc": 0.8533119318181819
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1159,
          "fn": 41,
          "accuracy": 0.9658333333333333
        },
        "0.01": null
      },
      "auroc": 0.9738107638888889
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2354,
          "fn": 1046,
          "accuracy": 0.6923529411764706
        },
        "0.01": null
      },
      "auroc": 0.895840931372549
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3058,
          "fn": 1342,
          "accuracy": 0.695
        },
        "0.01": null
      },
      "auroc": 0.8968807291666667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2321,
          "fn": 79,
          "accuracy": 0.9670833333333333
        },
        "0.01": null
      },
      "auroc": 0.9743559027777777
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5379,
          "fn": 1421,
          "accuracy": 0.7910294117647059
        },
        "0.01": null
      },
      "auroc": 0.9242249080882352
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9727864583333332
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9755020833333332
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9741442708333332
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9737083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9770489583333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9753786458333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9732473958333332
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9762755208333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9747614583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9761333333333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.972690625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9744119791666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.9117145833333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9660489583333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.9388817708333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": null
      },
      "auroc": 0.9439239583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9693697916666668
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 691,
          "fn": 109,
          "accuracy": 0.86375
        },
        "0.01": null
      },
      "auroc": 0.956646875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9753770833333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.974471875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9749244791666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9764947916666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9721020833333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9742984375000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9759359375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9732869791666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9746114583333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9760322916666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9752604166666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9756463541666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.9066208333333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9766468749999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.9416338541666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9413265625000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9759536458333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 692,
          "fn": 108,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9586401041666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9763729166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9720458333333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.974209375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.8553385416666668
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9770552083333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.916196875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": null
      },
      "auroc": 0.9158557291666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9745505208333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 664,
          "fn": 136,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9452031249999999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.97045
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9729218749999999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9716859375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9732989583333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9722364583333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9727677083333335
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9718744791666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9725791666666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9722268229166667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9589822916666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9589822916666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.9461104166666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.9461104166666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9525463541666668
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9525463541666668
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.9227916666666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.9227916666666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": null
      },
      "auroc": 0.8679864583333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": null
      },
      "auroc": 0.8679864583333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.8953890625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.8953890625
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9674739583333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9674739583333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9695916666666666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9695916666666666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9685328125
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9685328125
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.972534375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.972534375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.949178125
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.949178125
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.96085625
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.96085625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9331802083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9331802083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.9127572916666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.9127572916666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        },
        "0.01": null
      },
      "auroc": 0.9229687499999999
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        },
        "0.01": null
      },
      "auroc": 0.9229687499999999
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2031,
          "fn": 169,
          "accuracy": 0.9231818181818182
        },
        "0.01": null
      },
      "auroc": 0.9638285984848485
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1163,
          "fn": 37,
          "accuracy": 0.9691666666666666
        },
        "0.01": null
      },
      "auroc": 0.9738154513888889
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3194,
          "fn": 206,
          "accuracy": 0.9394117647058824
        },
        "0.01": null
      },
      "auroc": 0.9673533700980392
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1630,
          "fn": 570,
          "accuracy": 0.740909090909091
        },
        "0.01": null
      },
      "auroc": 0.9311636363636363
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1159,
          "fn": 41,
          "accuracy": 0.9658333333333333
        },
        "0.01": null
      },
      "auroc": 0.9735230902777776
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2789,
          "fn": 611,
          "accuracy": 0.8202941176470588
        },
        "0.01": null
      },
      "auroc": 0.9461140318627451
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3661,
          "fn": 739,
          "accuracy": 0.8320454545454545
        },
        "0.01": null
      },
      "auroc": 0.9474961174242424
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2322,
          "fn": 78,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9736692708333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5983,
          "fn": 817,
          "accuracy": 0.8798529411764706
        },
        "0.01": null
      },
      "auroc": 0.9567337009803921
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.75310625
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7572302083333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7551682291666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7564989583333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.75206875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7542838541666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7548026041666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7546494791666668
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7547260416666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.762278125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.79735625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7798171875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7604708333333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7974583333333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7789645833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7613744791666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7974072916666668
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7793908854166667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.75341875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7576291666666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7555239583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7575979166666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7542166666666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7559072916666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7555083333333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7559229166666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.755715625
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.7670708333333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7476197916666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.7573453125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7552
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7628885416666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7590442708333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.7611354166666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7552541666666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 799,
          "accuracy": 0.00125
        },
        "0.01": null
      },
      "auroc": 0.7581947916666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7408895833333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7415010416666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7411953125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7408354166666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7447114583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7427734374999999
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7408625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7431062499999999
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.741984375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7485250000000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7556885416666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7521067708333332
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7512416666666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.745859375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7485505208333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7498833333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7507739583333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7503286458333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.733903125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.733903125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7259489583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7259489583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7299260416666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7299260416666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7322552083333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7322552083333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7372135416666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7372135416666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.734734375
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.734734375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.74515
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.74515
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7429166666666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7429166666666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7440333333333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7440333333333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7414875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7414875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.74531875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.74531875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7434031249999999
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7434031249999999
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7415770833333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7415770833333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7332833333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7332833333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7374302083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7374302083333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 2199,
          "accuracy": 0.00045454545454545455
        },
        "0.01": null
      },
      "auroc": 0.7472419507575757
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7595041666666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 3399,
          "accuracy": 0.0002941176470588235
        },
        "0.01": null
      },
      "auroc": 0.7515697916666667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 2200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7460478219696969
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7595338541666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 3400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7508075980392156
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 4399,
          "accuracy": 0.00022727272727272727
        },
        "0.01": null
      },
      "auroc": 0.7466448863636365
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 2400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7595190104166667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 6799,
          "accuracy": 0.00014705882352941175
        },
        "0.01": null
      },
      "auroc": 0.7511886948529412
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1998,
          "fn": 402,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.9413947916666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1990,
          "fn": 410,
          "accuracy": 0.8291666666666667
        },
        "0.01": null
      },
      "auroc": 0.9227422743055557
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3988,
          "fn": 812,
          "accuracy": 0.8308333333333333
        },
        "0.01": null
      },
      "auroc": 0.9320685329861111
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1996,
          "fn": 404,
          "accuracy": 0.8316666666666667
        },
        "0.01": null
      },
      "auroc": 0.9403933159722222
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1917,
          "fn": 483,
          "accuracy": 0.79875
        },
        "0.01": null
      },
      "auroc": 0.9100907118055555
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3913,
          "fn": 887,
          "accuracy": 0.8152083333333333
        },
        "0.01": null
      },
      "auroc": 0.9252420138888888
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3994,
          "fn": 806,
          "accuracy": 0.8320833333333333
        },
        "0.01": null
      },
      "auroc": 0.9408940538194445
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3907,
          "fn": 893,
          "accuracy": 0.8139583333333333
        },
        "0.01": null
      },
      "auroc": 0.9164164930555556
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7901,
          "fn": 1699,
          "accuracy": 0.8230208333333333
        },
        "0.01": null
      },
      "auroc": 0.9286552734375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2187,
          "fn": 213,
          "accuracy": 0.91125
        },
        "0.01": null
      },
      "auroc": 0.9600037326388888
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2008,
          "fn": 392,
          "accuracy": 0.8366666666666667
        },
        "0.01": null
      },
      "auroc": 0.9521165798611111
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4195,
          "fn": 605,
          "accuracy": 0.8739583333333333
        },
        "0.01": null
      },
      "auroc": 0.9560601562500001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 937,
          "fn": 1463,
          "accuracy": 0.3904166666666667
        },
        "0.01": null
      },
      "auroc": 0.8370930555555555
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1939,
          "fn": 461,
          "accuracy": 0.8079166666666666
        },
        "0.01": null
      },
      "auroc": 0.9372215277777778
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2876,
          "fn": 1924,
          "accuracy": 0.5991666666666666
        },
        "0.01": null
      },
      "auroc": 0.8871572916666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3124,
          "fn": 1676,
          "accuracy": 0.6508333333333334
        },
        "0.01": null
      },
      "auroc": 0.8985483940972223
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3947,
          "fn": 853,
          "accuracy": 0.8222916666666666
        },
        "0.01": null
      },
      "auroc": 0.9446690538194444
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7071,
          "fn": 2529,
          "accuracy": 0.7365625
        },
        "0.01": null
      },
      "auroc": 0.9216087239583333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1945,
          "fn": 455,
          "accuracy": 0.8104166666666667
        },
        "0.01": null
      },
      "auroc": 0.9290400173611112
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1920,
          "fn": 480,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9000963541666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3865,
          "fn": 935,
          "accuracy": 0.8052083333333333
        },
        "0.01": null
      },
      "auroc": 0.9145681857638889
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1915,
          "fn": 485,
          "accuracy": 0.7979166666666667
        },
        "0.01": null
      },
      "auroc": 0.9227276041666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1861,
          "fn": 539,
          "accuracy": 0.7754166666666666
        },
        "0.01": null
      },
      "auroc": 0.9054150173611111
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3776,
          "fn": 1024,
          "accuracy": 0.7866666666666666
        },
        "0.01": null
      },
      "auroc": 0.9140713107638889
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3860,
          "fn": 940,
          "accuracy": 0.8041666666666667
        },
        "0.01": null
      },
      "auroc": 0.9258838107638889
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3781,
          "fn": 1019,
          "accuracy": 0.7877083333333333
        },
        "0.01": null
      },
      "auroc": 0.9027556857638889
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7641,
          "fn": 1959,
          "accuracy": 0.7959375
        },
        "0.01": null
      },
      "auroc": 0.9143197482638888
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2180,
          "fn": 220,
          "accuracy": 0.9083333333333333
        },
        "0.01": null
      },
      "auroc": 0.9594805555555554
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1879,
          "fn": 521,
          "accuracy": 0.7829166666666667
        },
        "0.01": null
      },
      "auroc": 0.9199134548611112
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4059,
          "fn": 741,
          "accuracy": 0.845625
        },
        "0.01": null
      },
      "auroc": 0.9396970052083333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 908,
          "fn": 1492,
          "accuracy": 0.37833333333333335
        },
        "0.01": null
      },
      "auroc": 0.8271925347222222
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1774,
          "fn": 626,
          "accuracy": 0.7391666666666666
        },
        "0.01": null
      },
      "auroc": 0.8899321180555555
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2682,
          "fn": 2118,
          "accuracy": 0.55875
        },
        "0.01": null
      },
      "auroc": 0.8585623263888889
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3088,
          "fn": 1712,
          "accuracy": 0.6433333333333333
        },
        "0.01": null
      },
      "auroc": 0.8933365451388888
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3653,
          "fn": 1147,
          "accuracy": 0.7610416666666666
        },
        "0.01": null
      },
      "auroc": 0.9049227864583334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6741,
          "fn": 2859,
          "accuracy": 0.7021875
        },
        "0.01": null
      },
      "auroc": 0.8991296657986112
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2143,
          "fn": 257,
          "accuracy": 0.8929166666666667
        },
        "0.01": null
      },
      "auroc": 0.9565153645833333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1712,
          "fn": 688,
          "accuracy": 0.7133333333333334
        },
        "0.01": null
      },
      "auroc": 0.8887555555555555
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3855,
          "fn": 945,
          "accuracy": 0.803125
        },
        "0.01": null
      },
      "auroc": 0.9226354600694444
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 697,
          "fn": 1703,
          "accuracy": 0.29041666666666666
        },
        "0.01": null
      },
      "auroc": 0.7759424479166667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1752,
          "fn": 648,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.8949568576388889
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2449,
          "fn": 2351,
          "accuracy": 0.5102083333333334
        },
        "0.01": null
      },
      "auroc": 0.8354496527777778
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2840,
          "fn": 1960,
          "accuracy": 0.5916666666666667
        },
        "0.01": null
      },
      "auroc": 0.86622890625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3464,
          "fn": 1336,
          "accuracy": 0.7216666666666667
        },
        "0.01": null
      },
      "auroc": 0.8918562065972221
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6304,
          "fn": 3296,
          "accuracy": 0.6566666666666666
        },
        "0.01": null
      },
      "auroc": 0.8790425564236112
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1996,
          "fn": 404,
          "accuracy": 0.8316666666666667
        },
        "0.01": null
      },
      "auroc": 0.9347960937500001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1966,
          "fn": 434,
          "accuracy": 0.8191666666666667
        },
        "0.01": null
      },
      "auroc": 0.9111513020833333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3962,
          "fn": 838,
          "accuracy": 0.8254166666666667
        },
        "0.01": null
      },
      "auroc": 0.9229736979166667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1939,
          "fn": 461,
          "accuracy": 0.8079166666666666
        },
        "0.01": null
      },
      "auroc": 0.9248155381944445
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1913,
          "fn": 487,
          "accuracy": 0.7970833333333334
        },
        "0.01": null
      },
      "auroc": 0.8939987847222223
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3852,
          "fn": 948,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.9094071614583333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3935,
          "fn": 865,
          "accuracy": 0.8197916666666667
        },
        "0.01": null
      },
      "auroc": 0.9298058159722223
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3879,
          "fn": 921,
          "accuracy": 0.808125
        },
        "0.01": null
      },
      "auroc": 0.9025750434027777
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7814,
          "fn": 1786,
          "accuracy": 0.8139583333333333
        },
        "0.01": null
      },
      "auroc": 0.9161904296875001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1494,
          "fn": 906,
          "accuracy": 0.6225
        },
        "0.01": null
      },
      "auroc": 0.901174826388889
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1494,
          "fn": 906,
          "accuracy": 0.6225
        },
        "0.01": null
      },
      "auroc": 0.901174826388889
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1322,
          "fn": 1078,
          "accuracy": 0.5508333333333333
        },
        "0.01": null
      },
      "auroc": 0.8738640625
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1322,
          "fn": 1078,
          "accuracy": 0.5508333333333333
        },
        "0.01": null
      },
      "auroc": 0.8738640625
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2816,
          "fn": 1984,
          "accuracy": 0.5866666666666667
        },
        "0.01": null
      },
      "auroc": 0.8875194444444445
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2816,
          "fn": 1984,
          "accuracy": 0.5866666666666667
        },
        "0.01": null
      },
      "auroc": 0.8875194444444445
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1059,
          "fn": 1341,
          "accuracy": 0.44125
        },
        "0.01": null
      },
      "auroc": 0.8341490451388889
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1059,
          "fn": 1341,
          "accuracy": 0.44125
        },
        "0.01": null
      },
      "auroc": 0.8341490451388889
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 782,
          "fn": 1618,
          "accuracy": 0.3258333333333333
        },
        "0.01": null
      },
      "auroc": 0.7852196180555555
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 782,
          "fn": 1618,
          "accuracy": 0.3258333333333333
        },
        "0.01": null
      },
      "auroc": 0.7852196180555555
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1841,
          "fn": 2959,
          "accuracy": 0.38354166666666667
        },
        "0.01": null
      },
      "auroc": 0.8096843315972222
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1841,
          "fn": 2959,
          "accuracy": 0.38354166666666667
        },
        "0.01": null
      },
      "auroc": 0.8096843315972222
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1999,
          "fn": 401,
          "accuracy": 0.8329166666666666
        },
        "0.01": null
      },
      "auroc": 0.925193142361111
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1999,
          "fn": 401,
          "accuracy": 0.8329166666666666
        },
        "0.01": null
      },
      "auroc": 0.925193142361111
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1993,
          "fn": 407,
          "accuracy": 0.8304166666666667
        },
        "0.01": null
      },
      "auroc": 0.9189413194444445
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1993,
          "fn": 407,
          "accuracy": 0.8304166666666667
        },
        "0.01": null
      },
      "auroc": 0.9189413194444445
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3992,
          "fn": 808,
          "accuracy": 0.8316666666666667
        },
        "0.01": null
      },
      "auroc": 0.9220672309027778
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3992,
          "fn": 808,
          "accuracy": 0.8316666666666667
        },
        "0.01": null
      },
      "auroc": 0.9220672309027778
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1959,
          "fn": 441,
          "accuracy": 0.81625
        },
        "0.01": null
      },
      "auroc": 0.9287365451388889
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1959,
          "fn": 441,
          "accuracy": 0.81625
        },
        "0.01": null
      },
      "auroc": 0.9287365451388889
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1379,
          "fn": 1021,
          "accuracy": 0.5745833333333333
        },
        "0.01": null
      },
      "auroc": 0.8634143229166666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1379,
          "fn": 1021,
          "accuracy": 0.5745833333333333
        },
        "0.01": null
      },
      "auroc": 0.8634143229166666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3338,
          "fn": 1462,
          "accuracy": 0.6954166666666667
        },
        "0.01": null
      },
      "auroc": 0.8960754340277777
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3338,
          "fn": 1462,
          "accuracy": 0.6954166666666667
        },
        "0.01": null
      },
      "auroc": 0.8960754340277777
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1289,
          "fn": 1111,
          "accuracy": 0.5370833333333334
        },
        "0.01": null
      },
      "auroc": 0.85799453125
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1289,
          "fn": 1111,
          "accuracy": 0.5370833333333334
        },
        "0.01": null
      },
      "auroc": 0.85799453125
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1064,
          "fn": 1336,
          "accuracy": 0.44333333333333336
        },
        "0.01": null
      },
      "auroc": 0.8323888020833332
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1064,
          "fn": 1336,
          "accuracy": 0.44333333333333336
        },
        "0.01": null
      },
      "auroc": 0.8323888020833332
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2353,
          "fn": 2447,
          "accuracy": 0.49020833333333336
        },
        "0.01": null
      },
      "auroc": 0.8451916666666668
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2353,
          "fn": 2447,
          "accuracy": 0.49020833333333336
        },
        "0.01": null
      },
      "auroc": 0.8451916666666668
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 20249,
          "fn": 6151,
          "accuracy": 0.7670075757575757
        },
        "0.01": null
      },
      "auroc": 0.9207707859848484
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 11475,
          "fn": 2925,
          "accuracy": 0.796875
        },
        "0.01": null
      },
      "auroc": 0.9157959201388889
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 31724,
          "fn": 9076,
          "accuracy": 0.7775490196078432
        },
        "0.01": null
      },
      "auroc": 0.9190149509803922
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14932,
          "fn": 11468,
          "accuracy": 0.5656060606060606
        },
        "0.01": null
      },
      "auroc": 0.8638175110479798
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 11156,
          "fn": 3244,
          "accuracy": 0.7747222222222222
        },
        "0.01": null
      },
      "auroc": 0.9052691695601852
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 26088,
          "fn": 14712,
          "accuracy": 0.6394117647058823
        },
        "0.01": null
      },
      "auroc": 0.8784475081699346
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 35181,
          "fn": 17619,
          "accuracy": 0.6663068181818181
        },
        "0.01": null
      },
      "auroc": 0.8922941485164142
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 22631,
          "fn": 6169,
          "accuracy": 0.7857986111111112
        },
        "0.01": null
      },
      "auroc": 0.910532544849537
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 57812,
          "fn": 23788,
          "accuracy": 0.7084803921568628
        },
        "0.01": null
      },
      "auroc": 0.8987312295751634
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9832302083333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.9612510416666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.972240625
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9801104166666668
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9132916666666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        }
      },
      "auroc": 0.9467010416666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.9816703125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        }
      },
      "auroc": 0.9372713541666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 697,
          "fn": 103,
          "accuracy": 0.87125
        },
        "0.01": {
          "tp": 297,
          "fn": 503,
          "accuracy": 0.37125
        }
      },
      "auroc": 0.9594708333333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.994065625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9748177083333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.9844416666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9685239583333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9751697916666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.971846875
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9812947916666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.97499375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        },
        "0.01": {
          "tp": 442,
          "fn": 358,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.9781442708333332
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9863333333333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9792145833333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        }
      },
      "auroc": 0.9827739583333335
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9686791666666668
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.977009375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.9728442708333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        }
      },
      "auroc": 0.97750625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.9781119791666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 752,
          "fn": 48,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 486,
          "fn": 314,
          "accuracy": 0.6075
        }
      },
      "auroc": 0.9778091145833333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9903583333333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9929822916666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9916703125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9598802083333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9784947916666668
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        }
      },
      "auroc": 0.9691875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        }
      },
      "auroc": 0.9751192708333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9857385416666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 759,
          "fn": 41,
          "accuracy": 0.94875
        },
        "0.01": {
          "tp": 528,
          "fn": 272,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9804289062500001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9872364583333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.9654041666666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        }
      },
      "auroc": 0.9763203125000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.907628125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9624447916666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.9350364583333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.9474322916666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9639244791666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 695,
          "fn": 105,
          "accuracy": 0.86875
        },
        "0.01": {
          "tp": 408,
          "fn": 392,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9556783854166667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.994525
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9796958333333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.9871104166666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9856760416666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9525218750000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9690989583333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9901005208333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        }
      },
      "auroc": 0.9661088541666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 752,
          "fn": 48,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 445,
          "fn": 355,
          "accuracy": 0.55625
        }
      },
      "auroc": 0.9781046875000001
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.967971875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.967971875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9568302083333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9568302083333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9624010416666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9624010416666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.89205
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.89205
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.7842520833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.7842520833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.8381510416666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.8381510416666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9820145833333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9820145833333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9491249999999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9491249999999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9655697916666668
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9655697916666668
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9779437499999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9779437499999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7658968749999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7658968749999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.8719203124999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.8719203124999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9653875000000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9653875000000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9150625000000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9150625000000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.940225
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.940225
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2034,
          "fn": 166,
          "accuracy": 0.9245454545454546
        },
        "0.01": {
          "tp": 1364,
          "fn": 836,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9746469696969696
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1115,
          "fn": 85,
          "accuracy": 0.9291666666666667
        },
        "0.01": {
          "tp": 607,
          "fn": 593,
          "accuracy": 0.5058333333333334
        }
      },
      "auroc": 0.9755609375000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3149,
          "fn": 251,
          "accuracy": 0.9261764705882353
        },
        "0.01": {
          "tp": 1971,
          "fn": 1429,
          "accuracy": 0.5797058823529412
        }
      },
      "auroc": 0.9749695465686274
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1650,
          "fn": 550,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 736,
          "fn": 1464,
          "accuracy": 0.33454545454545453
        }
      },
      "auroc": 0.9219695075757576
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1045,
          "fn": 155,
          "accuracy": 0.8708333333333333
        },
        "0.01": {
          "tp": 541,
          "fn": 659,
          "accuracy": 0.4508333333333333
        }
      },
      "auroc": 0.9598220486111111
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2695,
          "fn": 705,
          "accuracy": 0.7926470588235294
        },
        "0.01": {
          "tp": 1277,
          "fn": 2123,
          "accuracy": 0.37558823529411767
        }
      },
      "auroc": 0.9353292279411765
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3684,
          "fn": 716,
          "accuracy": 0.8372727272727273
        },
        "0.01": {
          "tp": 2100,
          "fn": 2300,
          "accuracy": 0.4772727272727273
        }
      },
      "auroc": 0.9483082386363635
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2160,
          "fn": 240,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 1148,
          "fn": 1252,
          "accuracy": 0.47833333333333333
        }
      },
      "auroc": 0.9676914930555556
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5844,
          "fn": 956,
          "accuracy": 0.8594117647058823
        },
        "0.01": {
          "tp": 3248,
          "fn": 3552,
          "accuracy": 0.4776470588235294
        }
      },
      "auroc": 0.9551493872549018
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9832302083333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.9612510416666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.972240625
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9801104166666668
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9132916666666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        }
      },
      "auroc": 0.9467010416666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.9816703125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        }
      },
      "auroc": 0.9372713541666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 697,
          "fn": 103,
          "accuracy": 0.87125
        },
        "0.01": {
          "tp": 297,
          "fn": 503,
          "accuracy": 0.37125
        }
      },
      "auroc": 0.9594708333333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.994065625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9748177083333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.9844416666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9685239583333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9751697916666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.971846875
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9812947916666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.97499375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        },
        "0.01": {
          "tp": 442,
          "fn": 358,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.9781442708333332
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9863333333333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9792145833333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        }
      },
      "auroc": 0.9827739583333335
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9686791666666668
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.977009375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.9728442708333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        }
      },
      "auroc": 0.97750625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.9781119791666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 752,
          "fn": 48,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 486,
          "fn": 314,
          "accuracy": 0.6075
        }
      },
      "auroc": 0.9778091145833333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9903583333333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9929822916666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9916703125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9598802083333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9784947916666668
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        }
      },
      "auroc": 0.9691875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        }
      },
      "auroc": 0.9751192708333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9857385416666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 759,
          "fn": 41,
          "accuracy": 0.94875
        },
        "0.01": {
          "tp": 528,
          "fn": 272,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9804289062500001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9872364583333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.9654041666666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        }
      },
      "auroc": 0.9763203125000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.907628125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9624447916666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.9350364583333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.9474322916666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9639244791666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 695,
          "fn": 105,
          "accuracy": 0.86875
        },
        "0.01": {
          "tp": 408,
          "fn": 392,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9556783854166667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.994525
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9796958333333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.9871104166666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9856760416666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9525218750000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9690989583333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9901005208333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        }
      },
      "auroc": 0.9661088541666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 752,
          "fn": 48,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 445,
          "fn": 355,
          "accuracy": 0.55625
        }
      },
      "auroc": 0.9781046875000001
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.967971875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.967971875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9568302083333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9568302083333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9624010416666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9624010416666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.89205
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.89205
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.7842520833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.7842520833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.8381510416666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.8381510416666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9820145833333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9820145833333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9491249999999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9491249999999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9655697916666668
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9655697916666668
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9779437499999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9779437499999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7658968749999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7658968749999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.8719203124999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.8719203124999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9653875000000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9653875000000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9150625000000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9150625000000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.940225
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.940225
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2034,
          "fn": 166,
          "accuracy": 0.9245454545454546
        },
        "0.01": {
          "tp": 1364,
          "fn": 836,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9746469696969696
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1115,
          "fn": 85,
          "accuracy": 0.9291666666666667
        },
        "0.01": {
          "tp": 607,
          "fn": 593,
          "accuracy": 0.5058333333333334
        }
      },
      "auroc": 0.9755609375000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3149,
          "fn": 251,
          "accuracy": 0.9261764705882353
        },
        "0.01": {
          "tp": 1971,
          "fn": 1429,
          "accuracy": 0.5797058823529412
        }
      },
      "auroc": 0.9749695465686274
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1650,
          "fn": 550,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 736,
          "fn": 1464,
          "accuracy": 0.33454545454545453
        }
      },
      "auroc": 0.9219695075757576
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1045,
          "fn": 155,
          "accuracy": 0.8708333333333333
        },
        "0.01": {
          "tp": 541,
          "fn": 659,
          "accuracy": 0.4508333333333333
        }
      },
      "auroc": 0.9598220486111111
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2695,
          "fn": 705,
          "accuracy": 0.7926470588235294
        },
        "0.01": {
          "tp": 1277,
          "fn": 2123,
          "accuracy": 0.37558823529411767
        }
      },
      "auroc": 0.9353292279411765
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3684,
          "fn": 716,
          "accuracy": 0.8372727272727273
        },
        "0.01": {
          "tp": 2100,
          "fn": 2300,
          "accuracy": 0.4772727272727273
        }
      },
      "auroc": 0.9483082386363635
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2160,
          "fn": 240,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 1148,
          "fn": 1252,
          "accuracy": 0.47833333333333333
        }
      },
      "auroc": 0.9676914930555556
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 5844,
          "fn": 956,
          "accuracy": 0.8594117647058823
        },
        "0.01": {
          "tp": 3248,
          "fn": 3552,
          "accuracy": 0.4776470588235294
        }
      },
      "auroc": 0.9551493872549018
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.9742135416666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9440958333333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9591546874999999
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.9707229166666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.887584375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 74,
          "fn": 326,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9291536458333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        }
      },
      "auroc": 0.9724682291666668
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        }
      },
      "auroc": 0.9158401041666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 647,
          "fn": 153,
          "accuracy": 0.80875
        },
        "0.01": {
          "tp": 180,
          "fn": 620,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9441541666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.994578125
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.9754760416666668
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9850270833333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.950584375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.975665625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.963125
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        }
      },
      "auroc": 0.97258125
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        }
      },
      "auroc": 0.9755708333333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 741,
          "fn": 59,
          "accuracy": 0.92625
        },
        "0.01": {
          "tp": 461,
          "fn": 339,
          "accuracy": 0.57625
        }
      },
      "auroc": 0.9740760416666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        }
      },
      "auroc": 0.979903125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9754854166666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        }
      },
      "auroc": 0.9776942708333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9550947916666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9736989583333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.9643968750000002
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9674989583333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9745921875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 728,
          "fn": 72,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 425,
          "fn": 375,
          "accuracy": 0.53125
        }
      },
      "auroc": 0.9710455729166667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9900864583333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9958270833333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9929567708333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.9353489583333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9722177083333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.9537833333333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.9627177083333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9840223958333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 731,
          "fn": 69,
          "accuracy": 0.91375
        },
        "0.01": {
          "tp": 533,
          "fn": 267,
          "accuracy": 0.66625
        }
      },
      "auroc": 0.9733700520833333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9871489583333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9575479166666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9723484375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.8730864583333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9553041666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        },
        "0.01": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        }
      },
      "auroc": 0.9141953125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        }
      },
      "auroc": 0.9301177083333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        }
      },
      "auroc": 0.9564260416666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 655,
          "fn": 145,
          "accuracy": 0.81875
        },
        "0.01": {
          "tp": 386,
          "fn": 414,
          "accuracy": 0.4825
        }
      },
      "auroc": 0.9432718750000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9916854166666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.97119375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9814395833333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9783708333333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.9357614583333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        }
      },
      "auroc": 0.9570661458333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.985028125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9534776041666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 720,
          "fn": 80,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 326,
          "fn": 474,
          "accuracy": 0.4075
        }
      },
      "auroc": 0.9692528645833334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        }
      },
      "auroc": 0.963378125
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        }
      },
      "auroc": 0.963378125
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.9510614583333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.9510614583333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 155,
          "fn": 245,
          "accuracy": 0.3875
        }
      },
      "auroc": 0.9572197916666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 155,
          "fn": 245,
          "accuracy": 0.3875
        }
      },
      "auroc": 0.9572197916666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.8709927083333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.8709927083333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.7623875
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.7623875
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        },
        "0.01": {
          "tp": 39,
          "fn": 361,
          "accuracy": 0.0975
        }
      },
      "auroc": 0.8166901041666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        },
        "0.01": {
          "tp": 39,
          "fn": 361,
          "accuracy": 0.0975
        }
      },
      "auroc": 0.8166901041666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9728145833333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9728145833333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.931259375
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.931259375
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9520369791666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9520369791666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.9660447916666666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.9660447916666666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7499625
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7499625
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        }
      },
      "auroc": 0.8580036458333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        }
      },
      "auroc": 0.8580036458333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.95618125
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.95618125
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9012114583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9012114583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        }
      },
      "auroc": 0.9286963541666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        }
      },
      "auroc": 0.9286963541666666
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1981,
          "fn": 219,
          "accuracy": 0.9004545454545455
        },
        "0.01": {
          "tp": 1128,
          "fn": 1072,
          "accuracy": 0.5127272727272727
        }
      },
      "auroc": 0.967911553030303
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1085,
          "fn": 115,
          "accuracy": 0.9041666666666667
        },
        "0.01": {
          "tp": 637,
          "fn": 563,
          "accuracy": 0.5308333333333334
        }
      },
      "auroc": 0.969937673611111
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3066,
          "fn": 334,
          "accuracy": 0.9017647058823529
        },
        "0.01": {
          "tp": 1765,
          "fn": 1635,
          "accuracy": 0.5191176470588236
        }
      },
      "auroc": 0.9686266544117648
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1518,
          "fn": 682,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 468,
          "fn": 1732,
          "accuracy": 0.21272727272727274
        }
      },
      "auroc": 0.9053718749999999
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 980,
          "fn": 220,
          "accuracy": 0.8166666666666667
        },
        "0.01": {
          "tp": 545,
          "fn": 655,
          "accuracy": 0.45416666666666666
        }
      },
      "auroc": 0.9500387152777778
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2498,
          "fn": 902,
          "accuracy": 0.7347058823529412
        },
        "0.01": {
          "tp": 1013,
          "fn": 2387,
          "accuracy": 0.2979411764705882
        }
      },
      "auroc": 0.9211366421568628
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3499,
          "fn": 901,
          "accuracy": 0.7952272727272728
        },
        "0.01": {
          "tp": 1596,
          "fn": 2804,
          "accuracy": 0.36272727272727273
        }
      },
      "auroc": 0.9366417140151515
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2065,
          "fn": 335,
          "accuracy": 0.8604166666666667
        },
        "0.01": {
          "tp": 1182,
          "fn": 1218,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.9599881944444445
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5564,
          "fn": 1236,
          "accuracy": 0.8182352941176471
        },
        "0.01": {
          "tp": 2778,
          "fn": 4022,
          "accuracy": 0.40852941176470586
        }
      },
      "auroc": 0.9448816482843136
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.970425
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9327000000000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 99,
          "fn": 301,
          "accuracy": 0.2475
        }
      },
      "auroc": 0.9515625000000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.96816875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.8776958333333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 69,
          "fn": 331,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.9229322916666668
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        }
      },
      "auroc": 0.9692968750000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.9051979166666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 613,
          "fn": 187,
          "accuracy": 0.76625
        },
        "0.01": {
          "tp": 168,
          "fn": 632,
          "accuracy": 0.21
        }
      },
      "auroc": 0.9372473958333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9940979166666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9656
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9798489583333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9450489583333335
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.97378125
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9594151041666668
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9695734375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.969690625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 737,
          "fn": 63,
          "accuracy": 0.92125
        },
        "0.01": {
          "tp": 438,
          "fn": 362,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.9696320312500001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.97408125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.968796875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9714390625000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9462364583333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9720958333333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9591661458333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        }
      },
      "auroc": 0.9601588541666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9704463541666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 705,
          "fn": 95,
          "accuracy": 0.88125
        },
        "0.01": {
          "tp": 380,
          "fn": 420,
          "accuracy": 0.475
        }
      },
      "auroc": 0.9653026041666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9881906250000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9936270833333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9909088541666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9199322916666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.958428125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9391802083333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9540614583333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9760276041666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 710,
          "fn": 90,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 485,
          "fn": 315,
          "accuracy": 0.60625
        }
      },
      "auroc": 0.96504453125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9865666666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.9384385416666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        }
      },
      "auroc": 0.9625026041666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.8659656250000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.9423833333333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        },
        "0.01": {
          "tp": 123,
          "fn": 277,
          "accuracy": 0.3075
        }
      },
      "auroc": 0.9041744791666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9262661458333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 171,
          "fn": 229,
          "accuracy": 0.4275
        }
      },
      "auroc": 0.9404109375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 630,
          "fn": 170,
          "accuracy": 0.7875
        },
        "0.01": {
          "tp": 355,
          "fn": 445,
          "accuracy": 0.44375
        }
      },
      "auroc": 0.9333385416666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.9898927083333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.959375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9746338541666668
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.9728343749999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.911478125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 100,
          "fn": 300,
          "accuracy": 0.25
        }
      },
      "auroc": 0.94215625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.9813635416666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        }
      },
      "auroc": 0.9354265625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 700,
          "fn": 100,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 269,
          "fn": 531,
          "accuracy": 0.33625
        }
      },
      "auroc": 0.9583950520833333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.957884375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.957884375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.9379604166666665
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.9379604166666665
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 136,
          "fn": 264,
          "accuracy": 0.34
        }
      },
      "auroc": 0.9479223958333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 136,
          "fn": 264,
          "accuracy": 0.34
        }
      },
      "auroc": 0.9479223958333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.8538197916666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.8538197916666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.741503125
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.741503125
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        },
        "0.01": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.7976614583333332
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        },
        "0.01": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.7976614583333332
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.97001875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.97001875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.9220041666666666
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.9220041666666666
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9460114583333332
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9460114583333332
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.963840625
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.963840625
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7292989583333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7292989583333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 42,
          "fn": 358,
          "accuracy": 0.105
        }
      },
      "auroc": 0.8465697916666666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 42,
          "fn": 358,
          "accuracy": 0.105
        }
      },
      "auroc": 0.8465697916666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9461572916666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9461572916666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.8855156249999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.8855156249999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        }
      },
      "auroc": 0.9158364583333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        }
      },
      "auroc": 0.9158364583333334
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1947,
          "fn": 253,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 1077,
          "fn": 1123,
          "accuracy": 0.48954545454545456
        }
      },
      "auroc": 0.9631795454545455
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1033,
          "fn": 167,
          "accuracy": 0.8608333333333333
        },
        "0.01": {
          "tp": 545,
          "fn": 655,
          "accuracy": 0.45416666666666666
        }
      },
      "auroc": 0.9597562499999999
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2980,
          "fn": 420,
          "accuracy": 0.8764705882352941
        },
        "0.01": {
          "tp": 1622,
          "fn": 1778,
          "accuracy": 0.47705882352941176
        }
      },
      "auroc": 0.9619713235294116
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1463,
          "fn": 737,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 432,
          "fn": 1768,
          "accuracy": 0.19636363636363635
        }
      },
      "auroc": 0.8940426136363637
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 943,
          "fn": 257,
          "accuracy": 0.7858333333333334
        },
        "0.01": {
          "tp": 488,
          "fn": 712,
          "accuracy": 0.4066666666666667
        }
      },
      "auroc": 0.9393104166666668
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2406,
          "fn": 994,
          "accuracy": 0.7076470588235294
        },
        "0.01": {
          "tp": 920,
          "fn": 2480,
          "accuracy": 0.27058823529411763
        }
      },
      "auroc": 0.9100194852941177
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3410,
          "fn": 990,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 1509,
          "fn": 2891,
          "accuracy": 0.34295454545454546
        }
      },
      "auroc": 0.9286110795454545
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1976,
          "fn": 424,
          "accuracy": 0.8233333333333334
        },
        "0.01": {
          "tp": 1033,
          "fn": 1367,
          "accuracy": 0.43041666666666667
        }
      },
      "auroc": 0.9495333333333333
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 5386,
          "fn": 1414,
          "accuracy": 0.7920588235294118
        },
        "0.01": {
          "tp": 2542,
          "fn": 4258,
          "accuracy": 0.3738235294117647
        }
      },
      "auroc": 0.9359954044117647
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        }
      },
      "auroc": 0.9810812499999999
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.9551208333333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 150,
          "fn": 250,
          "accuracy": 0.375
        }
      },
      "auroc": 0.9681010416666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9771375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.9049302083333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        }
      },
      "auroc": 0.9410338541666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        }
      },
      "auroc": 0.979109375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9300255208333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 677,
          "fn": 123,
          "accuracy": 0.84625
        },
        "0.01": {
          "tp": 251,
          "fn": 549,
          "accuracy": 0.31375
        }
      },
      "auroc": 0.9545674479166666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.993996875
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9715510416666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9827739583333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.9575552083333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.974896875
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        }
      },
      "auroc": 0.9662260416666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        }
      },
      "auroc": 0.9757760416666665
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9732239583333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 751,
          "fn": 49,
          "accuracy": 0.93875
        },
        "0.01": {
          "tp": 438,
          "fn": 362,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.9745
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9839885416666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9776520833333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        }
      },
      "auroc": 0.9808203125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9615864583333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9763989583333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.9689927083333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9727875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        }
      },
      "auroc": 0.9770255208333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 738,
          "fn": 62,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 477,
          "fn": 323,
          "accuracy": 0.59625
        }
      },
      "auroc": 0.9749065104166668
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9903708333333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9938385416666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9921046874999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.9423989583333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.974109375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.9582541666666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.9663848958333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        }
      },
      "auroc": 0.9839739583333335
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 747,
          "fn": 53,
          "accuracy": 0.93375
        },
        "0.01": {
          "tp": 522,
          "fn": 278,
          "accuracy": 0.6525
        }
      },
      "auroc": 0.9751794270833334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9873802083333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.958715625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9730479166666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.8945072916666665
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9583854166666668
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 144,
          "fn": 256,
          "accuracy": 0.36
        }
      },
      "auroc": 0.9264463541666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        }
      },
      "auroc": 0.94094375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9585505208333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 670,
          "fn": 130,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 388,
          "fn": 412,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9497471354166669
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9935666666666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.9756572916666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        }
      },
      "auroc": 0.9846119791666665
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.98306875
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9441604166666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        }
      },
      "auroc": 0.9636145833333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.9883177083333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 123,
          "fn": 277,
          "accuracy": 0.3075
        }
      },
      "auroc": 0.9599088541666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 744,
          "fn": 56,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 390,
          "fn": 410,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.97411328125
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.962421875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.962421875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.9523864583333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.9523864583333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9574041666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9574041666666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.8790770833333335
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.8790770833333335
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.7764229166666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.7764229166666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        }
      },
      "auroc": 0.82775
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        }
      },
      "auroc": 0.82775
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.9797177083333335
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.9797177083333335
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.9428281249999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.9428281249999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9612729166666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9612729166666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9746270833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9746270833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7545041666666666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7545041666666666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        },
        "0.01": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        }
      },
      "auroc": 0.864565625
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        },
        "0.01": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        }
      },
      "auroc": 0.864565625
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9604802083333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9604802083333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9092260416666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9092260416666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        }
      },
      "auroc": 0.9348531250000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        }
      },
      "auroc": 0.9348531250000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2009,
          "fn": 191,
          "accuracy": 0.9131818181818182
        },
        "0.01": {
          "tp": 1290,
          "fn": 910,
          "accuracy": 0.5863636363636363
        }
      },
      "auroc": 0.9715189393939394
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1093,
          "fn": 107,
          "accuracy": 0.9108333333333334
        },
        "0.01": {
          "tp": 599,
          "fn": 601,
          "accuracy": 0.49916666666666665
        }
      },
      "auroc": 0.9720892361111111
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3102,
          "fn": 298,
          "accuracy": 0.9123529411764706
        },
        "0.01": {
          "tp": 1889,
          "fn": 1511,
          "accuracy": 0.5555882352941176
        }
      },
      "auroc": 0.9717202205882354
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1585,
          "fn": 615,
          "accuracy": 0.7204545454545455
        },
        "0.01": {
          "tp": 620,
          "fn": 1580,
          "accuracy": 0.2818181818181818
        }
      },
      "auroc": 0.9137838068181818
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1026,
          "fn": 174,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 538,
          "fn": 662,
          "accuracy": 0.4483333333333333
        }
      },
      "auroc": 0.9554802083333334
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2611,
          "fn": 789,
          "accuracy": 0.7679411764705882
        },
        "0.01": {
          "tp": 1158,
          "fn": 2242,
          "accuracy": 0.34058823529411764
        }
      },
      "auroc": 0.9285001838235294
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3594,
          "fn": 806,
          "accuracy": 0.8168181818181818
        },
        "0.01": {
          "tp": 1910,
          "fn": 2490,
          "accuracy": 0.4340909090909091
        }
      },
      "auroc": 0.9426513731060606
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2119,
          "fn": 281,
          "accuracy": 0.8829166666666667
        },
        "0.01": {
          "tp": 1137,
          "fn": 1263,
          "accuracy": 0.47375
        }
      },
      "auroc": 0.9637847222222222
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5713,
          "fn": 1087,
          "accuracy": 0.8401470588235294
        },
        "0.01": {
          "tp": 3047,
          "fn": 3753,
          "accuracy": 0.4480882352941176
        }
      },
      "auroc": 0.9501102022058823
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9654447916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.946140625
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        }
      },
      "auroc": 0.9557927083333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9693197916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9302239583333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 138,
          "fn": 262,
          "accuracy": 0.345
        }
      },
      "auroc": 0.949771875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 171,
          "fn": 229,
          "accuracy": 0.4275
        }
      },
      "auroc": 0.9673822916666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9381822916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 687,
          "fn": 113,
          "accuracy": 0.85875
        },
        "0.01": {
          "tp": 279,
          "fn": 521,
          "accuracy": 0.34875
        }
      },
      "auroc": 0.9527822916666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.9711135416666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9155520833333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.9433328125
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.9310208333333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.8498208333333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        }
      },
      "auroc": 0.8904208333333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.9510671875000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        }
      },
      "auroc": 0.8826864583333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 593,
          "fn": 207,
          "accuracy": 0.74125
        },
        "0.01": {
          "tp": 292,
          "fn": 508,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9168768229166666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9723895833333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.9042291666666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 138,
          "fn": 262,
          "accuracy": 0.345
        }
      },
      "auroc": 0.9383093749999999
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.9653406250000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.8354885416666665
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        },
        "0.01": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        }
      },
      "auroc": 0.9004145833333332
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.9688651041666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.8698588541666665
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 616,
          "fn": 184,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 223,
          "fn": 577,
          "accuracy": 0.27875
        }
      },
      "auroc": 0.9193619791666665
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9506020833333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9921458333333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        }
      },
      "auroc": 0.9713739583333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.9517416666666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.8250510416666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8883963541666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.951171875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9085984375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 612,
          "fn": 188,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 331,
          "fn": 469,
          "accuracy": 0.41375
        }
      },
      "auroc": 0.92988515625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        }
      },
      "auroc": 0.9703843750000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.948665625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.959525
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9297739583333332
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.832590625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 76,
          "fn": 324,
          "accuracy": 0.19
        }
      },
      "auroc": 0.8811822916666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        }
      },
      "auroc": 0.9500791666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        },
        "0.01": {
          "tp": 107,
          "fn": 293,
          "accuracy": 0.2675
        }
      },
      "auroc": 0.890628125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 593,
          "fn": 207,
          "accuracy": 0.74125
        },
        "0.01": {
          "tp": 270,
          "fn": 530,
          "accuracy": 0.3375
        }
      },
      "auroc": 0.9203536458333335
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        }
      },
      "auroc": 0.9856322916666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9690083333333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        }
      },
      "auroc": 0.9773203125000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        }
      },
      "auroc": 0.9753625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.942884375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9591234375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9804973958333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9559463541666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 719,
          "fn": 81,
          "accuracy": 0.89875
        },
        "0.01": {
          "tp": 370,
          "fn": 430,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.968221875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9632947916666668
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9632947916666668
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9513489583333332
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9513489583333332
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9573218750000001
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9573218750000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.9032447916666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.9032447916666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.8437510416666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.8437510416666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        }
      },
      "auroc": 0.8734979166666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        }
      },
      "auroc": 0.8734979166666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9650302083333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9650302083333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9351604166666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9351604166666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9500953125000001
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9500953125000001
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9591989583333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9591989583333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.8204708333333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.8204708333333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.8898348958333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.8898348958333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9531906250000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9531906250000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.9276239583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.9276239583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": {
          "tp": 136,
          "fn": 264,
          "accuracy": 0.34
        }
      },
      "auroc": 0.9404072916666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": {
          "tp": 136,
          "fn": 264,
          "accuracy": 0.34
        }
      },
      "auroc": 0.9404072916666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1904,
          "fn": 296,
          "accuracy": 0.8654545454545455
        },
        "0.01": {
          "tp": 995,
          "fn": 1205,
          "accuracy": 0.45227272727272727
        }
      },
      "auroc": 0.9599569128787878
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 995,
          "fn": 205,
          "accuracy": 0.8291666666666667
        },
        "0.01": {
          "tp": 492,
          "fn": 708,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9459569444444444
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2899,
          "fn": 501,
          "accuracy": 0.8526470588235294
        },
        "0.01": {
          "tp": 1487,
          "fn": 1913,
          "accuracy": 0.4373529411764706
        }
      },
      "auroc": 0.9550157475490195
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1676,
          "fn": 524,
          "accuracy": 0.7618181818181818
        },
        "0.01": {
          "tp": 640,
          "fn": 1560,
          "accuracy": 0.2909090909090909
        }
      },
      "auroc": 0.9273558712121213
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 714,
          "fn": 486,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 196,
          "fn": 1004,
          "accuracy": 0.16333333333333333
        }
      },
      "auroc": 0.8693432291666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2390,
          "fn": 1010,
          "accuracy": 0.7029411764705882
        },
        "0.01": {
          "tp": 836,
          "fn": 2564,
          "accuracy": 0.24588235294117647
        }
      },
      "auroc": 0.9068808210784314
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3580,
          "fn": 820,
          "accuracy": 0.8136363636363636
        },
        "0.01": {
          "tp": 1635,
          "fn": 2765,
          "accuracy": 0.3715909090909091
        }
      },
      "auroc": 0.9436563920454546
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1709,
          "fn": 691,
          "accuracy": 0.7120833333333333
        },
        "0.01": {
          "tp": 688,
          "fn": 1712,
          "accuracy": 0.2866666666666667
        }
      },
      "auroc": 0.9076500868055556
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 5289,
          "fn": 1511,
          "accuracy": 0.7777941176470589
        },
        "0.01": {
          "tp": 2323,
          "fn": 4477,
          "accuracy": 0.3416176470588235
        }
      },
      "auroc": 0.9309482843137256
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9832302083333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.9612510416666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.972240625
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.980115625
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9132854166666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        }
      },
      "auroc": 0.9467005208333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.9816729166666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        }
      },
      "auroc": 0.9372682291666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 697,
          "fn": 103,
          "accuracy": 0.87125
        },
        "0.01": {
          "tp": 297,
          "fn": 503,
          "accuracy": 0.37125
        }
      },
      "auroc": 0.9594705729166666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.994065625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9748020833333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.9844338541666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9681927083333335
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.9750927083333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9716427083333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9811291666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.9749473958333335
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        },
        "0.01": {
          "tp": 439,
          "fn": 361,
          "accuracy": 0.54875
        }
      },
      "auroc": 0.9780382812499999
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9863333333333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9792145833333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        }
      },
      "auroc": 0.9827739583333335
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9686614583333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.9772427083333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.9729520833333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        }
      },
      "auroc": 0.9774973958333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.9782286458333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 752,
          "fn": 48,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 486,
          "fn": 314,
          "accuracy": 0.6075
        }
      },
      "auroc": 0.9778630208333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9901260416666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9930864583333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.99160625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9597520833333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9782697916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        }
      },
      "auroc": 0.9690109375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9749390625000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9856781250000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 757,
          "fn": 43,
          "accuracy": 0.94625
        },
        "0.01": {
          "tp": 527,
          "fn": 273,
          "accuracy": 0.65875
        }
      },
      "auroc": 0.98030859375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9872364583333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.9653447916666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        }
      },
      "auroc": 0.9762906250000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9073145833333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9623749999999999
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9348447916666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.9472755208333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9638598958333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 695,
          "fn": 105,
          "accuracy": 0.86875
        },
        "0.01": {
          "tp": 407,
          "fn": 393,
          "accuracy": 0.50875
        }
      },
      "auroc": 0.9555677083333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.994525
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9796166666666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.9870708333333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        }
      },
      "auroc": 0.9856197916666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9525156250000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.9690677083333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        }
      },
      "auroc": 0.9900723958333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        }
      },
      "auroc": 0.9660661458333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 752,
          "fn": 48,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 444,
          "fn": 356,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9780692708333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9679833333333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9679833333333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.95676875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.95676875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9623760416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9623760416666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.892015625
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.892015625
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.7844947916666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.7844947916666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        }
      },
      "auroc": 0.8382552083333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        }
      },
      "auroc": 0.8382552083333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9820145833333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9820145833333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9491249999999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9491249999999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9655697916666668
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9655697916666668
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.97794375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.97794375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7659260416666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7659260416666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.8719348958333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.8719348958333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9653875000000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9653875000000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9147927083333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9147927083333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.9400901041666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.9400901041666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2033,
          "fn": 167,
          "accuracy": 0.9240909090909091
        },
        "0.01": {
          "tp": 1363,
          "fn": 837,
          "accuracy": 0.6195454545454545
        }
      },
      "auroc": 0.974623768939394
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1115,
          "fn": 85,
          "accuracy": 0.9291666666666667
        },
        "0.01": {
          "tp": 607,
          "fn": 593,
          "accuracy": 0.5058333333333334
        }
      },
      "auroc": 0.9755526041666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3148,
          "fn": 252,
          "accuracy": 0.9258823529411765
        },
        "0.01": {
          "tp": 1970,
          "fn": 1430,
          "accuracy": 0.5794117647058824
        }
      },
      "auroc": 0.9749515931372549
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1650,
          "fn": 550,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 731,
          "fn": 1469,
          "accuracy": 0.3322727272727273
        }
      },
      "auroc": 0.9218875946969697
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1044,
          "fn": 156,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 540,
          "fn": 660,
          "accuracy": 0.45
        }
      },
      "auroc": 0.959796875
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2694,
          "fn": 706,
          "accuracy": 0.7923529411764706
        },
        "0.01": {
          "tp": 1271,
          "fn": 2129,
          "accuracy": 0.3738235294117647
        }
      },
      "auroc": 0.9352673406862745
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3683,
          "fn": 717,
          "accuracy": 0.8370454545454545
        },
        "0.01": {
          "tp": 2094,
          "fn": 2306,
          "accuracy": 0.4759090909090909
        }
      },
      "auroc": 0.9482556818181818
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2159,
          "fn": 241,
          "accuracy": 0.8995833333333333
        },
        "0.01": {
          "tp": 1147,
          "fn": 1253,
          "accuracy": 0.47791666666666666
        }
      },
      "auroc": 0.9676747395833334
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5842,
          "fn": 958,
          "accuracy": 0.8591176470588235
        },
        "0.01": {
          "tp": 3241,
          "fn": 3559,
          "accuracy": 0.47661764705882353
        }
      },
      "auroc": 0.9551094669117646
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9832302083333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.9612510416666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.972240625
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9801104166666668
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9132916666666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        }
      },
      "auroc": 0.9467010416666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.9816703125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        }
      },
      "auroc": 0.9372713541666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 697,
          "fn": 103,
          "accuracy": 0.87125
        },
        "0.01": {
          "tp": 297,
          "fn": 503,
          "accuracy": 0.37125
        }
      },
      "auroc": 0.9594708333333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.994065625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9748177083333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.9844416666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9684197916666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9751489583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.9717843749999999
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9812427083333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.9749833333333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        },
        "0.01": {
          "tp": 442,
          "fn": 358,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.9781130208333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9863333333333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9792145833333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        }
      },
      "auroc": 0.9827739583333335
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9686791666666668
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9770427083333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.9728609375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        }
      },
      "auroc": 0.97750625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9781286458333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 752,
          "fn": 48,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 488,
          "fn": 312,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9778174479166666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9903583333333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9929822916666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9916703125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9599802083333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9784135416666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        }
      },
      "auroc": 0.969196875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        }
      },
      "auroc": 0.9751692708333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9856979166666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 759,
          "fn": 41,
          "accuracy": 0.94875
        },
        "0.01": {
          "tp": 528,
          "fn": 272,
          "accuracy": 0.66
        }
      },
      "auroc": 0.98043359375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9872364583333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.9654041666666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        }
      },
      "auroc": 0.9763203125000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.907628125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.96229375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9349609375000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.9474322916666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.9638489583333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 695,
          "fn": 105,
          "accuracy": 0.86875
        },
        "0.01": {
          "tp": 407,
          "fn": 393,
          "accuracy": 0.50875
        }
      },
      "auroc": 0.9556406250000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.994525
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9796958333333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.9871104166666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9855864583333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9525218750000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9690541666666668
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9900557291666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        }
      },
      "auroc": 0.9661088541666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 752,
          "fn": 48,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 445,
          "fn": 355,
          "accuracy": 0.55625
        }
      },
      "auroc": 0.9780822916666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.967971875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.967971875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9568302083333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9568302083333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9624010416666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9624010416666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.89205
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.89205
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.7842520833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.7842520833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.8381510416666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.8381510416666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9820145833333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9820145833333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9491249999999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9491249999999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9655697916666668
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9655697916666668
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9779437499999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9779437499999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7658968749999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7658968749999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.8719203124999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.8719203124999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9653875000000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9653875000000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9150625000000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9150625000000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.940225
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.940225
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2034,
          "fn": 166,
          "accuracy": 0.9245454545454546
        },
        "0.01": {
          "tp": 1364,
          "fn": 836,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9746469696969696
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1115,
          "fn": 85,
          "accuracy": 0.9291666666666667
        },
        "0.01": {
          "tp": 607,
          "fn": 593,
          "accuracy": 0.5058333333333334
        }
      },
      "auroc": 0.9755609375000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3149,
          "fn": 251,
          "accuracy": 0.9261764705882353
        },
        "0.01": {
          "tp": 1971,
          "fn": 1429,
          "accuracy": 0.5797058823529412
        }
      },
      "auroc": 0.9749695465686274
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1651,
          "fn": 549,
          "accuracy": 0.7504545454545455
        },
        "0.01": {
          "tp": 736,
          "fn": 1464,
          "accuracy": 0.33454545454545453
        }
      },
      "auroc": 0.9219609848484849
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1044,
          "fn": 156,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 542,
          "fn": 658,
          "accuracy": 0.45166666666666666
        }
      },
      "auroc": 0.9597854166666666
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2695,
          "fn": 705,
          "accuracy": 0.7926470588235294
        },
        "0.01": {
          "tp": 1278,
          "fn": 2122,
          "accuracy": 0.37588235294117645
        }
      },
      "auroc": 0.9353107843137255
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3685,
          "fn": 715,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 2100,
          "fn": 2300,
          "accuracy": 0.4772727272727273
        }
      },
      "auroc": 0.9483039772727272
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2159,
          "fn": 241,
          "accuracy": 0.8995833333333333
        },
        "0.01": {
          "tp": 1149,
          "fn": 1251,
          "accuracy": 0.47875
        }
      },
      "auroc": 0.9676731770833333
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5844,
          "fn": 956,
          "accuracy": 0.8594117647058823
        },
        "0.01": {
          "tp": 3249,
          "fn": 3551,
          "accuracy": 0.4777941176470588
        }
      },
      "auroc": 0.9551401654411764
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8368416666666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6466677083333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7417546875000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8225302083333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5965260416666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7095281250000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8296859375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6215968749999999
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 690,
          "accuracy": 0.1375
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.72564140625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9913614583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9570322916666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9741968750000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.7840177083333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9514177083333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        }
      },
      "auroc": 0.8677177083333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        }
      },
      "auroc": 0.8876895833333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9542249999999999
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 591,
          "fn": 209,
          "accuracy": 0.73875
        },
        "0.01": {
          "tp": 465,
          "fn": 335,
          "accuracy": 0.58125
        }
      },
      "auroc": 0.9209572916666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.8399416666666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.7925791666666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 262,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.8162604166666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7637010416666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.8457791666666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 279,
          "accuracy": 0.3025
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.8047401041666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.8018213541666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        },
        "0.01": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        }
      },
      "auroc": 0.8191791666666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 259,
          "fn": 541,
          "accuracy": 0.32375
        },
        "0.01": {
          "tp": 83,
          "fn": 717,
          "accuracy": 0.10375
        }
      },
      "auroc": 0.8105002604166667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9863875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.8597468749999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.9230671875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.7606989583333335
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.45709375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.6088963541666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.8735432291666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.6584203125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 434,
          "accuracy": 0.4575
        },
        "0.01": {
          "tp": 203,
          "fn": 597,
          "accuracy": 0.25375
        }
      },
      "auroc": 0.7659817708333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9792125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.7169010416666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.8480567708333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7041583333333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.7650145833333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.7345864583333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        }
      },
      "auroc": 0.8416854166666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        }
      },
      "auroc": 0.7409578125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 434,
          "accuracy": 0.4575
        },
        "0.01": {
          "tp": 232,
          "fn": 568,
          "accuracy": 0.29
        }
      },
      "auroc": 0.7913216145833333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.88551875
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6780781250000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 297,
          "accuracy": 0.2575
        },
        "0.01": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        }
      },
      "auroc": 0.7817984375000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7904729166666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6075770833333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.699025
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 273,
          "accuracy": 0.3175
        },
        "0.01": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        }
      },
      "auroc": 0.8379958333333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 379,
          "accuracy": 0.0525
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6428276041666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 652,
          "accuracy": 0.185
        },
        "0.01": {
          "tp": 12,
          "fn": 788,
          "accuracy": 0.015
        }
      },
      "auroc": 0.74041171875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.8825343750000001
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.8825343750000001
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.8277291666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.8277291666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        }
      },
      "auroc": 0.8551317708333335
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        }
      },
      "auroc": 0.8551317708333335
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6995260416666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6995260416666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6356124999999999
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6356124999999999
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6675692708333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6675692708333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7703802083333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7703802083333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6661781250000001
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6661781250000001
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7182791666666666
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7182791666666666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8271729166666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8271729166666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.644175
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.644175
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7356739583333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7356739583333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7904635416666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7904635416666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7272645833333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7272645833333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7588640625
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7588640625
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1034,
          "fn": 1166,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 515,
          "fn": 1685,
          "accuracy": 0.2340909090909091
        }
      },
      "auroc": 0.8626673295454546
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 435,
          "fn": 765,
          "accuracy": 0.3625
        },
        "0.01": {
          "tp": 263,
          "fn": 937,
          "accuracy": 0.21916666666666668
        }
      },
      "auroc": 0.7751675347222222
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1469,
          "fn": 1931,
          "accuracy": 0.4320588235294118
        },
        "0.01": {
          "tp": 778,
          "fn": 2622,
          "accuracy": 0.2288235294117647
        }
      },
      "auroc": 0.8317850490196078
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 1832,
          "accuracy": 0.16727272727272727
        },
        "0.01": {
          "tp": 29,
          "fn": 2171,
          "accuracy": 0.013181818181818182
        }
      },
      "auroc": 0.7387762310606061
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 818,
          "accuracy": 0.31833333333333336
        },
        "0.01": {
          "tp": 238,
          "fn": 962,
          "accuracy": 0.19833333333333333
        }
      },
      "auroc": 0.7039013888888889
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 750,
          "fn": 2650,
          "accuracy": 0.22058823529411764
        },
        "0.01": {
          "tp": 267,
          "fn": 3133,
          "accuracy": 0.07852941176470589
        }
      },
      "auroc": 0.7264674632352941
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1402,
          "fn": 2998,
          "accuracy": 0.31863636363636366
        },
        "0.01": {
          "tp": 544,
          "fn": 3856,
          "accuracy": 0.12363636363636364
        }
      },
      "auroc": 0.8007217803030303
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 817,
          "fn": 1583,
          "accuracy": 0.34041666666666665
        },
        "0.01": {
          "tp": 501,
          "fn": 1899,
          "accuracy": 0.20875
        }
      },
      "auroc": 0.7395344618055555
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2219,
          "fn": 4581,
          "accuracy": 0.3263235294117647
        },
        "0.01": {
          "tp": 1045,
          "fn": 5755,
          "accuracy": 0.1536764705882353
        }
      },
      "auroc": 0.779126256127451
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9660395833333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.9389958333333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 88,
          "fn": 312,
          "accuracy": 0.22
        }
      },
      "auroc": 0.9525177083333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.9586395833333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.8905510416666668
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9245953125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 103,
          "fn": 297,
          "accuracy": 0.2575
        }
      },
      "auroc": 0.9623395833333332
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 41,
          "fn": 359,
          "accuracy": 0.1025
        }
      },
      "auroc": 0.9147734375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 626,
          "fn": 174,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 144,
          "fn": 656,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9385565104166668
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.994415625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9748177083333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.9846166666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.9358166666666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9751697916666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 150,
          "fn": 250,
          "accuracy": 0.375
        }
      },
      "auroc": 0.9554932291666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.9651161458333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.97499375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 727,
          "fn": 73,
          "accuracy": 0.90875
        },
        "0.01": {
          "tp": 403,
          "fn": 397,
          "accuracy": 0.50375
        }
      },
      "auroc": 0.9700549479166667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9709291666666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9785625000000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.9747458333333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.937871875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9770458333333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        }
      },
      "auroc": 0.9574588541666668
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        }
      },
      "auroc": 0.9544005208333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9778041666666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 714,
          "fn": 86,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 389,
          "fn": 411,
          "accuracy": 0.48625
        }
      },
      "auroc": 0.9661023437499999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9902124999999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9935364583333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        }
      },
      "auroc": 0.9918744791666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.911446875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9769604166666668
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        }
      },
      "auroc": 0.9442036458333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.9508296875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9852484375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 721,
          "fn": 79,
          "accuracy": 0.90125
        },
        "0.01": {
          "tp": 505,
          "fn": 295,
          "accuracy": 0.63125
        }
      },
      "auroc": 0.9680390624999999
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9864208333333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9583354166666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        }
      },
      "auroc": 0.9723781249999999
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.8398885416666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.963096875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        },
        "0.01": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        }
      },
      "auroc": 0.9014927083333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9131546875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        }
      },
      "auroc": 0.9607161458333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 653,
          "fn": 147,
          "accuracy": 0.81625
        },
        "0.01": {
          "tp": 374,
          "fn": 426,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.9369354166666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.989465625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9734770833333335
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9814713541666668
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.9718875
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.9448333333333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        }
      },
      "auroc": 0.9583604166666668
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9806765625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9591552083333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 725,
          "fn": 75,
          "accuracy": 0.90625
        },
        "0.01": {
          "tp": 312,
          "fn": 488,
          "accuracy": 0.39
        }
      },
      "auroc": 0.9699158854166667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9585479166666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9585479166666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9451583333333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9451583333333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9518531250000002
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9518531250000002
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.8284989583333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.8284989583333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.7172364583333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.7172364583333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        }
      },
      "auroc": 0.7728677083333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        }
      },
      "auroc": 0.7728677083333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.9620416666666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.9620416666666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.914415625
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.914415625
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 74,
          "fn": 326,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9382286458333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 74,
          "fn": 326,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9382286458333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.9126614583333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.9126614583333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6425197916666666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6425197916666666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        },
        "0.01": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        }
      },
      "auroc": 0.777590625
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        },
        "0.01": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        }
      },
      "auroc": 0.777590625
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9390677083333332
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9390677083333332
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.8751229166666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.8751229166666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        }
      },
      "auroc": 0.9070953125
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        }
      },
      "auroc": 0.9070953125
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1876,
          "fn": 324,
          "accuracy": 0.8527272727272728
        },
        "0.01": {
          "tp": 1017,
          "fn": 1183,
          "accuracy": 0.4622727272727273
        }
      },
      "auroc": 0.9543910037878788
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1085,
          "fn": 115,
          "accuracy": 0.9041666666666667
        },
        "0.01": {
          "tp": 563,
          "fn": 637,
          "accuracy": 0.4691666666666667
        }
      },
      "auroc": 0.9696208333333334
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2961,
          "fn": 439,
          "accuracy": 0.8708823529411764
        },
        "0.01": {
          "tp": 1580,
          "fn": 1820,
          "accuracy": 0.4647058823529412
        }
      },
      "auroc": 0.9597662377450981
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1391,
          "fn": 809,
          "accuracy": 0.6322727272727273
        },
        "0.01": {
          "tp": 407,
          "fn": 1793,
          "accuracy": 0.185
        }
      },
      "auroc": 0.877273106060606
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1016,
          "fn": 184,
          "accuracy": 0.8466666666666667
        },
        "0.01": {
          "tp": 521,
          "fn": 679,
          "accuracy": 0.43416666666666665
        }
      },
      "auroc": 0.9546095486111112
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2407,
          "fn": 993,
          "accuracy": 0.7079411764705882
        },
        "0.01": {
          "tp": 928,
          "fn": 2472,
          "accuracy": 0.27294117647058824
        }
      },
      "auroc": 0.9045683210784313
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3267,
          "fn": 1133,
          "accuracy": 0.7425
        },
        "0.01": {
          "tp": 1424,
          "fn": 2976,
          "accuracy": 0.3236363636363636
        }
      },
      "auroc": 0.9158320549242426
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2101,
          "fn": 299,
          "accuracy": 0.8754166666666666
        },
        "0.01": {
          "tp": 1084,
          "fn": 1316,
          "accuracy": 0.45166666666666666
        }
      },
      "auroc": 0.9621151909722222
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5368,
          "fn": 1432,
          "accuracy": 0.7894117647058824
        },
        "0.01": {
          "tp": 2508,
          "fn": 4292,
          "accuracy": 0.3688235294117647
        }
      },
      "auroc": 0.9321672794117648
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        }
      },
      "auroc": 0.9821520833333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9564958333333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        }
      },
      "auroc": 0.9693239583333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.97861875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9094927083333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 118,
          "fn": 282,
          "accuracy": 0.295
        }
      },
      "auroc": 0.9440557291666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9803854166666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": {
          "tp": 74,
          "fn": 326,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9329942708333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 689,
          "fn": 111,
          "accuracy": 0.86125
        },
        "0.01": {
          "tp": 284,
          "fn": 516,
          "accuracy": 0.355
        }
      },
      "auroc": 0.95668984375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9940614583333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9711239583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9825927083333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.965759375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.9752572916666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.9705083333333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        }
      },
      "auroc": 0.9799104166666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        }
      },
      "auroc": 0.973190625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 756,
          "fn": 44,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 441,
          "fn": 359,
          "accuracy": 0.55125
        }
      },
      "auroc": 0.9765505208333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.9850854166666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.978346875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.9817161458333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.967090625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.9768749999999999
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        }
      },
      "auroc": 0.9719828125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.9760880208333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        }
      },
      "auroc": 0.9776109375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 751,
          "fn": 49,
          "accuracy": 0.93875
        },
        "0.01": {
          "tp": 476,
          "fn": 324,
          "accuracy": 0.595
        }
      },
      "auroc": 0.9768494791666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9902770833333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9934010416666668
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        }
      },
      "auroc": 0.9918390625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9569708333333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.976921875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        }
      },
      "auroc": 0.9669463541666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9736239583333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9851614583333335
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 753,
          "fn": 47,
          "accuracy": 0.94125
        },
        "0.01": {
          "tp": 534,
          "fn": 266,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.9793927083333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9872052083333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        }
      },
      "auroc": 0.9645447916666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        }
      },
      "auroc": 0.975875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9031385416666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.962925
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9330317708333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.9451718750000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.9637348958333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 691,
          "fn": 109,
          "accuracy": 0.86375
        },
        "0.01": {
          "tp": 405,
          "fn": 395,
          "accuracy": 0.50625
        }
      },
      "auroc": 0.9544533854166667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9942916666666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9774614583333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        }
      },
      "auroc": 0.9858765625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.9844645833333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.9471427083333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        }
      },
      "auroc": 0.9658036458333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        }
      },
      "auroc": 0.989378125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9623020833333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 745,
          "fn": 55,
          "accuracy": 0.93125
        },
        "0.01": {
          "tp": 430,
          "fn": 370,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.9758401041666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.9672583333333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.9672583333333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9547770833333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9547770833333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.9610177083333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.9610177083333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.8877302083333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.8877302083333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.7808427083333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.7808427083333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 57,
          "fn": 343,
          "accuracy": 0.1425
        }
      },
      "auroc": 0.8342864583333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 57,
          "fn": 343,
          "accuracy": 0.1425
        }
      },
      "auroc": 0.8342864583333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.9805916666666668
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.9805916666666668
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.9430947916666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.9430947916666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        }
      },
      "auroc": 0.9618432291666668
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        }
      },
      "auroc": 0.9618432291666668
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9756989583333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9756989583333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7576302083333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7576302083333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.8666645833333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.8666645833333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9644979166666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9644979166666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.9123989583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.9123989583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9384484375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9384484375
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2025,
          "fn": 175,
          "accuracy": 0.9204545454545454
        },
        "0.01": {
          "tp": 1335,
          "fn": 865,
          "accuracy": 0.6068181818181818
        }
      },
      "auroc": 0.9735318181818182
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1108,
          "fn": 92,
          "accuracy": 0.9233333333333333
        },
        "0.01": {
          "tp": 611,
          "fn": 589,
          "accuracy": 0.5091666666666667
        }
      },
      "auroc": 0.9735623263888888
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3133,
          "fn": 267,
          "accuracy": 0.9214705882352942
        },
        "0.01": {
          "tp": 1946,
          "fn": 1454,
          "accuracy": 0.5723529411764706
        }
      },
      "auroc": 0.9735425857843137
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1626,
          "fn": 574,
          "accuracy": 0.7390909090909091
        },
        "0.01": {
          "tp": 704,
          "fn": 1496,
          "accuracy": 0.32
        }
      },
      "auroc": 0.9186169507575758
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1034,
          "fn": 166,
          "accuracy": 0.8616666666666667
        },
        "0.01": {
          "tp": 538,
          "fn": 662,
          "accuracy": 0.4483333333333333
        }
      },
      "auroc": 0.9581024305555556
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2660,
          "fn": 740,
          "accuracy": 0.7823529411764706
        },
        "0.01": {
          "tp": 1242,
          "fn": 2158,
          "accuracy": 0.3652941176470588
        }
      },
      "auroc": 0.9325530024509804
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3651,
          "fn": 749,
          "accuracy": 0.8297727272727272
        },
        "0.01": {
          "tp": 2039,
          "fn": 2361,
          "accuracy": 0.4634090909090909
        }
      },
      "auroc": 0.946074384469697
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2142,
          "fn": 258,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 1149,
          "fn": 1251,
          "accuracy": 0.47875
        }
      },
      "auroc": 0.9658323784722223
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5793,
          "fn": 1007,
          "accuracy": 0.8519117647058824
        },
        "0.01": {
          "tp": 3188,
          "fn": 3612,
          "accuracy": 0.4688235294117647
        }
      },
      "auroc": 0.9530477941176471
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.880653125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8824208333333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8815369791666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.883009375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8823833333333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 362,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8826963541666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 364,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.88183125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8824020833333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 730,
          "accuracy": 0.0875
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8821166666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.929840625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9235895833333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9267151041666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8880322916666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.91153125
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8997817708333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9089364583333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9175604166666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 471,
          "accuracy": 0.41125
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9132484375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8654302083333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8930802083333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 356,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8792552083333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8670979166666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9000333333333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 340,
          "accuracy": 0.15
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.883565625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 383,
          "accuracy": 0.0425
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8662640625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 313,
          "accuracy": 0.2175
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8965567708333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 696,
          "accuracy": 0.13
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8814104166666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.938784375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9128343750000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.925809375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9005072916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9046281249999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 312,
          "accuracy": 0.22
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9025677083333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9196458333333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.90873125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 500,
          "accuracy": 0.375
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9141885416666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9257333333333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8794104166666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 256,
          "accuracy": 0.36
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9025718750000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8888718750000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9094479166666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8991598958333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9073026041666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 329,
          "accuracy": 0.1775
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8944291666666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 222,
          "fn": 578,
          "accuracy": 0.2775
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9008658854166668
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8644822916666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8631885416666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8638354166666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8652604166666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8763
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 369,
          "accuracy": 0.0775
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8707802083333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8648713541666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 369,
          "accuracy": 0.0775
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8697442708333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 747,
          "accuracy": 0.06625
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8673078125
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.902365625
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.902365625
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8983427083333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8983427083333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 262,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9003541666666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 262,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9003541666666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8706218750000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8706218750000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8641385416666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8641385416666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8673802083333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8673802083333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8630427083333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8630427083333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8591208333333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8591208333333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8610817708333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8610817708333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8852
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8852
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8693916666666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8693916666666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8772958333333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8772958333333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8724708333333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8724708333333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8753593749999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8753593749999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 347,
          "accuracy": 0.1325
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8739151041666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 347,
          "accuracy": 0.1325
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8739151041666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 568,
          "fn": 1632,
          "accuracy": 0.2581818181818182
        },
        "0.01": {
          "tp": 0,
          "fn": 2200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8907840909090908
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 956,
          "accuracy": 0.20333333333333334
        },
        "0.01": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8924206597222223
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 812,
          "fn": 2588,
          "accuracy": 0.2388235294117647
        },
        "0.01": {
          "tp": 0,
          "fn": 3400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8913617034313726
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 1952,
          "accuracy": 0.11272727272727273
        },
        "0.01": {
          "tp": 0,
          "fn": 2200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8781029356060607
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 938,
          "accuracy": 0.21833333333333332
        },
        "0.01": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8973873263888889
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 510,
          "fn": 2890,
          "accuracy": 0.15
        },
        "0.01": {
          "tp": 0,
          "fn": 3400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8849091911764706
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 816,
          "fn": 3584,
          "accuracy": 0.18545454545454546
        },
        "0.01": {
          "tp": 0,
          "fn": 4400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8844435132575758
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 506,
          "fn": 1894,
          "accuracy": 0.21083333333333334
        },
        "0.01": {
          "tp": 0,
          "fn": 2400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8949039930555556
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1322,
          "fn": 5478,
          "accuracy": 0.19441176470588234
        },
        "0.01": {
          "tp": 0,
          "fn": 6800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8881354473039216
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1974,
          "fn": 426,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 1042,
          "fn": 1358,
          "accuracy": 0.43416666666666665
        }
      },
      "auroc": 0.9574809895833334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1671,
          "fn": 729,
          "accuracy": 0.69625
        },
        "0.01": {
          "tp": 416,
          "fn": 1984,
          "accuracy": 0.17333333333333334
        }
      },
      "auroc": 0.9206368055555556
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3645,
          "fn": 1155,
          "accuracy": 0.759375
        },
        "0.01": {
          "tp": 1458,
          "fn": 3342,
          "accuracy": 0.30375
        }
      },
      "auroc": 0.9390588975694445
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1947,
          "fn": 453,
          "accuracy": 0.81125
        },
        "0.01": {
          "tp": 776,
          "fn": 1624,
          "accuracy": 0.3233333333333333
        }
      },
      "auroc": 0.9540494791666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1315,
          "fn": 1085,
          "accuracy": 0.5479166666666667
        },
        "0.01": {
          "tp": 260,
          "fn": 2140,
          "accuracy": 0.10833333333333334
        }
      },
      "auroc": 0.8777123263888889
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3262,
          "fn": 1538,
          "accuracy": 0.6795833333333333
        },
        "0.01": {
          "tp": 1036,
          "fn": 3764,
          "accuracy": 0.21583333333333332
        }
      },
      "auroc": 0.9158809027777777
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3921,
          "fn": 879,
          "accuracy": 0.816875
        },
        "0.01": {
          "tp": 1818,
          "fn": 2982,
          "accuracy": 0.37875
        }
      },
      "auroc": 0.955765234375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2986,
          "fn": 1814,
          "accuracy": 0.6220833333333333
        },
        "0.01": {
          "tp": 676,
          "fn": 4124,
          "accuracy": 0.14083333333333334
        }
      },
      "auroc": 0.8991745659722222
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6907,
          "fn": 2693,
          "accuracy": 0.7194791666666667
        },
        "0.01": {
          "tp": 2494,
          "fn": 7106,
          "accuracy": 0.25979166666666664
        }
      },
      "auroc": 0.9274699001736111
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2311,
          "fn": 89,
          "accuracy": 0.9629166666666666
        },
        "0.01": {
          "tp": 1852,
          "fn": 548,
          "accuracy": 0.7716666666666666
        }
      },
      "auroc": 0.9866440104166666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2081,
          "fn": 319,
          "accuracy": 0.8670833333333333
        },
        "0.01": {
          "tp": 1010,
          "fn": 1390,
          "accuracy": 0.42083333333333334
        }
      },
      "auroc": 0.9628331597222222
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4392,
          "fn": 408,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 2862,
          "fn": 1938,
          "accuracy": 0.59625
        }
      },
      "auroc": 0.9747385850694443
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1825,
          "fn": 575,
          "accuracy": 0.7604166666666666
        },
        "0.01": {
          "tp": 722,
          "fn": 1678,
          "accuracy": 0.30083333333333334
        }
      },
      "auroc": 0.9359579861111111
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2052,
          "fn": 348,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 1119,
          "fn": 1281,
          "accuracy": 0.46625
        }
      },
      "auroc": 0.9573434895833334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3877,
          "fn": 923,
          "accuracy": 0.8077083333333334
        },
        "0.01": {
          "tp": 1841,
          "fn": 2959,
          "accuracy": 0.38354166666666667
        }
      },
      "auroc": 0.9466507378472222
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4136,
          "fn": 664,
          "accuracy": 0.8616666666666667
        },
        "0.01": {
          "tp": 2574,
          "fn": 2226,
          "accuracy": 0.53625
        }
      },
      "auroc": 0.9613009982638888
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4133,
          "fn": 667,
          "accuracy": 0.8610416666666667
        },
        "0.01": {
          "tp": 2129,
          "fn": 2671,
          "accuracy": 0.44354166666666667
        }
      },
      "auroc": 0.9600883246527777
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8269,
          "fn": 1331,
          "accuracy": 0.8613541666666666
        },
        "0.01": {
          "tp": 4703,
          "fn": 4897,
          "accuracy": 0.4898958333333333
        }
      },
      "auroc": 0.9606946614583334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1985,
          "fn": 415,
          "accuracy": 0.8270833333333333
        },
        "0.01": {
          "tp": 1121,
          "fn": 1279,
          "accuracy": 0.46708333333333335
        }
      },
      "auroc": 0.959756857638889
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1926,
          "fn": 474,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 1314,
          "fn": 1086,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.94879921875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3911,
          "fn": 889,
          "accuracy": 0.8147916666666667
        },
        "0.01": {
          "tp": 2435,
          "fn": 2365,
          "accuracy": 0.5072916666666667
        }
      },
      "auroc": 0.9542780381944445
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1770,
          "fn": 630,
          "accuracy": 0.7375
        },
        "0.01": {
          "tp": 663,
          "fn": 1737,
          "accuracy": 0.27625
        }
      },
      "auroc": 0.9365598958333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1942,
          "fn": 458,
          "accuracy": 0.8091666666666667
        },
        "0.01": {
          "tp": 1301,
          "fn": 1099,
          "accuracy": 0.5420833333333334
        }
      },
      "auroc": 0.9471433159722222
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3712,
          "fn": 1088,
          "accuracy": 0.7733333333333333
        },
        "0.01": {
          "tp": 1964,
          "fn": 2836,
          "accuracy": 0.4091666666666667
        }
      },
      "auroc": 0.9418516059027777
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3755,
          "fn": 1045,
          "accuracy": 0.7822916666666667
        },
        "0.01": {
          "tp": 1784,
          "fn": 3016,
          "accuracy": 0.37166666666666665
        }
      },
      "auroc": 0.948158376736111
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3868,
          "fn": 932,
          "accuracy": 0.8058333333333333
        },
        "0.01": {
          "tp": 2615,
          "fn": 2185,
          "accuracy": 0.5447916666666667
        }
      },
      "auroc": 0.9479712673611111
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7623,
          "fn": 1977,
          "accuracy": 0.7940625
        },
        "0.01": {
          "tp": 4399,
          "fn": 5201,
          "accuracy": 0.4582291666666667
        }
      },
      "auroc": 0.9480648220486111
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2264,
          "fn": 136,
          "accuracy": 0.9433333333333334
        },
        "0.01": {
          "tp": 1721,
          "fn": 679,
          "accuracy": 0.7170833333333333
        }
      },
      "auroc": 0.9821760416666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2184,
          "fn": 216,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 1643,
          "fn": 757,
          "accuracy": 0.6845833333333333
        }
      },
      "auroc": 0.9755825520833333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4448,
          "fn": 352,
          "accuracy": 0.9266666666666666
        },
        "0.01": {
          "tp": 3364,
          "fn": 1436,
          "accuracy": 0.7008333333333333
        }
      },
      "auroc": 0.9788792968750001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1728,
          "fn": 672,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 702,
          "fn": 1698,
          "accuracy": 0.2925
        }
      },
      "auroc": 0.9265448784722223
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1798,
          "fn": 602,
          "accuracy": 0.7491666666666666
        },
        "0.01": {
          "tp": 1158,
          "fn": 1242,
          "accuracy": 0.4825
        }
      },
      "auroc": 0.9132569444444444
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3526,
          "fn": 1274,
          "accuracy": 0.7345833333333334
        },
        "0.01": {
          "tp": 1860,
          "fn": 2940,
          "accuracy": 0.3875
        }
      },
      "auroc": 0.9199009114583335
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3992,
          "fn": 808,
          "accuracy": 0.8316666666666667
        },
        "0.01": {
          "tp": 2423,
          "fn": 2377,
          "accuracy": 0.5047916666666666
        }
      },
      "auroc": 0.9543604600694444
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3982,
          "fn": 818,
          "accuracy": 0.8295833333333333
        },
        "0.01": {
          "tp": 2801,
          "fn": 1999,
          "accuracy": 0.5835416666666666
        }
      },
      "auroc": 0.9444197482638889
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7974,
          "fn": 1626,
          "accuracy": 0.830625
        },
        "0.01": {
          "tp": 5224,
          "fn": 4376,
          "accuracy": 0.5441666666666667
        }
      },
      "auroc": 0.9493901041666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2225,
          "fn": 175,
          "accuracy": 0.9270833333333334
        },
        "0.01": {
          "tp": 1774,
          "fn": 626,
          "accuracy": 0.7391666666666666
        }
      },
      "auroc": 0.9799164930555555
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1789,
          "fn": 611,
          "accuracy": 0.7454166666666666
        },
        "0.01": {
          "tp": 866,
          "fn": 1534,
          "accuracy": 0.36083333333333334
        }
      },
      "auroc": 0.9320097222222222
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4014,
          "fn": 786,
          "accuracy": 0.83625
        },
        "0.01": {
          "tp": 2640,
          "fn": 2160,
          "accuracy": 0.55
        }
      },
      "auroc": 0.9559631076388889
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1420,
          "fn": 980,
          "accuracy": 0.5916666666666667
        },
        "0.01": {
          "tp": 418,
          "fn": 1982,
          "accuracy": 0.17416666666666666
        }
      },
      "auroc": 0.8774657986111112
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1826,
          "fn": 574,
          "accuracy": 0.7608333333333334
        },
        "0.01": {
          "tp": 982,
          "fn": 1418,
          "accuracy": 0.4091666666666667
        }
      },
      "auroc": 0.9282255208333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3246,
          "fn": 1554,
          "accuracy": 0.67625
        },
        "0.01": {
          "tp": 1400,
          "fn": 3400,
          "accuracy": 0.2916666666666667
        }
      },
      "auroc": 0.9028456597222223
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3645,
          "fn": 1155,
          "accuracy": 0.759375
        },
        "0.01": {
          "tp": 2192,
          "fn": 2608,
          "accuracy": 0.45666666666666667
        }
      },
      "auroc": 0.9286911458333332
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3615,
          "fn": 1185,
          "accuracy": 0.753125
        },
        "0.01": {
          "tp": 1848,
          "fn": 2952,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9301176215277778
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7260,
          "fn": 2340,
          "accuracy": 0.75625
        },
        "0.01": {
          "tp": 4040,
          "fn": 5560,
          "accuracy": 0.42083333333333334
        }
      },
      "auroc": 0.9294043836805556
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2099,
          "fn": 301,
          "accuracy": 0.8745833333333334
        },
        "0.01": {
          "tp": 1557,
          "fn": 843,
          "accuracy": 0.64875
        }
      },
      "auroc": 0.9727196180555555
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1887,
          "fn": 513,
          "accuracy": 0.78625
        },
        "0.01": {
          "tp": 889,
          "fn": 1511,
          "accuracy": 0.37041666666666667
        }
      },
      "auroc": 0.9405119791666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3986,
          "fn": 814,
          "accuracy": 0.8304166666666667
        },
        "0.01": {
          "tp": 2446,
          "fn": 2354,
          "accuracy": 0.5095833333333334
        }
      },
      "auroc": 0.9566157986111111
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1974,
          "fn": 426,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 1034,
          "fn": 1366,
          "accuracy": 0.43083333333333335
        }
      },
      "auroc": 0.9553566840277778
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1602,
          "fn": 798,
          "accuracy": 0.6675
        },
        "0.01": {
          "tp": 408,
          "fn": 1992,
          "accuracy": 0.17
        }
      },
      "auroc": 0.9100182291666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3576,
          "fn": 1224,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 1442,
          "fn": 3358,
          "accuracy": 0.30041666666666667
        }
      },
      "auroc": 0.9326874565972222
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4073,
          "fn": 727,
          "accuracy": 0.8485416666666666
        },
        "0.01": {
          "tp": 2591,
          "fn": 2209,
          "accuracy": 0.5397916666666667
        }
      },
      "auroc": 0.9640381510416667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3489,
          "fn": 1311,
          "accuracy": 0.726875
        },
        "0.01": {
          "tp": 1297,
          "fn": 3503,
          "accuracy": 0.27020833333333333
        }
      },
      "auroc": 0.9252651041666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7562,
          "fn": 2038,
          "accuracy": 0.7877083333333333
        },
        "0.01": {
          "tp": 3888,
          "fn": 5712,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9446516276041667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1926,
          "fn": 474,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 1019,
          "fn": 1381,
          "accuracy": 0.4245833333333333
        }
      },
      "auroc": 0.9524653645833333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1926,
          "fn": 474,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 1019,
          "fn": 1381,
          "accuracy": 0.4245833333333333
        }
      },
      "auroc": 0.9524653645833333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1811,
          "fn": 589,
          "accuracy": 0.7545833333333334
        },
        "0.01": {
          "tp": 705,
          "fn": 1695,
          "accuracy": 0.29375
        }
      },
      "auroc": 0.9371686631944445
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1811,
          "fn": 589,
          "accuracy": 0.7545833333333334
        },
        "0.01": {
          "tp": 705,
          "fn": 1695,
          "accuracy": 0.29375
        }
      },
      "auroc": 0.9371686631944445
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3737,
          "fn": 1063,
          "accuracy": 0.7785416666666667
        },
        "0.01": {
          "tp": 1724,
          "fn": 3076,
          "accuracy": 0.3591666666666667
        }
      },
      "auroc": 0.944817013888889
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3737,
          "fn": 1063,
          "accuracy": 0.7785416666666667
        },
        "0.01": {
          "tp": 1724,
          "fn": 3076,
          "accuracy": 0.3591666666666667
        }
      },
      "auroc": 0.944817013888889
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1086,
          "fn": 1314,
          "accuracy": 0.4525
        },
        "0.01": {
          "tp": 275,
          "fn": 2125,
          "accuracy": 0.11458333333333333
        }
      },
      "auroc": 0.8634730902777777
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1086,
          "fn": 1314,
          "accuracy": 0.4525
        },
        "0.01": {
          "tp": 275,
          "fn": 2125,
          "accuracy": 0.11458333333333333
        }
      },
      "auroc": 0.8634730902777777
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 712,
          "fn": 1688,
          "accuracy": 0.2966666666666667
        },
        "0.01": {
          "tp": 222,
          "fn": 2178,
          "accuracy": 0.0925
        }
      },
      "auroc": 0.7715954861111112
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 712,
          "fn": 1688,
          "accuracy": 0.2966666666666667
        },
        "0.01": {
          "tp": 222,
          "fn": 2178,
          "accuracy": 0.0925
        }
      },
      "auroc": 0.7715954861111112
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1798,
          "fn": 3002,
          "accuracy": 0.3745833333333333
        },
        "0.01": {
          "tp": 497,
          "fn": 4303,
          "accuracy": 0.10354166666666667
        }
      },
      "auroc": 0.8175342881944444
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1798,
          "fn": 3002,
          "accuracy": 0.3745833333333333
        },
        "0.01": {
          "tp": 497,
          "fn": 4303,
          "accuracy": 0.10354166666666667
        }
      },
      "auroc": 0.8175342881944444
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1928,
          "fn": 472,
          "accuracy": 0.8033333333333333
        },
        "0.01": {
          "tp": 1037,
          "fn": 1363,
          "accuracy": 0.4320833333333333
        }
      },
      "auroc": 0.9493079861111111
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1928,
          "fn": 472,
          "accuracy": 0.8033333333333333
        },
        "0.01": {
          "tp": 1037,
          "fn": 1363,
          "accuracy": 0.4320833333333333
        }
      },
      "auroc": 0.9493079861111111
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 804,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 372,
          "fn": 2028,
          "accuracy": 0.155
        }
      },
      "auroc": 0.909213454861111
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 804,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 372,
          "fn": 2028,
          "accuracy": 0.155
        }
      },
      "auroc": 0.909213454861111
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3524,
          "fn": 1276,
          "accuracy": 0.7341666666666666
        },
        "0.01": {
          "tp": 1409,
          "fn": 3391,
          "accuracy": 0.29354166666666665
        }
      },
      "auroc": 0.9292607204861112
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3524,
          "fn": 1276,
          "accuracy": 0.7341666666666666
        },
        "0.01": {
          "tp": 1409,
          "fn": 3391,
          "accuracy": 0.29354166666666665
        }
      },
      "auroc": 0.9292607204861112
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1839,
          "fn": 561,
          "accuracy": 0.76625
        },
        "0.01": {
          "tp": 490,
          "fn": 1910,
          "accuracy": 0.20416666666666666
        }
      },
      "auroc": 0.9480183159722222
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1839,
          "fn": 561,
          "accuracy": 0.76625
        },
        "0.01": {
          "tp": 490,
          "fn": 1910,
          "accuracy": 0.20416666666666666
        }
      },
      "auroc": 0.9480183159722222
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 2143,
          "accuracy": 0.10708333333333334
        },
        "0.01": {
          "tp": 2,
          "fn": 2398,
          "accuracy": 0.0008333333333333334
        }
      },
      "auroc": 0.7526308159722223
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 2143,
          "accuracy": 0.10708333333333334
        },
        "0.01": {
          "tp": 2,
          "fn": 2398,
          "accuracy": 0.0008333333333333334
        }
      },
      "auroc": 0.7526308159722223
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2096,
          "fn": 2704,
          "accuracy": 0.43666666666666665
        },
        "0.01": {
          "tp": 492,
          "fn": 4308,
          "accuracy": 0.1025
        }
      },
      "auroc": 0.8503245659722223
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2096,
          "fn": 2704,
          "accuracy": 0.43666666666666665
        },
        "0.01": {
          "tp": 492,
          "fn": 4308,
          "accuracy": 0.1025
        }
      },
      "auroc": 0.8503245659722223
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1842,
          "fn": 558,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 924,
          "fn": 1476,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9370049479166667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1842,
          "fn": 558,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 924,
          "fn": 1476,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9370049479166667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1436,
          "fn": 964,
          "accuracy": 0.5983333333333334
        },
        "0.01": {
          "tp": 623,
          "fn": 1777,
          "accuracy": 0.25958333333333333
        }
      },
      "auroc": 0.8894752604166666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1436,
          "fn": 964,
          "accuracy": 0.5983333333333334
        },
        "0.01": {
          "tp": 623,
          "fn": 1777,
          "accuracy": 0.25958333333333333
        }
      },
      "auroc": 0.8894752604166666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3278,
          "fn": 1522,
          "accuracy": 0.6829166666666666
        },
        "0.01": {
          "tp": 1547,
          "fn": 3253,
          "accuracy": 0.32229166666666664
        }
      },
      "auroc": 0.9132401041666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3278,
          "fn": 1522,
          "accuracy": 0.6829166666666666
        },
        "0.01": {
          "tp": 1547,
          "fn": 3253,
          "accuracy": 0.32229166666666664
        }
      },
      "auroc": 0.9132401041666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 21479,
          "fn": 4921,
          "accuracy": 0.8135984848484848
        },
        "0.01": {
          "tp": 12812,
          "fn": 13588,
          "accuracy": 0.4853030303030303
        }
      },
      "auroc": 0.9535421559343434
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 11538,
          "fn": 2862,
          "accuracy": 0.80125
        },
        "0.01": {
          "tp": 6138,
          "fn": 8262,
          "accuracy": 0.42625
        }
      },
      "auroc": 0.9467289062500001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 33017,
          "fn": 7783,
          "accuracy": 0.8092401960784313
        },
        "0.01": {
          "tp": 18950,
          "fn": 21850,
          "accuracy": 0.4644607843137255
        }
      },
      "auroc": 0.9511374795751634
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 16476,
          "fn": 9924,
          "accuracy": 0.6240909090909091
        },
        "0.01": {
          "tp": 6239,
          "fn": 20161,
          "accuracy": 0.23632575757575758
        }
      },
      "auroc": 0.895092582070707
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 10535,
          "fn": 3865,
          "accuracy": 0.7315972222222222
        },
        "0.01": {
          "tp": 5228,
          "fn": 9172,
          "accuracy": 0.3630555555555556
        }
      },
      "auroc": 0.922283304398148
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 27011,
          "fn": 13789,
          "accuracy": 0.6620343137254902
        },
        "0.01": {
          "tp": 11467,
          "fn": 29333,
          "accuracy": 0.28105392156862746
        }
      },
      "auroc": 0.9046893075980391
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37955,
          "fn": 14845,
          "accuracy": 0.718844696969697
        },
        "0.01": {
          "tp": 19051,
          "fn": 33749,
          "accuracy": 0.36081439393939396
        }
      },
      "auroc": 0.9243173690025253
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 22073,
          "fn": 6727,
          "accuracy": 0.7664236111111111
        },
        "0.01": {
          "tp": 11366,
          "fn": 17434,
          "accuracy": 0.3946527777777778
        }
      },
      "auroc": 0.934506105324074
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 60028,
          "fn": 21572,
          "accuracy": 0.7356372549019607
        },
        "0.01": {
          "tp": 30417,
          "fn": 51183,
          "accuracy": 0.37275735294117646
        }
      },
      "auroc": 0.9279133935866013
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8371510416666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8059385416666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8215447916666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8337218750000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.80110625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8174140624999999
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8354364583333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8035223958333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8194794270833333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6533166666666665
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6530687500000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6531927083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.756540625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6475083333333332
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7020244791666665
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7049286458333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6502885416666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.67760859375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.865153125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6770864583333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7711197916666668
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8603833333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6706833333333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7655333333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8627682291666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6738848958333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7683265625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6529031249999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.81619375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7345484375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.768296875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6870666666666665
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7276817708333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7105999999999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7516302083333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7311151041666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6331385416666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8046322916666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7188854166666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6875364583333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7023802083333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6949583333333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6603375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.75350625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7069218749999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8437864583333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8303999999999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8370932291666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8529458333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.782659375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8178026041666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8483661458333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8065296875000001
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8274479166666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7797135416666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7797135416666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7583656249999999
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7583656249999999
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7690395833333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7690395833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7864208333333335
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7864208333333335
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7964
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7964
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7914104166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7914104166666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.842715625
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.842715625
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8363125
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8363125
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8395140624999999
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8395140624999999
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8381
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8381
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8586510416666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8586510416666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8483755208333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8483755208333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.839228125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.839228125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8219125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8219125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8305703125000001
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8305703125000001
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7792388257575757
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7645532986111111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7740556985294118
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8028242424242424
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7152340277777778
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7719100490196078
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7910315340909091
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7398936631944445
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7729828737745097
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8371510416666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8059385416666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8215447916666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8337218750000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.80110625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8174140624999999
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8354364583333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8035223958333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8194794270833333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6533166666666665
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6530687500000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6531927083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.756540625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6475083333333332
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7020244791666665
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7049286458333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6502885416666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.67760859375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.865153125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6770864583333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7711197916666668
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8603833333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6706833333333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7655333333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8627682291666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6738848958333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7683265625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6529031249999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.81619375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7345484375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.768296875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6870666666666665
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7276817708333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7105999999999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7516302083333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7311151041666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6331385416666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8046322916666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7188854166666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6875364583333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7023802083333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6949583333333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6603375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.75350625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7069218749999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8437864583333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8303999999999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8370932291666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8529458333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.782659375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8178026041666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8483661458333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8065296875000001
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8274479166666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7797135416666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7797135416666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7583656249999999
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7583656249999999
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7690395833333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7690395833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7864208333333335
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7864208333333335
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7964
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7964
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7914104166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7914104166666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.842715625
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.842715625
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8363125
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8363125
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8395140624999999
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8395140624999999
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8381
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8381
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8586510416666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8586510416666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8483755208333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8483755208333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.839228125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.839228125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8219125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8219125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8305703125000001
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8305703125000001
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7792388257575757
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7645532986111111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7740556985294118
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8028242424242424
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7152340277777778
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7719100490196078
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7910315340909091
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7398936631944445
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7729828737745097
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8566083333333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8639927083333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8603005208333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8573666666666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8547197916666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8560432291666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8569875
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8593562499999999
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8581718749999999
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.670425
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6653500000000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6678875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.65159375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6579249999999999
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.654759375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.661009375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6616375000000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6613234375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8033239583333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6968604166666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7500921875
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7976187500000002
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6784552083333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7380369791666668
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8004713541666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6876578125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7440645833333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.61535
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8038895833333335
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7096197916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.64510625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7342791666666668
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6896927083333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.630228125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.769084375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.69965625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6097802083333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.775478125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6926291666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.568753125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.705084375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.63691875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5892666666666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.74028125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6647739583333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.80471875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8394239583333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8220713541666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8137427083333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.796621875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8051822916666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8092307291666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8180229166666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8136268229166665
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.666678125
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.666678125
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.647403125
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.647403125
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.657040625
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.657040625
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.68310625
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.68310625
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6902354166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6902354166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6866708333333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6866708333333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8644604166666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8644604166666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8552968750000001
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8552968750000001
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8598786458333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8598786458333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8587604166666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8587604166666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8124822916666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8124822916666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8356213541666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8356213541666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7594937500000001
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7594937500000001
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7338635416666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7338635416666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7466786458333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7466786458333334
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7447913825757576
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.774165798611111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7551588235294118
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7339511363636364
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7378475694444444
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7353263480392157
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.739371259469697
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7560066840277777
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7452425857843137
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8700114583333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.84215625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8560838541666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8605864583333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8449333333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8527598958333332
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8652989583333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8435447916666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.854421875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.64525625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6670854166666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6561708333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6862739583333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6563895833333332
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6713317708333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6657651041666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6617375000000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6637513020833332
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8510989583333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6891770833333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7701380208333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8356135416666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6756958333333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7556546875000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.84335625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6824364583333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7628963541666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.617709375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8126958333333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7152026041666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6806177083333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7043135416666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.692465625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6491635416666668
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7585046875000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7038341145833333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6168302083333332
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.78351875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7001744791666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6039270833333332
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7007385416666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6523328125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6103786458333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7421286458333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6762536458333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8245052083333335
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8444958333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8345005208333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.839359375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7851020833333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8122307291666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8319322916666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8147989583333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8233656249999999
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7040500000000001
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7040500000000001
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6725729166666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6725729166666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6883114583333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6883114583333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.723584375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.723584375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7421104166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7421104166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7328473958333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7328473958333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.870303125
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.870303125
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8637947916666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8637947916666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8670489583333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8670489583333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8751833333333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8751833333333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8571208333333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8571208333333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8661520833333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8661520833333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8073572916666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8073572916666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7856291666666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7856291666666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7964932291666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7964932291666666
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7641717803030303
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7731881944444445
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7673540441176471
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7661460227272727
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7278621527777778
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.752634068627451
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7651589015151514
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7505251736111112
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7599940563725489
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.85451875
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8594135416666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8569661458333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8519489583333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8401531250000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8460510416666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8532338541666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8497833333333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.85150859375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6662666666666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6549989583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6606328124999999
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.691125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6479302083333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6695276041666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6786958333333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6514645833333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6650802083333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7897239583333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6813781250000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7355510416666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7592375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6745614583333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7168994791666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7744807291666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6779697916666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7262252604166667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.650409375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.735140625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6927749999999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6503520833333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7098333333333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6800927083333335
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6503807291666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7224869791666668
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6864338541666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6431697916666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6774104166666668
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6602901041666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.577653125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6985125000000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6380828125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6104114583333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6879614583333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6491864583333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7818687500000001
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.796628125
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7892484375000002
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7808604166666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.76168125
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7712708333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7813645833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7791546875000002
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7802596354166667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6166281249999999
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6166281249999999
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5988895833333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5988895833333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6077588541666668
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6077588541666668
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6348906249999999
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6348906249999999
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6398187500000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6398187500000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6373546875
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6373546875
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8537520833333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8537520833333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8398510416666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8398510416666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8468015624999999
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8468015624999999
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8412437500000001
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8412437500000001
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8111875000000001
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8111875000000001
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8262156249999999
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8262156249999999
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7053895833333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7053895833333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6936052083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6936052083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6994973958333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6994973958333334
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7307146780303031
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7341616319444444
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7319312499999999
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7176844696969698
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7221119791666666
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7192471200980393
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7241995738636363
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7281368055555556
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7255891850490196
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7977447916666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7972166666666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7974807291666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7841354166666668
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7815479166666668
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7828416666666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7909401041666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7893822916666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7901611979166667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5403427083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.41202916666666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.47618593750000004
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6229927083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.3917260416666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5073593750000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5816677083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.4018776041666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.49177265625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7894229166666668
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5070135416666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6482182291666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7556864583333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.42519583333333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5904411458333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7725546875
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.46610468749999995
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6193296875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.36249791666666664
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.588775
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.47563645833333335
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6471114583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.540253125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5936822916666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5048046875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5645140625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5346593749999999
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5102302083333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5524093750000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5313197916666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5969510416666668
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.44218958333333336
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5195703125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.553590625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.49729947916666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5254450520833334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7439572916666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7752645833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7596109375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7802854166666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7131447916666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7467151041666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7621213541666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7442046875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7531630208333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6655208333333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6655208333333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6630479166666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6630479166666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.664284375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.664284375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6919947916666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6919947916666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.697465625
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.697465625
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6947302083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6947302083333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.806640625
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.806640625
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8140385416666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8140385416666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8103395833333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8103395833333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8041479166666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8041479166666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8147541666666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8147541666666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8094510416666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8094510416666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7428427083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7428427083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7300718749999999
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7300718749999999
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7364572916666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7364572916666666
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6777584280303031
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6054513888888889
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6522382965686275
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7187764204545455
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5490095486111111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6588587009803921
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6982674242424243
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5772304687500001
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6555484987745098
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8628604166666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.840759375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8518098958333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.861909375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8319156249999999
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8469125000000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8623848958333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8363375000000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8493611979166666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6639583333333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6563385416666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6601484375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7496864583333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6466375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6981619791666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7068223958333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6514880208333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6791552083333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8454135416666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6813239583333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.76336875
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8348770833333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6698822916666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7523796875
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8401453125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6756031250000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.75787421875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5722927083333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8053083333333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6888005208333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.74549375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6924145833333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7189541666666668
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6588932291666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7488614583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7038773437499999
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5626770833333332
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7906177083333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6766473958333332
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6627333333333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7027791666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.68275625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6127052083333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7466984375000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6797018229166667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8369177083333335
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8545291666666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8457234375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8542427083333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.789725
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8219838541666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8455802083333332
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8221270833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8338536458333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6709979166666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6709979166666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6705229166666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6705229166666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6707604166666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6707604166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7201802083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7201802083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7260645833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7260645833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7231223958333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7231223958333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8718687500000001
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8718687500000001
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8595958333333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8595958333333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8657322916666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8657322916666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.86985
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.86985
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8642031250000001
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8642031250000001
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8670265625
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8670265625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7871864583333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7871864583333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7661625000000001
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7661625000000001
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7766744791666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7766744791666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7512911931818183
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7714795138888888
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7584164828431372
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7814083333333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7222256944444444
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7605203431372549
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7663497632575758
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7468526041666668
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7594684129901961
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8371510416666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8059385416666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8215447916666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8337218750000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.80110625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8174140624999999
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8354364583333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8035223958333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8194794270833333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6531114583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6530687500000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6530901041666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7558333333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6475895833333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7017114583333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7044723958333332
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6503291666666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6774007812499999
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.865153125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6770864583333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7711197916666668
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8603833333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6709291666666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.76565625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8627682291666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6740078125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7683880208333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6529031249999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.81619375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7345484375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.768296875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6878260416666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7280614583333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7105999999999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7520098958333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7313049479166667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6331385416666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8046322916666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7188854166666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6879697916666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7025989583333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.695284375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6605541666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.753615625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7070848958333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8437864583333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8303999999999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8370932291666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8529458333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.782659375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8178026041666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8483661458333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8065296875000001
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8274479166666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7797135416666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7797135416666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7583656249999999
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7583656249999999
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7690395833333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7690395833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7864208333333335
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7864208333333335
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7964
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7964
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7914104166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7914104166666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.842715625
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.842715625
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8363125
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8363125
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8395140624999999
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8395140624999999
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8381
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8381
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8586510416666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8586510416666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8483755208333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8483755208333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8395291666666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8395291666666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8219125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8219125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8307208333333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8307208333333334
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7792475378787879
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7645532986111111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7740613357843138
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8027993371212121
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7154515625
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7719707107843137
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7910234375
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7400024305555557
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7730160232843137
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.30911666666666665
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2686166666666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.28886666666666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.305540625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.251771875
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.27865625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.30732864583333336
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.26019427083333335
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2837614583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.48840833333333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7260385416666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6072234375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.18532916666666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6914197916666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.43837447916666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.33686874999999994
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7087291666666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5227989583333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22581874999999998
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.519865625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.3728421875
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21463229166666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.4792833333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.3469578125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22022552083333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.49957447916666664
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.3599
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.37896979166666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.20065729166666668
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2898135416666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.14506041666666664
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.09634895833333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.1207046875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2620151041666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.148503125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.20525911458333335
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.38406875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.14018541666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.26212708333333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.14970624999999999
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.295553125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2226296875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.26688750000000006
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21786927083333335
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.24237838541666668
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.23043541666666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.18147083333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.205953125
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22568437499999996
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.13811979166666669
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.18190208333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22805989583333336
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.15979531249999998
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.19392760416666668
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21921875000000005
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21921875000000005
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21158020833333335
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21158020833333335
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21539947916666669
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21539947916666669
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.19043958333333336
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.19043958333333336
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.19645
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.19645
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.19344479166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.19344479166666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22625416666666665
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22625416666666665
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21401250000000002
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21401250000000002
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22013333333333332
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22013333333333332
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.23329375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.23329375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.20389479166666669
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.20389479166666669
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21859427083333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21859427083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21671875000000002
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21671875000000002
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22184374999999998
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22184374999999998
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21928124999999998
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21928124999999998
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.28206751893939397
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.3394723958333334
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.30232806372549026
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.20670312500000002
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.32541614583333334
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.24860183823529414
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.24438532196969698
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.3324442708333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.27546495098039214
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8607520833333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8337125
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8472322916666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8592406250000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8152270833333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8372338541666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8599963541666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8244697916666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8422330729166666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6608833333333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6530687500000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6569760416666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6912302083333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6475083333333332
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6693692708333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6760567708333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6502885416666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.66317265625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8534239583333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6775958333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7655098958333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8451958333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6722447916666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7587203125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8493098958333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6749203125000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7621151041666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6531447916666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8155229166666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7343338541666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7048593750000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.680515625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6926875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6790020833333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7480192708333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7135106770833333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.64119375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8017218750000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7214578125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6260145833333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7025885416666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6643015625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6336041666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7521552083333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6928796875000001
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.843440625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8380875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8407640625000001
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8457197916666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.774925
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8103223958333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8445802083333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.80650625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8255432291666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.75173125
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.75173125
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7267281250000001
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7267281250000001
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7392296875000001
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7392296875000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7498510416666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7498510416666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.75150625
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.75150625
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7506786458333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7506786458333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8625697916666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8625697916666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8513427083333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8513427083333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.85695625
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.85695625
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8649770833333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8649770833333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.848628125
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.848628125
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8568026041666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8568026041666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8057562500000001
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8057562500000001
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7925927083333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7925927083333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7991744791666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7991744791666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7770658143939395
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7699515625
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7745549019607842
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7766416666666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7155015625000001
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.755062806372549
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.776853740530303
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7427265624999999
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7648088541666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8433291666666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.815403125
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8293661458333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8405135416666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8102031249999999
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8253583333333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8419213541666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.812803125
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8273622395833333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.654690625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6550312500000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6548609374999999
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7491666666666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6486489583333332
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6989078124999999
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7019286458333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6518401041666668
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.676884375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8659125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6772958333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7716041666666668
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8592052083333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6696333333333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7644192708333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8625588541666668
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6734645833333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.76801171875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6529791666666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8151187500000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7340489583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7601145833333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6897333333333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7249239583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.706546875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7524260416666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7294864583333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.63688125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8006479166666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7187645833333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6786760416666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7022270833333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6904515624999998
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6577786458333335
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7514375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7046080729166666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8439968750000001
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.835503125
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.83975
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8541208333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7832843749999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8187026041666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8490588541666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.80939375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8292263020833334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7754041666666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7754041666666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.756040625
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.756040625
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7657223958333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7657223958333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7804000000000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7804000000000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7890083333333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7890083333333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7847041666666668
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7847041666666668
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.850625
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.850625
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8419010416666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8419010416666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8462630208333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8462630208333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8429718749999999
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8429718749999999
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8596427083333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8596427083333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8513072916666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8513072916666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.836109375
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.836109375
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.820740625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.820740625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8284250000000001
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8284250000000001
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7803
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7665000000000001
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7754294117647059
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.800830018939394
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7172883680555555
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7713447303921569
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.790565009469697
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7418941840277777
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7733870710784314
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.23472812499999998
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2288479166666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.23178802083333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22958541666666665
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22936041666666665
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22947291666666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.23215677083333336
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22910416666666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.23063046875000004
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.266909375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.25476354166666665
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.26083645833333335
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21093749999999997
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.25034895833333337
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2306432291666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2389234375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.25255625000000004
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.24573984375000002
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2231614583333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22056041666666668
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22186093750000002
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21911666666666665
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.23269062499999998
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22590364583333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2211390625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22662552083333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22388229166666668
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.28031458333333337
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2037979166666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.24205625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21760000000000002
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21574375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21667187499999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.24895729166666664
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2097708333333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2293640625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.26235520833333337
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.18961979166666665
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22598749999999998
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21309375000000003
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22178854166666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21744114583333335
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.23772447916666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.20570416666666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22171432291666665
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22669375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2131708333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2199322916666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22316979166666664
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21374791666666665
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21845885416666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22493177083333332
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.213459375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2191955729166667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.19203125000000001
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.19203125000000001
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.19039270833333335
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.19039270833333335
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.19121197916666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.19121197916666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.20087291666666668
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.20087291666666668
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.20278854166666668
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.20278854166666668
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.20183072916666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.20183072916666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22793125
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22793125
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22170729166666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22170729166666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22481927083333336
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22481927083333336
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.23926458333333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.23926458333333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2292479166666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2292479166666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.23425625
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.23425625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22183124999999998
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22183124999999998
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21940625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21940625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22061875000000003
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22061875000000003
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.23419034090909094
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.2184600694444445
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22863848039215687
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.21609507575757575
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22728003472222222
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22004270833333334
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22514270833333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22287005208333335
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.22434059436274506
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7500935763888888
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7306611979166667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7403773871527777
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.745999392361111
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7219292534722221
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7339643229166666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.748046484375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7262952256944445
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7371708550347222
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6014071180555556
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.608659201388889
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6050331597222222
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6256041666666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5984283854166665
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6120162760416666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.613505642361111
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6035437934027778
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6085247178819445
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7368966145833333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6151941840277778
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6760453993055555
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7251944444444444
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5991615451388889
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6621779947916667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7310455295138889
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6071778645833333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6691116970486111
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5618647569444444
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.685790625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6238276909722222
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6251005208333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5937828993055556
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6094417100694445
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.593482638888889
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6397867621527777
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6166347005208332
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5638835069444444
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6604588541666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6121711805555555
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5617125868055555
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6065684027777778
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5841404947916666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.562798046875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6335136284722223
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5981558376736111
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7223244791666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7224811631944444
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7224028211805555
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7313352430555555
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6753608506944444
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.703348046875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7268298611111111
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6989210069444445
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7128754340277779
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6334500868055555
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6334500868055555
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6176895833333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6176895833333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6255698350694445
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6255698350694445
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6445485243055554
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6445485243055554
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6520539930555556
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6520539930555556
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6483012586805555
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6483012586805555
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7468793402777776
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7468793402777776
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7392065104166666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7392065104166666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7430429253472222
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7430429253472222
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7453327256944444
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7453327256944444
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7397595486111112
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7397595486111112
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7425461371527778
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7425461371527778
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7000559027777777
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7000559027777777
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6858044270833333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6858044270833333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6929301649305556
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6929301649305556
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6733396938131312
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6705408709490741
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6723518739787582
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6772236742424241
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6325385561342592
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6614524560866013
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6752816840277778
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6515397135416666
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6669021650326797
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.9868895833333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.983928125
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9854088541666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.983659375
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9667583333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9752088541666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        }
      },
      "auroc": 0.9852744791666668
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        }
      },
      "auroc": 0.9753432291666668
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 769,
          "fn": 31,
          "accuracy": 0.96125
        },
        "0.01": {
          "tp": 314,
          "fn": 486,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.9803088541666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9860041666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.9846156250000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9853098958333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.9289510416666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9734375000000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.9511942708333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.9574776041666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9790265625000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 728,
          "fn": 72,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 145,
          "fn": 655,
          "accuracy": 0.18125
        }
      },
      "auroc": 0.9682520833333332
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9650239583333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9739427083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9694833333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9639520833333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.96245625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9632041666666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9644880208333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9681994791666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 711,
          "fn": 89,
          "accuracy": 0.88875
        },
        "0.01": {
          "tp": 306,
          "fn": 494,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.9663437500000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9865270833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.986565625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        }
      },
      "auroc": 0.9865463541666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.9367656249999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9792010416666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        }
      },
      "auroc": 0.9579833333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 93,
          "fn": 307,
          "accuracy": 0.2325
        }
      },
      "auroc": 0.9616463541666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        }
      },
      "auroc": 0.9828833333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 734,
          "fn": 66,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 198,
          "fn": 602,
          "accuracy": 0.2475
        }
      },
      "auroc": 0.9722648437500001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9856739583333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9786250000000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 75,
          "fn": 325,
          "accuracy": 0.1875
        }
      },
      "auroc": 0.9821494791666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.8702333333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.984984375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.9276088541666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 76,
          "fn": 324,
          "accuracy": 0.19
        }
      },
      "auroc": 0.9279536458333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9818046874999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 693,
          "fn": 107,
          "accuracy": 0.86625
        },
        "0.01": {
          "tp": 138,
          "fn": 662,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.9548791666666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.9761583333333335
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9806791666666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9784187500000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9763020833333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.9803947916666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        }
      },
      "auroc": 0.9783484375000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9762302083333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        }
      },
      "auroc": 0.9805369791666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 769,
          "fn": 31,
          "accuracy": 0.96125
        },
        "0.01": {
          "tp": 221,
          "fn": 579,
          "accuracy": 0.27625
        }
      },
      "auroc": 0.9783835937500001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.950521875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.950521875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9541510416666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9541510416666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.9523364583333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.9523364583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.8100052083333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.8100052083333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.74301875
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.74301875
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.7765119791666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.7765119791666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9817781250000001
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9817781250000001
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.9735072916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.9735072916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        }
      },
      "auroc": 0.9776427083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        }
      },
      "auroc": 0.9776427083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9815052083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9815052083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.8859125
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.8859125
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        }
      },
      "auroc": 0.9337088541666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        }
      },
      "auroc": 0.9337088541666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8786197916666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8786197916666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.8135979166666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.8135979166666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        }
      },
      "auroc": 0.8461088541666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        }
      },
      "auroc": 0.8461088541666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1920,
          "fn": 280,
          "accuracy": 0.8727272727272727
        },
        "0.01": {
          "tp": 623,
          "fn": 1577,
          "accuracy": 0.2831818181818182
        }
      },
      "auroc": 0.9535188446969697
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1167,
          "fn": 33,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 317,
          "fn": 883,
          "accuracy": 0.26416666666666666
        }
      },
      "auroc": 0.9813927083333334
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3087,
          "fn": 313,
          "accuracy": 0.9079411764705883
        },
        "0.01": {
          "tp": 940,
          "fn": 2460,
          "accuracy": 0.27647058823529413
        }
      },
      "auroc": 0.9633566789215686
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1575,
          "fn": 625,
          "accuracy": 0.7159090909090909
        },
        "0.01": {
          "tp": 590,
          "fn": 1610,
          "accuracy": 0.2681818181818182
        }
      },
      "auroc": 0.9118228219696969
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1131,
          "fn": 69,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 341,
          "fn": 859,
          "accuracy": 0.2841666666666667
        }
      },
      "auroc": 0.9745387152777778
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2706,
          "fn": 694,
          "accuracy": 0.7958823529411765
        },
        "0.01": {
          "tp": 931,
          "fn": 2469,
          "accuracy": 0.2738235294117647
        }
      },
      "auroc": 0.9339578431372548
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3495,
          "fn": 905,
          "accuracy": 0.7943181818181818
        },
        "0.01": {
          "tp": 1213,
          "fn": 3187,
          "accuracy": 0.2756818181818182
        }
      },
      "auroc": 0.9326708333333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2298,
          "fn": 102,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 658,
          "fn": 1742,
          "accuracy": 0.27416666666666667
        }
      },
      "auroc": 0.9779657118055556
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5793,
          "fn": 1007,
          "accuracy": 0.8519117647058824
        },
        "0.01": {
          "tp": 1871,
          "fn": 4929,
          "accuracy": 0.2751470588235294
        }
      },
      "auroc": 0.9486572610294118
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.9868895833333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.983928125
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9854088541666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.983659375
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9667583333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9752088541666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        }
      },
      "auroc": 0.9852744791666668
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        }
      },
      "auroc": 0.9753432291666668
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 769,
          "fn": 31,
          "accuracy": 0.96125
        },
        "0.01": {
          "tp": 314,
          "fn": 486,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.9803088541666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9860041666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.9846156250000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9853098958333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.9289510416666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9734375000000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.9511942708333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.9574776041666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9790265625000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 728,
          "fn": 72,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 145,
          "fn": 655,
          "accuracy": 0.18125
        }
      },
      "auroc": 0.9682520833333332
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9650239583333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9739427083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9694833333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9639520833333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.96245625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9632041666666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9644880208333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9681994791666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 711,
          "fn": 89,
          "accuracy": 0.88875
        },
        "0.01": {
          "tp": 306,
          "fn": 494,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.9663437500000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9865270833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.986565625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        }
      },
      "auroc": 0.9865463541666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.9367656249999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9792010416666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        }
      },
      "auroc": 0.9579833333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 93,
          "fn": 307,
          "accuracy": 0.2325
        }
      },
      "auroc": 0.9616463541666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        }
      },
      "auroc": 0.9828833333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 734,
          "fn": 66,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 198,
          "fn": 602,
          "accuracy": 0.2475
        }
      },
      "auroc": 0.9722648437500001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9856739583333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9786250000000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 75,
          "fn": 325,
          "accuracy": 0.1875
        }
      },
      "auroc": 0.9821494791666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.8702333333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.984984375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.9276088541666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 76,
          "fn": 324,
          "accuracy": 0.19
        }
      },
      "auroc": 0.9279536458333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9818046874999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 693,
          "fn": 107,
          "accuracy": 0.86625
        },
        "0.01": {
          "tp": 138,
          "fn": 662,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.9548791666666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.9761583333333335
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9806791666666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9784187500000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9763020833333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.9803947916666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        }
      },
      "auroc": 0.9783484375000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9762302083333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        }
      },
      "auroc": 0.9805369791666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 769,
          "fn": 31,
          "accuracy": 0.96125
        },
        "0.01": {
          "tp": 221,
          "fn": 579,
          "accuracy": 0.27625
        }
      },
      "auroc": 0.9783835937500001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.950521875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.950521875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9541510416666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9541510416666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.9523364583333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.9523364583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.8100052083333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.8100052083333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.74301875
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.74301875
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.7765119791666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.7765119791666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9817781250000001
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9817781250000001
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.9735072916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.9735072916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        }
      },
      "auroc": 0.9776427083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        }
      },
      "auroc": 0.9776427083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9815052083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9815052083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.8859125
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.8859125
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        }
      },
      "auroc": 0.9337088541666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        }
      },
      "auroc": 0.9337088541666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8786197916666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8786197916666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.8135979166666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.8135979166666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        }
      },
      "auroc": 0.8461088541666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        }
      },
      "auroc": 0.8461088541666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1920,
          "fn": 280,
          "accuracy": 0.8727272727272727
        },
        "0.01": {
          "tp": 623,
          "fn": 1577,
          "accuracy": 0.2831818181818182
        }
      },
      "auroc": 0.9535188446969697
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1167,
          "fn": 33,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 317,
          "fn": 883,
          "accuracy": 0.26416666666666666
        }
      },
      "auroc": 0.9813927083333334
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3087,
          "fn": 313,
          "accuracy": 0.9079411764705883
        },
        "0.01": {
          "tp": 940,
          "fn": 2460,
          "accuracy": 0.27647058823529413
        }
      },
      "auroc": 0.9633566789215686
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1575,
          "fn": 625,
          "accuracy": 0.7159090909090909
        },
        "0.01": {
          "tp": 590,
          "fn": 1610,
          "accuracy": 0.2681818181818182
        }
      },
      "auroc": 0.9118228219696969
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1131,
          "fn": 69,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 341,
          "fn": 859,
          "accuracy": 0.2841666666666667
        }
      },
      "auroc": 0.9745387152777778
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2706,
          "fn": 694,
          "accuracy": 0.7958823529411765
        },
        "0.01": {
          "tp": 931,
          "fn": 2469,
          "accuracy": 0.2738235294117647
        }
      },
      "auroc": 0.9339578431372548
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3495,
          "fn": 905,
          "accuracy": 0.7943181818181818
        },
        "0.01": {
          "tp": 1213,
          "fn": 3187,
          "accuracy": 0.2756818181818182
        }
      },
      "auroc": 0.9326708333333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2298,
          "fn": 102,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 658,
          "fn": 1742,
          "accuracy": 0.27416666666666667
        }
      },
      "auroc": 0.9779657118055556
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 5793,
          "fn": 1007,
          "accuracy": 0.8519117647058824
        },
        "0.01": {
          "tp": 1871,
          "fn": 4929,
          "accuracy": 0.2751470588235294
        }
      },
      "auroc": 0.9486572610294118
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9808760416666668
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        }
      },
      "auroc": 0.9565395833333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.9687078125
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.9651468750000001
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9279479166666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9465473958333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        }
      },
      "auroc": 0.9730114583333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        }
      },
      "auroc": 0.94224375
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 671,
          "fn": 129,
          "accuracy": 0.83875
        },
        "0.01": {
          "tp": 401,
          "fn": 399,
          "accuracy": 0.50125
        }
      },
      "auroc": 0.9576276041666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9884083333333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.9847302083333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9865692708333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.8402229166666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.974021875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9071223958333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        }
      },
      "auroc": 0.914315625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 74,
          "fn": 326,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9793760416666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 643,
          "fn": 157,
          "accuracy": 0.80375
        },
        "0.01": {
          "tp": 166,
          "fn": 634,
          "accuracy": 0.2075
        }
      },
      "auroc": 0.9468458333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.9573302083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9657083333333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        }
      },
      "auroc": 0.9615192708333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9509166666666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9523427083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.9516296875
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.9541234375000001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9590255208333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 655,
          "fn": 145,
          "accuracy": 0.81875
        },
        "0.01": {
          "tp": 381,
          "fn": 419,
          "accuracy": 0.47625
        }
      },
      "auroc": 0.9565744791666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.9871375
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9900489583333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        }
      },
      "auroc": 0.9885932291666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.833365625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.9817583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9075619791666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 70,
          "fn": 330,
          "accuracy": 0.175
        }
      },
      "auroc": 0.9102515625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        }
      },
      "auroc": 0.9859036458333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 631,
          "fn": 169,
          "accuracy": 0.78875
        },
        "0.01": {
          "tp": 351,
          "fn": 449,
          "accuracy": 0.43875
        }
      },
      "auroc": 0.9480776041666668
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.987903125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.9822645833333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 147,
          "fn": 253,
          "accuracy": 0.3675
        }
      },
      "auroc": 0.9850838541666668
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.7597479166666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.9839635416666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 73,
          "fn": 327,
          "accuracy": 0.1825
        }
      },
      "auroc": 0.8718557291666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        }
      },
      "auroc": 0.8738255208333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 119,
          "fn": 281,
          "accuracy": 0.2975
        }
      },
      "auroc": 0.9831140625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 626,
          "fn": 174,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 220,
          "fn": 580,
          "accuracy": 0.275
        }
      },
      "auroc": 0.9284697916666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9762572916666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.9822666666666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9792619791666668
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.972409375
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.973228125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        }
      },
      "auroc": 0.9728187500000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.9743333333333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        }
      },
      "auroc": 0.9777473958333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 732,
          "fn": 68,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 461,
          "fn": 339,
          "accuracy": 0.57625
        }
      },
      "auroc": 0.9760403645833334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.9423656249999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.9423656249999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.9390166666666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.9390166666666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        },
        "0.01": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        }
      },
      "auroc": 0.9406911458333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        },
        "0.01": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        }
      },
      "auroc": 0.9406911458333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.6878614583333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.6878614583333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.6469802083333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.6469802083333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        },
        "0.01": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.6674208333333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        },
        "0.01": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.6674208333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.9780416666666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.9780416666666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.9565114583333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.9565114583333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9672765625
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9672765625
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9516687500000001
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9516687500000001
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.7173854166666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.7173854166666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        },
        "0.01": {
          "tp": 89,
          "fn": 311,
          "accuracy": 0.2225
        }
      },
      "auroc": 0.8345270833333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        },
        "0.01": {
          "tp": 89,
          "fn": 311,
          "accuracy": 0.2225
        }
      },
      "auroc": 0.8345270833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.8144083333333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.8144083333333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.7572020833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.7572020833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 80,
          "fn": 320,
          "accuracy": 0.2
        }
      },
      "auroc": 0.7858052083333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 80,
          "fn": 320,
          "accuracy": 0.2
        }
      },
      "auroc": 0.7858052083333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1749,
          "fn": 451,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 889,
          "fn": 1311,
          "accuracy": 0.4040909090909091
        }
      },
      "auroc": 0.9320234848484849
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1114,
          "fn": 86,
          "accuracy": 0.9283333333333333
        },
        "0.01": {
          "tp": 573,
          "fn": 627,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.9769263888888888
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2863,
          "fn": 537,
          "accuracy": 0.8420588235294117
        },
        "0.01": {
          "tp": 1462,
          "fn": 1938,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9478715686274509
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1091,
          "fn": 1109,
          "accuracy": 0.4959090909090909
        },
        "0.01": {
          "tp": 577,
          "fn": 1623,
          "accuracy": 0.26227272727272727
        }
      },
      "auroc": 0.8489913825757576
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1064,
          "fn": 136,
          "accuracy": 0.8866666666666667
        },
        "0.01": {
          "tp": 510,
          "fn": 690,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9655437499999999
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2155,
          "fn": 1245,
          "accuracy": 0.6338235294117647
        },
        "0.01": {
          "tp": 1087,
          "fn": 2313,
          "accuracy": 0.3197058823529412
        }
      },
      "auroc": 0.890127512254902
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2840,
          "fn": 1560,
          "accuracy": 0.6454545454545455
        },
        "0.01": {
          "tp": 1466,
          "fn": 2934,
          "accuracy": 0.3331818181818182
        }
      },
      "auroc": 0.8905074337121213
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2178,
          "fn": 222,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 1083,
          "fn": 1317,
          "accuracy": 0.45125
        }
      },
      "auroc": 0.9712350694444445
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5018,
          "fn": 1782,
          "accuracy": 0.7379411764705882
        },
        "0.01": {
          "tp": 2549,
          "fn": 4251,
          "accuracy": 0.3748529411764706
        }
      },
      "auroc": 0.9189995404411764
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9811010416666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.96413125
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        }
      },
      "auroc": 0.9726161458333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.972240625
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9301375000000001
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.9511890625000001
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        }
      },
      "auroc": 0.9766708333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        }
      },
      "auroc": 0.947134375
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 703,
          "fn": 97,
          "accuracy": 0.87875
        },
        "0.01": {
          "tp": 394,
          "fn": 406,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.9619026041666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.9859281249999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.9847302083333335
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        }
      },
      "auroc": 0.9853291666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.8657072916666668
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9741177083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 60,
          "fn": 340,
          "accuracy": 0.15
        }
      },
      "auroc": 0.9199124999999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9258177083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        }
      },
      "auroc": 0.9794239583333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 671,
          "fn": 129,
          "accuracy": 0.83875
        },
        "0.01": {
          "tp": 124,
          "fn": 676,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9526208333333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.947478125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9561895833333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.9518338541666668
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.9457718749999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.94609375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        }
      },
      "auroc": 0.9459328124999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 144,
          "fn": 256,
          "accuracy": 0.36
        }
      },
      "auroc": 0.946625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9511416666666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 639,
          "fn": 161,
          "accuracy": 0.79875
        },
        "0.01": {
          "tp": 290,
          "fn": 510,
          "accuracy": 0.3625
        }
      },
      "auroc": 0.9488833333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9867166666666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9889364583333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        }
      },
      "auroc": 0.9878265625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.8489249999999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9724354166666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 111,
          "fn": 289,
          "accuracy": 0.2775
        }
      },
      "auroc": 0.9106802083333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        }
      },
      "auroc": 0.9178208333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.9806859375
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 644,
          "fn": 156,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 292,
          "fn": 508,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9492533854166667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.9860625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.9777645833333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        }
      },
      "auroc": 0.9819135416666668
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.7801218749999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.979078125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        },
        "0.01": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        }
      },
      "auroc": 0.8795999999999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        }
      },
      "auroc": 0.8830921875
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 96,
          "fn": 304,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9784213541666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 630,
          "fn": 170,
          "accuracy": 0.7875
        },
        "0.01": {
          "tp": 173,
          "fn": 627,
          "accuracy": 0.21625
        }
      },
      "auroc": 0.9307567708333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.9746166666666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.978609375
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        }
      },
      "auroc": 0.9766130208333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.966734375
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9745041666666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.9706192708333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        }
      },
      "auroc": 0.9706755208333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.9765567708333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 737,
          "fn": 63,
          "accuracy": 0.92125
        },
        "0.01": {
          "tp": 357,
          "fn": 443,
          "accuracy": 0.44625
        }
      },
      "auroc": 0.9736161458333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9351583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9351583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.934365625
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.934365625
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9347619791666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9347619791666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.7189541666666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.7189541666666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.6468624999999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.6468624999999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        }
      },
      "auroc": 0.6829083333333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        }
      },
      "auroc": 0.6829083333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.970709375
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.970709375
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9569041666666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9569041666666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.9638067708333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.9638067708333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.96385
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.96385
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.7581614583333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.7581614583333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        },
        "0.01": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        }
      },
      "auroc": 0.8610057291666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        },
        "0.01": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        }
      },
      "auroc": 0.8610057291666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.8072677083333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.8072677083333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.7549208333333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.7549208333333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.7810942708333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.7810942708333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1761,
          "fn": 439,
          "accuracy": 0.8004545454545454
        },
        "0.01": {
          "tp": 718,
          "fn": 1482,
          "accuracy": 0.32636363636363636
        }
      },
      "auroc": 0.9325311553030303
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1119,
          "fn": 81,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 471,
          "fn": 729,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.9750602430555555
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2880,
          "fn": 520,
          "accuracy": 0.8470588235294118
        },
        "0.01": {
          "tp": 1189,
          "fn": 2211,
          "accuracy": 0.3497058823529412
        }
      },
      "auroc": 0.9475414215686275
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1189,
          "fn": 1011,
          "accuracy": 0.5404545454545454
        },
        "0.01": {
          "tp": 534,
          "fn": 1666,
          "accuracy": 0.24272727272727274
        }
      },
      "auroc": 0.8573377840909091
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1067,
          "fn": 133,
          "accuracy": 0.8891666666666667
        },
        "0.01": {
          "tp": 439,
          "fn": 761,
          "accuracy": 0.36583333333333334
        }
      },
      "auroc": 0.9627277777777777
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2256,
          "fn": 1144,
          "accuracy": 0.6635294117647059
        },
        "0.01": {
          "tp": 973,
          "fn": 2427,
          "accuracy": 0.2861764705882353
        }
      },
      "auroc": 0.8945342524509805
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2950,
          "fn": 1450,
          "accuracy": 0.6704545454545454
        },
        "0.01": {
          "tp": 1252,
          "fn": 3148,
          "accuracy": 0.28454545454545455
        }
      },
      "auroc": 0.8949344696969697
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2186,
          "fn": 214,
          "accuracy": 0.9108333333333334
        },
        "0.01": {
          "tp": 910,
          "fn": 1490,
          "accuracy": 0.37916666666666665
        }
      },
      "auroc": 0.9688940104166667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 5136,
          "fn": 1664,
          "accuracy": 0.7552941176470588
        },
        "0.01": {
          "tp": 2162,
          "fn": 4638,
          "accuracy": 0.3179411764705882
        }
      },
      "auroc": 0.9210378370098039
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.9855302083333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9746208333333335
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9800755208333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9785874999999999
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.9418677083333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        }
      },
      "auroc": 0.9602276041666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.9820588541666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.9582442708333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 733,
          "fn": 67,
          "accuracy": 0.91625
        },
        "0.01": {
          "tp": 375,
          "fn": 425,
          "accuracy": 0.46875
        }
      },
      "auroc": 0.9701515625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.9864218749999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9846604166666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9855411458333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.8881635416666668
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.9735822916666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 69,
          "fn": 331,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.9308729166666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 73,
          "fn": 327,
          "accuracy": 0.1825
        }
      },
      "auroc": 0.9372927083333336
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 58,
          "fn": 342,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9791213541666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 684,
          "fn": 116,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 131,
          "fn": 669,
          "accuracy": 0.16375
        }
      },
      "auroc": 0.95820703125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.955190625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9593958333333332
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        }
      },
      "auroc": 0.9572932291666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.9490125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.9502177083333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        }
      },
      "auroc": 0.9496151041666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9521015625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        }
      },
      "auroc": 0.9548067708333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 660,
          "fn": 140,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 297,
          "fn": 503,
          "accuracy": 0.37125
        }
      },
      "auroc": 0.9534541666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.986725
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.989165625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        }
      },
      "auroc": 0.9879453125000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.8891187500000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9782854166666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 98,
          "fn": 302,
          "accuracy": 0.245
        }
      },
      "auroc": 0.9337020833333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 73,
          "fn": 327,
          "accuracy": 0.1825
        }
      },
      "auroc": 0.937921875
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        }
      },
      "auroc": 0.9837255208333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 691,
          "fn": 109,
          "accuracy": 0.86375
        },
        "0.01": {
          "tp": 254,
          "fn": 546,
          "accuracy": 0.3175
        }
      },
      "auroc": 0.9608236979166667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9861020833333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.9782510416666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9821765625000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.8063135416666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.9845822916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        }
      },
      "auroc": 0.8954479166666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        }
      },
      "auroc": 0.8962078125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 79,
          "fn": 321,
          "accuracy": 0.1975
        }
      },
      "auroc": 0.9814166666666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 651,
          "fn": 149,
          "accuracy": 0.81375
        },
        "0.01": {
          "tp": 156,
          "fn": 644,
          "accuracy": 0.195
        }
      },
      "auroc": 0.9388122395833334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9746645833333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.9799927083333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        }
      },
      "auroc": 0.9773286458333332
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9718427083333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9790562500000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.9754494791666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        }
      },
      "auroc": 0.9732536458333335
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9795244791666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 751,
          "fn": 49,
          "accuracy": 0.93875
        },
        "0.01": {
          "tp": 304,
          "fn": 496,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9763890625
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.9380656249999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.9380656249999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9401864583333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9401864583333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9391260416666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9391260416666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.7424895833333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.7424895833333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.672446875
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.672446875
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 310,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        }
      },
      "auroc": 0.7074682291666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 310,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        }
      },
      "auroc": 0.7074682291666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9742552083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9742552083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9653447916666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9653447916666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9698
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9698
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.9735802083333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.9735802083333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.7916447916666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.7916447916666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        },
        "0.01": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8826125
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        },
        "0.01": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8826125
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.8335260416666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.8335260416666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.7704291666666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.7704291666666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": {
          "tp": 69,
          "fn": 331,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.8019776041666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": {
          "tp": 69,
          "fn": 331,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.8019776041666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1822,
          "fn": 378,
          "accuracy": 0.8281818181818181
        },
        "0.01": {
          "tp": 702,
          "fn": 1498,
          "accuracy": 0.3190909090909091
        }
      },
      "auroc": 0.9396864583333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1132,
          "fn": 68,
          "accuracy": 0.9433333333333334
        },
        "0.01": {
          "tp": 421,
          "fn": 779,
          "accuracy": 0.35083333333333333
        }
      },
      "auroc": 0.9776810763888889
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2954,
          "fn": 446,
          "accuracy": 0.8688235294117647
        },
        "0.01": {
          "tp": 1123,
          "fn": 2277,
          "accuracy": 0.33029411764705885
        }
      },
      "auroc": 0.9530963235294119
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1306,
          "fn": 894,
          "accuracy": 0.5936363636363636
        },
        "0.01": {
          "tp": 545,
          "fn": 1655,
          "accuracy": 0.24772727272727274
        }
      },
      "auroc": 0.8748264204545455
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1098,
          "fn": 102,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 396,
          "fn": 804,
          "accuracy": 0.33
        }
      },
      "auroc": 0.9679319444444443
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2404,
          "fn": 996,
          "accuracy": 0.7070588235294117
        },
        "0.01": {
          "tp": 941,
          "fn": 2459,
          "accuracy": 0.27676470588235297
        }
      },
      "auroc": 0.907687193627451
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3128,
          "fn": 1272,
          "accuracy": 0.7109090909090909
        },
        "0.01": {
          "tp": 1247,
          "fn": 3153,
          "accuracy": 0.2834090909090909
        }
      },
      "auroc": 0.9072564393939394
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2230,
          "fn": 170,
          "accuracy": 0.9291666666666667
        },
        "0.01": {
          "tp": 817,
          "fn": 1583,
          "accuracy": 0.34041666666666665
        }
      },
      "auroc": 0.9728065104166668
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5358,
          "fn": 1442,
          "accuracy": 0.7879411764705883
        },
        "0.01": {
          "tp": 2064,
          "fn": 4736,
          "accuracy": 0.3035294117647059
        }
      },
      "auroc": 0.9303917585784314
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.98681875
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.9761031250000001
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9814609375
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.982509375
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9522697916666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9673895833333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9846640624999999
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9641864583333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 740,
          "fn": 60,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 430,
          "fn": 370,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.9744252604166668
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.9827520833333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9405302083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        }
      },
      "auroc": 0.9616411458333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9338031250000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.8326802083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 76,
          "fn": 324,
          "accuracy": 0.19
        }
      },
      "auroc": 0.8832416666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 94,
          "fn": 306,
          "accuracy": 0.235
        }
      },
      "auroc": 0.9582776041666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 98,
          "fn": 302,
          "accuracy": 0.245
        }
      },
      "auroc": 0.8866052083333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 562,
          "fn": 238,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 192,
          "fn": 608,
          "accuracy": 0.24
        }
      },
      "auroc": 0.92244140625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.96243125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9598416666666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.9611364583333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.955015625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.936515625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        },
        "0.01": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        }
      },
      "auroc": 0.945765625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        }
      },
      "auroc": 0.9587234375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.9481786458333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 625,
          "fn": 175,
          "accuracy": 0.78125
        },
        "0.01": {
          "tp": 327,
          "fn": 473,
          "accuracy": 0.40875
        }
      },
      "auroc": 0.9534510416666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9844802083333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.98566875
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        }
      },
      "auroc": 0.9850744791666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9434666666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.8655406250000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        },
        "0.01": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        }
      },
      "auroc": 0.9045036458333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 90,
          "fn": 310,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9639734375
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9256046875
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 619,
          "fn": 181,
          "accuracy": 0.77375
        },
        "0.01": {
          "tp": 242,
          "fn": 558,
          "accuracy": 0.3025
        }
      },
      "auroc": 0.9447890625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.9852166666666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9621343750000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        }
      },
      "auroc": 0.9736755208333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9035666666666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.8107354166666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        }
      },
      "auroc": 0.8571510416666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9443916666666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        },
        "0.01": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        }
      },
      "auroc": 0.8864348958333335
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 565,
          "fn": 235,
          "accuracy": 0.70625
        },
        "0.01": {
          "tp": 227,
          "fn": 573,
          "accuracy": 0.28375
        }
      },
      "auroc": 0.9154132812500001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.97430625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        }
      },
      "auroc": 0.9817729166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9780395833333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.9769875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.970575
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9737812499999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.975646875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        }
      },
      "auroc": 0.9761739583333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 730,
          "fn": 70,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 365,
          "fn": 435,
          "accuracy": 0.45625
        }
      },
      "auroc": 0.9759104166666668
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.939271875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.939271875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9467156250000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9467156250000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        }
      },
      "auroc": 0.94299375
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        }
      },
      "auroc": 0.94299375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.8807489583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.8807489583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.8637739583333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.8637739583333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        }
      },
      "auroc": 0.8722614583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        }
      },
      "auroc": 0.8722614583333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.9840177083333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.9840177083333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9794572916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9794572916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.9817375
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.9817375
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9847645833333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9847645833333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.9458166666666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.9458166666666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.965290625
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.965290625
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.9042760416666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.9042760416666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.89533125
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.89533125
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.8998036458333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.8998036458333334
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1884,
          "fn": 316,
          "accuracy": 0.8563636363636363
        },
        "0.01": {
          "tp": 785,
          "fn": 1415,
          "accuracy": 0.3568181818181818
        }
      },
      "auroc": 0.9608258522727272
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1049,
          "fn": 151,
          "accuracy": 0.8741666666666666
        },
        "0.01": {
          "tp": 594,
          "fn": 606,
          "accuracy": 0.495
        }
      },
      "auroc": 0.9676751736111111
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2933,
          "fn": 467,
          "accuracy": 0.8626470588235294
        },
        "0.01": {
          "tp": 1379,
          "fn": 2021,
          "accuracy": 0.40558823529411764
        }
      },
      "auroc": 0.9632432598039216
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1642,
          "fn": 558,
          "accuracy": 0.7463636363636363
        },
        "0.01": {
          "tp": 813,
          "fn": 1387,
          "accuracy": 0.36954545454545457
        }
      },
      "auroc": 0.9387676136363636
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 729,
          "fn": 471,
          "accuracy": 0.6075
        },
        "0.01": {
          "tp": 332,
          "fn": 868,
          "accuracy": 0.27666666666666667
        }
      },
      "auroc": 0.8947194444444444
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2371,
          "fn": 1029,
          "accuracy": 0.6973529411764706
        },
        "0.01": {
          "tp": 1145,
          "fn": 2255,
          "accuracy": 0.33676470588235297
        }
      },
      "auroc": 0.9232212009803922
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3526,
          "fn": 874,
          "accuracy": 0.8013636363636364
        },
        "0.01": {
          "tp": 1598,
          "fn": 2802,
          "accuracy": 0.36318181818181816
        }
      },
      "auroc": 0.9497967329545454
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1778,
          "fn": 622,
          "accuracy": 0.7408333333333333
        },
        "0.01": {
          "tp": 926,
          "fn": 1474,
          "accuracy": 0.3858333333333333
        }
      },
      "auroc": 0.9311973090277778
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 5304,
          "fn": 1496,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 2524,
          "fn": 4276,
          "accuracy": 0.3711764705882353
        }
      },
      "auroc": 0.9432322303921569
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.9869166666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9837822916666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9853494791666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9835333333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9666947916666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        }
      },
      "auroc": 0.9751140625000001
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        }
      },
      "auroc": 0.9852250000000001
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        }
      },
      "auroc": 0.9752385416666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 768,
          "fn": 32,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 315,
          "fn": 485,
          "accuracy": 0.39375
        }
      },
      "auroc": 0.9802317708333335
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9860656250000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.9845812500000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        }
      },
      "auroc": 0.9853234375000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.92636875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9735489583333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9499588541666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9562171875000002
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        }
      },
      "auroc": 0.9790651041666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 724,
          "fn": 76,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 141,
          "fn": 659,
          "accuracy": 0.17625
        }
      },
      "auroc": 0.9676411458333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.9650791666666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9740614583333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9695703125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9635895833333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.9625989583333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9630942708333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        }
      },
      "auroc": 0.964334375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 150,
          "fn": 250,
          "accuracy": 0.375
        }
      },
      "auroc": 0.9683302083333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 711,
          "fn": 89,
          "accuracy": 0.88875
        },
        "0.01": {
          "tp": 310,
          "fn": 490,
          "accuracy": 0.3875
        }
      },
      "auroc": 0.9663322916666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9866312500000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.9864291666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 100,
          "fn": 300,
          "accuracy": 0.25
        }
      },
      "auroc": 0.9865302083333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.9351083333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9791166666666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        }
      },
      "auroc": 0.9571125
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        }
      },
      "auroc": 0.9608697916666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9827729166666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 732,
          "fn": 68,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 201,
          "fn": 599,
          "accuracy": 0.25125
        }
      },
      "auroc": 0.9718213541666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9857781250000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9785760416666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 75,
          "fn": 325,
          "accuracy": 0.1875
        }
      },
      "auroc": 0.9821770833333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.8677979166666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.985075
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        }
      },
      "auroc": 0.9264364583333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 75,
          "fn": 325,
          "accuracy": 0.1875
        }
      },
      "auroc": 0.9267880208333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        }
      },
      "auroc": 0.9818255208333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 689,
          "fn": 111,
          "accuracy": 0.86125
        },
        "0.01": {
          "tp": 136,
          "fn": 664,
          "accuracy": 0.17
        }
      },
      "auroc": 0.9543067708333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9762770833333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.9807989583333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        }
      },
      "auroc": 0.9785380208333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.976321875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.9805239583333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        }
      },
      "auroc": 0.9784229166666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        }
      },
      "auroc": 0.9762994791666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 111,
          "fn": 289,
          "accuracy": 0.2775
        }
      },
      "auroc": 0.9806614583333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 769,
          "fn": 31,
          "accuracy": 0.96125
        },
        "0.01": {
          "tp": 225,
          "fn": 575,
          "accuracy": 0.28125
        }
      },
      "auroc": 0.97848046875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9501677083333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9501677083333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.9537979166666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.9537979166666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9519828125
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9519828125
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.8077875000000001
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.8077875000000001
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.7400322916666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.7400322916666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        },
        "0.01": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        }
      },
      "auroc": 0.7739098958333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        },
        "0.01": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        }
      },
      "auroc": 0.7739098958333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9816833333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9816833333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.9731427083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.9731427083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        }
      },
      "auroc": 0.9774130208333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        }
      },
      "auroc": 0.9774130208333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.981421875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.981421875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.8848010416666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.8848010416666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        }
      },
      "auroc": 0.9331114583333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        }
      },
      "auroc": 0.9331114583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.8765177083333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.8765177083333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.810075
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.810075
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        }
      },
      "auroc": 0.8432963541666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        }
      },
      "auroc": 0.8432963541666667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1917,
          "fn": 283,
          "accuracy": 0.8713636363636363
        },
        "0.01": {
          "tp": 631,
          "fn": 1569,
          "accuracy": 0.2868181818181818
        }
      },
      "auroc": 0.9531205492424242
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1167,
          "fn": 33,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 322,
          "fn": 878,
          "accuracy": 0.2683333333333333
        }
      },
      "auroc": 0.9813715277777777
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3084,
          "fn": 316,
          "accuracy": 0.9070588235294118
        },
        "0.01": {
          "tp": 953,
          "fn": 2447,
          "accuracy": 0.2802941176470588
        }
      },
      "auroc": 0.9630914828431372
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1562,
          "fn": 638,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 587,
          "fn": 1613,
          "accuracy": 0.26681818181818184
        }
      },
      "auroc": 0.9104153409090909
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1131,
          "fn": 69,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 341,
          "fn": 859,
          "accuracy": 0.2841666666666667
        }
      },
      "auroc": 0.9745930555555555
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2693,
          "fn": 707,
          "accuracy": 0.7920588235294118
        },
        "0.01": {
          "tp": 928,
          "fn": 2472,
          "accuracy": 0.27294117647058824
        }
      },
      "auroc": 0.9330662990196078
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3479,
          "fn": 921,
          "accuracy": 0.7906818181818182
        },
        "0.01": {
          "tp": 1218,
          "fn": 3182,
          "accuracy": 0.2768181818181818
        }
      },
      "auroc": 0.9317679450757576
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2298,
          "fn": 102,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 663,
          "fn": 1737,
          "accuracy": 0.27625
        }
      },
      "auroc": 0.9779822916666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5777,
          "fn": 1023,
          "accuracy": 0.8495588235294118
        },
        "0.01": {
          "tp": 1881,
          "fn": 4919,
          "accuracy": 0.2766176470588235
        }
      },
      "auroc": 0.9480788909313724
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.9868895833333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.983928125
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9854088541666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.983659375
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9667583333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9752088541666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        }
      },
      "auroc": 0.9852744791666668
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        }
      },
      "auroc": 0.9753432291666668
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 769,
          "fn": 31,
          "accuracy": 0.96125
        },
        "0.01": {
          "tp": 314,
          "fn": 486,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.9803088541666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9860041666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.9846156250000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9853098958333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.9290510416666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9732760416666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.9511635416666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.9575276041666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9789458333333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 729,
          "fn": 71,
          "accuracy": 0.91125
        },
        "0.01": {
          "tp": 145,
          "fn": 655,
          "accuracy": 0.18125
        }
      },
      "auroc": 0.9682367187500001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9650239583333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9739427083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9694833333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9639520833333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.96245625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9632041666666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9644880208333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9681994791666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 711,
          "fn": 89,
          "accuracy": 0.88875
        },
        "0.01": {
          "tp": 306,
          "fn": 494,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.9663437500000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9865270833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.986565625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        }
      },
      "auroc": 0.9865463541666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.9367656249999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9792072916666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9579864583333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 93,
          "fn": 307,
          "accuracy": 0.2325
        }
      },
      "auroc": 0.9616463541666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9828864583333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 734,
          "fn": 66,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 199,
          "fn": 601,
          "accuracy": 0.24875
        }
      },
      "auroc": 0.9722664062499999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9856739583333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9786250000000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 75,
          "fn": 325,
          "accuracy": 0.1875
        }
      },
      "auroc": 0.9821494791666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.8702333333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.984984375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.9276088541666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 76,
          "fn": 324,
          "accuracy": 0.19
        }
      },
      "auroc": 0.9279536458333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9818046874999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 693,
          "fn": 107,
          "accuracy": 0.86625
        },
        "0.01": {
          "tp": 138,
          "fn": 662,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.9548791666666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.9761739583333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9806791666666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9784265625000002
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9763020833333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.9803947916666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        }
      },
      "auroc": 0.9783484375000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9762380208333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        }
      },
      "auroc": 0.9805369791666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 769,
          "fn": 31,
          "accuracy": 0.96125
        },
        "0.01": {
          "tp": 221,
          "fn": 579,
          "accuracy": 0.27625
        }
      },
      "auroc": 0.9783875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.950521875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.950521875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9541510416666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9541510416666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.9523364583333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.9523364583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.8100052083333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.8100052083333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.7430458333333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.7430458333333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.7765255208333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.7765255208333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9817781250000001
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9817781250000001
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.9735072916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.9735072916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        }
      },
      "auroc": 0.9776427083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        }
      },
      "auroc": 0.9776427083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9815052083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9815052083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.8859125
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.8859125
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        }
      },
      "auroc": 0.9337088541666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        }
      },
      "auroc": 0.9337088541666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8786197916666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8786197916666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.8134666666666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.8134666666666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        }
      },
      "auroc": 0.8460432291666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        }
      },
      "auroc": 0.8460432291666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1920,
          "fn": 280,
          "accuracy": 0.8727272727272727
        },
        "0.01": {
          "tp": 623,
          "fn": 1577,
          "accuracy": 0.2831818181818182
        }
      },
      "auroc": 0.9535202651515151
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1167,
          "fn": 33,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 317,
          "fn": 883,
          "accuracy": 0.26416666666666666
        }
      },
      "auroc": 0.9813927083333334
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3087,
          "fn": 313,
          "accuracy": 0.9079411764705883
        },
        "0.01": {
          "tp": 940,
          "fn": 2460,
          "accuracy": 0.27647058823529413
        }
      },
      "auroc": 0.9633575980392157
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1576,
          "fn": 624,
          "accuracy": 0.7163636363636363
        },
        "0.01": {
          "tp": 590,
          "fn": 1610,
          "accuracy": 0.2681818181818182
        }
      },
      "auroc": 0.9118224431818182
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1131,
          "fn": 69,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 342,
          "fn": 858,
          "accuracy": 0.285
        }
      },
      "auroc": 0.9745128472222223
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2707,
          "fn": 693,
          "accuracy": 0.7961764705882353
        },
        "0.01": {
          "tp": 932,
          "fn": 2468,
          "accuracy": 0.2741176470588235
        }
      },
      "auroc": 0.933948468137255
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3496,
          "fn": 904,
          "accuracy": 0.7945454545454546
        },
        "0.01": {
          "tp": 1213,
          "fn": 3187,
          "accuracy": 0.2756818181818182
        }
      },
      "auroc": 0.9326713541666667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2298,
          "fn": 102,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 659,
          "fn": 1741,
          "accuracy": 0.27458333333333335
        }
      },
      "auroc": 0.9779527777777778
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5794,
          "fn": 1006,
          "accuracy": 0.8520588235294118
        },
        "0.01": {
          "tp": 1872,
          "fn": 4928,
          "accuracy": 0.2752941176470588
        }
      },
      "auroc": 0.9486530330882352
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.728415625
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.53295
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6306828125
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7194031249999999
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.44394479166666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.5816739583333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.723909375
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.4884473958333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 777,
          "accuracy": 0.02875
        },
        "0.01": {
          "tp": 2,
          "fn": 798,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.6061783854166667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9885968749999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        }
      },
      "auroc": 0.9817520833333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9851744791666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6331364583333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9694354166666668
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        }
      },
      "auroc": 0.8012859374999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        }
      },
      "auroc": 0.8108666666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        }
      },
      "auroc": 0.97559375
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 566,
          "fn": 234,
          "accuracy": 0.7075
        },
        "0.01": {
          "tp": 337,
          "fn": 463,
          "accuracy": 0.42125
        }
      },
      "auroc": 0.8932302083333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6821
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.5656458333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 383,
          "accuracy": 0.0425
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.6238729166666668
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6404708333333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.5699072916666668
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.6051890624999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6612854166666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.5677765625000001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 772,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 6,
          "fn": 794,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.6145309895833333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.988284375
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.7517041666666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        }
      },
      "auroc": 0.8699942708333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6210770833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4951229166666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.5581
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        }
      },
      "auroc": 0.8046807291666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.6234135416666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 229,
          "fn": 571,
          "accuracy": 0.28625
        },
        "0.01": {
          "tp": 82,
          "fn": 718,
          "accuracy": 0.1025
        }
      },
      "auroc": 0.7140471354166666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.98645625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.7458520833333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        },
        "0.01": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        }
      },
      "auroc": 0.8661541666666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.59145625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.7383062499999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 333,
          "accuracy": 0.1675
        },
        "0.01": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        }
      },
      "auroc": 0.66488125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        }
      },
      "auroc": 0.7889562499999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        },
        "0.01": {
          "tp": 69,
          "fn": 331,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.7420791666666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 470,
          "accuracy": 0.4125
        },
        "0.01": {
          "tp": 195,
          "fn": 605,
          "accuracy": 0.24375
        }
      },
      "auroc": 0.7655177083333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.7656531249999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5178739583333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6417635416666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6596791666666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.419871875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5397755208333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7126661458333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4688729166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 762,
          "accuracy": 0.0475
        },
        "0.01": {
          "tp": 2,
          "fn": 798,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.5907695312500001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.7876583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.7876583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.7547000000000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.7547000000000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        },
        "0.01": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        }
      },
      "auroc": 0.7711791666666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        },
        "0.01": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        }
      },
      "auroc": 0.7711791666666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.515590625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.515590625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.48707083333333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.48707083333333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.5013307291666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.5013307291666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5863614583333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5863614583333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5336333333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5336333333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5599973958333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5599973958333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6332125
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6332125
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.3639895833333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.3639895833333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4986010416666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4986010416666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.5648708333333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.5648708333333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5393364583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5393364583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.5521036458333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.5521036458333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 678,
          "fn": 1522,
          "accuracy": 0.30818181818181817
        },
        "0.01": {
          "tp": 315,
          "fn": 1885,
          "accuracy": 0.1431818181818182
        }
      },
      "auroc": 0.7479272727272728
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 286,
          "fn": 914,
          "accuracy": 0.23833333333333334
        },
        "0.01": {
          "tp": 162,
          "fn": 1038,
          "accuracy": 0.135
        }
      },
      "auroc": 0.6826296875
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 964,
          "fn": 2436,
          "accuracy": 0.28352941176470586
        },
        "0.01": {
          "tp": 477,
          "fn": 2923,
          "accuracy": 0.14029411764705882
        }
      },
      "auroc": 0.7248810661764706
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 2144,
          "accuracy": 0.025454545454545455
        },
        "0.01": {
          "tp": 9,
          "fn": 2191,
          "accuracy": 0.004090909090909091
        }
      },
      "auroc": 0.5949048295454546
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 944,
          "accuracy": 0.21333333333333335
        },
        "0.01": {
          "tp": 152,
          "fn": 1048,
          "accuracy": 0.12666666666666668
        }
      },
      "auroc": 0.6060980902777777
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 3088,
          "accuracy": 0.09176470588235294
        },
        "0.01": {
          "tp": 161,
          "fn": 3239,
          "accuracy": 0.04735294117647059
        }
      },
      "auroc": 0.5988553921568627
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 734,
          "fn": 3666,
          "accuracy": 0.1668181818181818
        },
        "0.01": {
          "tp": 324,
          "fn": 4076,
          "accuracy": 0.07363636363636364
        }
      },
      "auroc": 0.6714160511363636
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 542,
          "fn": 1858,
          "accuracy": 0.22583333333333333
        },
        "0.01": {
          "tp": 314,
          "fn": 2086,
          "accuracy": 0.13083333333333333
        }
      },
      "auroc": 0.644363888888889
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1276,
          "fn": 5524,
          "accuracy": 0.18764705882352942
        },
        "0.01": {
          "tp": 638,
          "fn": 6162,
          "accuracy": 0.09382352941176471
        }
      },
      "auroc": 0.6618682291666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.9833447916666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9784864583333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.980915625
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.9754770833333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.9581364583333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.9668067708333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9794109375000001
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        }
      },
      "auroc": 0.9683114583333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 747,
          "fn": 53,
          "accuracy": 0.93375
        },
        "0.01": {
          "tp": 350,
          "fn": 450,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.9738611979166667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9866552083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.9846156250000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        }
      },
      "auroc": 0.9856354166666668
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.8552395833333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9734375000000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9143385416666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 72,
          "fn": 328,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9209473958333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9790265625000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 667,
          "fn": 133,
          "accuracy": 0.83375
        },
        "0.01": {
          "tp": 126,
          "fn": 674,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.9499869791666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        }
      },
      "auroc": 0.9588822916666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9721854166666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9655338541666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.9534552083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9593979166666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9564265625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.95616875
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 150,
          "fn": 250,
          "accuracy": 0.375
        }
      },
      "auroc": 0.9657916666666668
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 689,
          "fn": 111,
          "accuracy": 0.86125
        },
        "0.01": {
          "tp": 323,
          "fn": 477,
          "accuracy": 0.40375
        }
      },
      "auroc": 0.9609802083333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.9866395833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9876375000000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 124,
          "fn": 276,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9871385416666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.8319406250000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.9794625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": {
          "tp": 65,
          "fn": 335,
          "accuracy": 0.1625
        }
      },
      "auroc": 0.9057015625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        },
        "0.01": {
          "tp": 65,
          "fn": 335,
          "accuracy": 0.1625
        }
      },
      "auroc": 0.9092901041666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 124,
          "fn": 276,
          "accuracy": 0.31
        }
      },
      "auroc": 0.98355
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 654,
          "fn": 146,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 189,
          "fn": 611,
          "accuracy": 0.23625
        }
      },
      "auroc": 0.9464200520833334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9860625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.9789010416666668
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9824817708333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.76715625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9855302083333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 53,
          "fn": 347,
          "accuracy": 0.1325
        }
      },
      "auroc": 0.8763432291666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 73,
          "fn": 327,
          "accuracy": 0.1825
        }
      },
      "auroc": 0.876609375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9822156250000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 646,
          "fn": 154,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 139,
          "fn": 661,
          "accuracy": 0.17375
        }
      },
      "auroc": 0.9294125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9746958333333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.9813552083333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9780255208333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9722302083333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.9793916666666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        }
      },
      "auroc": 0.9758109375
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        }
      },
      "auroc": 0.9734630208333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        }
      },
      "auroc": 0.9803734375000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 760,
          "fn": 40,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 257,
          "fn": 543,
          "accuracy": 0.32125
        }
      },
      "auroc": 0.9769182291666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.9385166666666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.9385166666666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        }
      },
      "auroc": 0.945659375
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        }
      },
      "auroc": 0.945659375
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        }
      },
      "auroc": 0.9420880208333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        }
      },
      "auroc": 0.9420880208333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.71633125
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.71633125
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.6575552083333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.6575552083333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 313,
          "accuracy": 0.2175
        },
        "0.01": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        }
      },
      "auroc": 0.6869432291666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 313,
          "accuracy": 0.2175
        },
        "0.01": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        }
      },
      "auroc": 0.6869432291666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9799760416666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9799760416666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9686979166666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9686979166666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.9743369791666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.9743369791666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.9677333333333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.9677333333333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.76915
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.76915
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 111,
          "fn": 289,
          "accuracy": 0.2775
        }
      },
      "auroc": 0.8684416666666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 111,
          "fn": 289,
          "accuracy": 0.2775
        }
      },
      "auroc": 0.8684416666666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.81075625
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.81075625
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.75084375
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.75084375
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 67,
          "fn": 333,
          "accuracy": 0.1675
        }
      },
      "auroc": 0.7808
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 67,
          "fn": 333,
          "accuracy": 0.1675
        }
      },
      "auroc": 0.7808
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1823,
          "fn": 377,
          "accuracy": 0.8286363636363636
        },
        "0.01": {
          "tp": 671,
          "fn": 1529,
          "accuracy": 0.305
        }
      },
      "auroc": 0.9354176136363637
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1159,
          "fn": 41,
          "accuracy": 0.9658333333333333
        },
        "0.01": {
          "tp": 351,
          "fn": 849,
          "accuracy": 0.2925
        }
      },
      "auroc": 0.9805302083333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2982,
          "fn": 418,
          "accuracy": 0.8770588235294118
        },
        "0.01": {
          "tp": 1022,
          "fn": 2378,
          "accuracy": 0.30058823529411766
        }
      },
      "auroc": 0.951339705882353
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1251,
          "fn": 949,
          "accuracy": 0.5686363636363636
        },
        "0.01": {
          "tp": 546,
          "fn": 1654,
          "accuracy": 0.24818181818181817
        }
      },
      "auroc": 0.858855018939394
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1123,
          "fn": 77,
          "accuracy": 0.9358333333333333
        },
        "0.01": {
          "tp": 348,
          "fn": 852,
          "accuracy": 0.29
        }
      },
      "auroc": 0.972559375
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2374,
          "fn": 1026,
          "accuracy": 0.6982352941176471
        },
        "0.01": {
          "tp": 894,
          "fn": 2506,
          "accuracy": 0.26294117647058823
        }
      },
      "auroc": 0.898985968137255
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3074,
          "fn": 1326,
          "accuracy": 0.6986363636363636
        },
        "0.01": {
          "tp": 1217,
          "fn": 3183,
          "accuracy": 0.2765909090909091
        }
      },
      "auroc": 0.8971363162878787
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2282,
          "fn": 118,
          "accuracy": 0.9508333333333333
        },
        "0.01": {
          "tp": 699,
          "fn": 1701,
          "accuracy": 0.29125
        }
      },
      "auroc": 0.9765447916666667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5356,
          "fn": 1444,
          "accuracy": 0.7876470588235294
        },
        "0.01": {
          "tp": 1916,
          "fn": 4884,
          "accuracy": 0.2817647058823529
        }
      },
      "auroc": 0.925162837009804
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.9867218750000001
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.98308125
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.9849015625
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9836208333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9619229166666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9727718750000001
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        }
      },
      "auroc": 0.9851713541666668
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9725020833333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 768,
          "fn": 32,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 371,
          "fn": 429,
          "accuracy": 0.46375
        }
      },
      "auroc": 0.97883671875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.9860552083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.9846927083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9853739583333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9194645833333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.9739718749999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 80,
          "fn": 320,
          "accuracy": 0.2
        }
      },
      "auroc": 0.9467182291666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        }
      },
      "auroc": 0.9527598958333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9793322916666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 713,
          "fn": 87,
          "accuracy": 0.89125
        },
        "0.01": {
          "tp": 136,
          "fn": 664,
          "accuracy": 0.17
        }
      },
      "auroc": 0.96604609375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        }
      },
      "auroc": 0.9653281250000001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.972625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.9689765625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9619635416666668
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.9605750000000001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 150,
          "fn": 250,
          "accuracy": 0.375
        }
      },
      "auroc": 0.9612692708333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9636458333333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 155,
          "fn": 245,
          "accuracy": 0.3875
        }
      },
      "auroc": 0.9666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 702,
          "fn": 98,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 325,
          "fn": 475,
          "accuracy": 0.40625
        }
      },
      "auroc": 0.9651229166666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9864989583333332
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.987278125
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9868885416666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9253854166666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9791875000000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        }
      },
      "auroc": 0.9522864583333335
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 88,
          "fn": 312,
          "accuracy": 0.22
        }
      },
      "auroc": 0.9559421875
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 121,
          "fn": 279,
          "accuracy": 0.3025
        }
      },
      "auroc": 0.9832328125
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 726,
          "fn": 74,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 209,
          "fn": 591,
          "accuracy": 0.26125
        }
      },
      "auroc": 0.9695874999999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.98574375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.9787291666666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        }
      },
      "auroc": 0.9822364583333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.8554812500000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9848874999999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.920184375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 74,
          "fn": 326,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9206125000000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9818083333333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 675,
          "fn": 125,
          "accuracy": 0.84375
        },
        "0.01": {
          "tp": 140,
          "fn": 660,
          "accuracy": 0.175
        }
      },
      "auroc": 0.9512104166666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9755916666666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9814791666666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        }
      },
      "auroc": 0.9785354166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.9764114583333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9812968750000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9788541666666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 123,
          "fn": 277,
          "accuracy": 0.3075
        }
      },
      "auroc": 0.9760015625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 136,
          "fn": 264,
          "accuracy": 0.34
        }
      },
      "auroc": 0.9813880208333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 259,
          "fn": 541,
          "accuracy": 0.32375
        }
      },
      "auroc": 0.9786947916666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9492583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9492583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9506510416666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9506510416666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        }
      },
      "auroc": 0.9499546875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        }
      },
      "auroc": 0.9499546875
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.7902604166666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.7902604166666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.726134375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.726134375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        },
        "0.01": {
          "tp": 42,
          "fn": 358,
          "accuracy": 0.105
        }
      },
      "auroc": 0.7581973958333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        },
        "0.01": {
          "tp": 42,
          "fn": 358,
          "accuracy": 0.105
        }
      },
      "auroc": 0.7581973958333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.982440625
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.982440625
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9736833333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9736833333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        }
      },
      "auroc": 0.9780619791666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        }
      },
      "auroc": 0.9780619791666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9813322916666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9813322916666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.8531385416666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.8531385416666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9172354166666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9172354166666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.8641145833333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.8641145833333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.8028614583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.8028614583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 79,
          "fn": 321,
          "accuracy": 0.1975
        }
      },
      "auroc": 0.8334880208333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 79,
          "fn": 321,
          "accuracy": 0.1975
        }
      },
      "auroc": 0.8334880208333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1900,
          "fn": 300,
          "accuracy": 0.8636363636363636
        },
        "0.01": {
          "tp": 685,
          "fn": 1515,
          "accuracy": 0.31136363636363634
        }
      },
      "auroc": 0.9503041666666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1163,
          "fn": 37,
          "accuracy": 0.9691666666666666
        },
        "0.01": {
          "tp": 365,
          "fn": 835,
          "accuracy": 0.30416666666666664
        }
      },
      "auroc": 0.9813142361111111
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3063,
          "fn": 337,
          "accuracy": 0.9008823529411765
        },
        "0.01": {
          "tp": 1050,
          "fn": 2350,
          "accuracy": 0.3088235294117647
        }
      },
      "auroc": 0.9612488970588235
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1489,
          "fn": 711,
          "accuracy": 0.6768181818181818
        },
        "0.01": {
          "tp": 606,
          "fn": 1594,
          "accuracy": 0.27545454545454545
        }
      },
      "auroc": 0.902617803030303
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1125,
          "fn": 75,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 377,
          "fn": 823,
          "accuracy": 0.31416666666666665
        }
      },
      "auroc": 0.9736402777777777
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2614,
          "fn": 786,
          "accuracy": 0.7688235294117647
        },
        "0.01": {
          "tp": 983,
          "fn": 2417,
          "accuracy": 0.28911764705882353
        }
      },
      "auroc": 0.9276845588235294
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3389,
          "fn": 1011,
          "accuracy": 0.7702272727272728
        },
        "0.01": {
          "tp": 1291,
          "fn": 3109,
          "accuracy": 0.2934090909090909
        }
      },
      "auroc": 0.9264609848484848
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2288,
          "fn": 112,
          "accuracy": 0.9533333333333334
        },
        "0.01": {
          "tp": 742,
          "fn": 1658,
          "accuracy": 0.30916666666666665
        }
      },
      "auroc": 0.9774772569444445
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5677,
          "fn": 1123,
          "accuracy": 0.8348529411764706
        },
        "0.01": {
          "tp": 2033,
          "fn": 4767,
          "accuracy": 0.2989705882352941
        }
      },
      "auroc": 0.9444667279411765
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8757791666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8630239583333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8694015625
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.877321875
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8601791666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8687505208333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8765505208333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8616015625
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8690760416666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9196927083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.880321875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 364,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9000072916666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8559531249999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8612614583333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8586072916666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8878229166666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8707916666666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 762,
          "accuracy": 0.0475
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8793072916666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8638781249999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8549791666666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8594286458333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.86260625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8443333333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8534697916666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8632421874999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.84965625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 799,
          "accuracy": 0.00125
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.85644921875
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9372104166666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8781958333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.9077031249999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8488010416666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8508718749999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8498364583333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.8930057291666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8645338541666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 716,
          "accuracy": 0.105
        },
        "0.01": {
          "tp": 1,
          "fn": 799,
          "accuracy": 0.00125
        }
      },
      "auroc": 0.8787697916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9246364583333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8358552083333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8802458333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8536166666666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.84345625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8485364583333332
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8891265625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8396557291666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 767,
          "accuracy": 0.04125
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8643911458333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.86605
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8344875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.85026875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8524729166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8325739583333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8425234375
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8592614583333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8335307291666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8463960937499999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8738947916666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8738947916666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8614479166666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8614479166666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8676713541666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8676713541666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8471135416666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8471135416666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8426208333333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8426208333333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8448671875
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8448671875
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8575552083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8575552083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8496208333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8496208333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8535880208333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8535880208333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8767770833333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8767770833333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8500145833333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8500145833333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8633958333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8633958333333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.84195
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.84195
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.842071875
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.842071875
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8420109375
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8420109375
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 2044,
          "accuracy": 0.07090909090909091
        },
        "0.01": {
          "tp": 1,
          "fn": 2199,
          "accuracy": 0.00045454545454545455
        }
      },
      "auroc": 0.8804124999999999
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 1199,
          "accuracy": 0.0008333333333333334
        },
        "0.01": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8578105902777777
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 3243,
          "accuracy": 0.04617647058823529
        },
        "0.01": {
          "tp": 1,
          "fn": 3399,
          "accuracy": 0.0002941176470588235
        }
      },
      "auroc": 0.8724353553921569
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 2198,
          "accuracy": 0.0009090909090909091
        },
        "0.01": {
          "tp": 0,
          "fn": 2200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8542316287878787
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 1198,
          "accuracy": 0.0016666666666666668
        },
        "0.01": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8487793402777778
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 3396,
          "accuracy": 0.001176470588235294
        },
        "0.01": {
          "tp": 0,
          "fn": 3400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8523072916666667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 4242,
          "accuracy": 0.03590909090909091
        },
        "0.01": {
          "tp": 1,
          "fn": 4399,
          "accuracy": 0.00022727272727272727
        }
      },
      "auroc": 0.8673220643939394
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 2397,
          "accuracy": 0.00125
        },
        "0.01": {
          "tp": 0,
          "fn": 2400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8532949652777778
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 6639,
          "accuracy": 0.023676470588235295
        },
        "0.01": {
          "tp": 1,
          "fn": 6799,
          "accuracy": 0.00014705882352941175
        }
      },
      "auroc": 0.8623713235294118
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1958,
          "fn": 442,
          "accuracy": 0.8158333333333333
        },
        "0.01": {
          "tp": 845,
          "fn": 1555,
          "accuracy": 0.35208333333333336
        }
      },
      "auroc": 0.9546810763888889
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1870,
          "fn": 530,
          "accuracy": 0.7791666666666667
        },
        "0.01": {
          "tp": 969,
          "fn": 1431,
          "accuracy": 0.40375
        }
      },
      "auroc": 0.9303752604166666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3828,
          "fn": 972,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 1814,
          "fn": 2986,
          "accuracy": 0.3779166666666667
        }
      },
      "auroc": 0.9425281684027778
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1908,
          "fn": 492,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 889,
          "fn": 1511,
          "accuracy": 0.37041666666666667
        }
      },
      "auroc": 0.9490682291666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1724,
          "fn": 676,
          "accuracy": 0.7183333333333334
        },
        "0.01": {
          "tp": 877,
          "fn": 1523,
          "accuracy": 0.36541666666666667
        }
      },
      "auroc": 0.903614670138889
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3632,
          "fn": 1168,
          "accuracy": 0.7566666666666667
        },
        "0.01": {
          "tp": 1766,
          "fn": 3034,
          "accuracy": 0.36791666666666667
        }
      },
      "auroc": 0.9263414496527778
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3866,
          "fn": 934,
          "accuracy": 0.8054166666666667
        },
        "0.01": {
          "tp": 1734,
          "fn": 3066,
          "accuracy": 0.36125
        }
      },
      "auroc": 0.9518746527777777
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3594,
          "fn": 1206,
          "accuracy": 0.74875
        },
        "0.01": {
          "tp": 1846,
          "fn": 2954,
          "accuracy": 0.38458333333333333
        }
      },
      "auroc": 0.9169949652777778
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7460,
          "fn": 2140,
          "accuracy": 0.7770833333333333
        },
        "0.01": {
          "tp": 3580,
          "fn": 6020,
          "accuracy": 0.3729166666666667
        }
      },
      "auroc": 0.9344348090277779
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2221,
          "fn": 179,
          "accuracy": 0.9254166666666667
        },
        "0.01": {
          "tp": 504,
          "fn": 1896,
          "accuracy": 0.21
        }
      },
      "auroc": 0.9807157118055556
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2105,
          "fn": 295,
          "accuracy": 0.8770833333333333
        },
        "0.01": {
          "tp": 403,
          "fn": 1997,
          "accuracy": 0.16791666666666666
        }
      },
      "auroc": 0.9720384548611111
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4326,
          "fn": 474,
          "accuracy": 0.90125
        },
        "0.01": {
          "tp": 907,
          "fn": 3893,
          "accuracy": 0.18895833333333334
        }
      },
      "auroc": 0.9763770833333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1170,
          "fn": 1230,
          "accuracy": 0.4875
        },
        "0.01": {
          "tp": 428,
          "fn": 1972,
          "accuracy": 0.17833333333333334
        }
      },
      "auroc": 0.8754177083333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1957,
          "fn": 443,
          "accuracy": 0.8154166666666667
        },
        "0.01": {
          "tp": 453,
          "fn": 1947,
          "accuracy": 0.18875
        }
      },
      "auroc": 0.9521840277777778
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3127,
          "fn": 1673,
          "accuracy": 0.6514583333333334
        },
        "0.01": {
          "tp": 881,
          "fn": 3919,
          "accuracy": 0.18354166666666666
        }
      },
      "auroc": 0.9138008680555555
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3391,
          "fn": 1409,
          "accuracy": 0.7064583333333333
        },
        "0.01": {
          "tp": 932,
          "fn": 3868,
          "accuracy": 0.19416666666666665
        }
      },
      "auroc": 0.9280667100694444
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4062,
          "fn": 738,
          "accuracy": 0.84625
        },
        "0.01": {
          "tp": 856,
          "fn": 3944,
          "accuracy": 0.17833333333333334
        }
      },
      "auroc": 0.9621112413194445
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7453,
          "fn": 2147,
          "accuracy": 0.7763541666666667
        },
        "0.01": {
          "tp": 1788,
          "fn": 7812,
          "accuracy": 0.18625
        }
      },
      "auroc": 0.9450889756944445
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1721,
          "fn": 679,
          "accuracy": 0.7170833333333333
        },
        "0.01": {
          "tp": 852,
          "fn": 1548,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9293974826388889
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1776,
          "fn": 624,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 834,
          "fn": 1566,
          "accuracy": 0.3475
        }
      },
      "auroc": 0.9252050347222223
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3497,
          "fn": 1303,
          "accuracy": 0.7285416666666666
        },
        "0.01": {
          "tp": 1686,
          "fn": 3114,
          "accuracy": 0.35125
        }
      },
      "auroc": 0.9273012586805556
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1657,
          "fn": 743,
          "accuracy": 0.6904166666666667
        },
        "0.01": {
          "tp": 782,
          "fn": 1618,
          "accuracy": 0.3258333333333333
        }
      },
      "auroc": 0.9228881944444445
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1689,
          "fn": 711,
          "accuracy": 0.70375
        },
        "0.01": {
          "tp": 709,
          "fn": 1691,
          "accuracy": 0.29541666666666666
        }
      },
      "auroc": 0.9141125868055555
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3346,
          "fn": 1454,
          "accuracy": 0.6970833333333334
        },
        "0.01": {
          "tp": 1491,
          "fn": 3309,
          "accuracy": 0.310625
        }
      },
      "auroc": 0.918500390625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3378,
          "fn": 1422,
          "accuracy": 0.70375
        },
        "0.01": {
          "tp": 1634,
          "fn": 3166,
          "accuracy": 0.34041666666666665
        }
      },
      "auroc": 0.9261428385416667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3465,
          "fn": 1335,
          "accuracy": 0.721875
        },
        "0.01": {
          "tp": 1543,
          "fn": 3257,
          "accuracy": 0.32145833333333335
        }
      },
      "auroc": 0.9196588107638889
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6843,
          "fn": 2757,
          "accuracy": 0.7128125
        },
        "0.01": {
          "tp": 3177,
          "fn": 6423,
          "accuracy": 0.3309375
        }
      },
      "auroc": 0.9229008246527777
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2273,
          "fn": 127,
          "accuracy": 0.9470833333333334
        },
        "0.01": {
          "tp": 540,
          "fn": 1860,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9824921006944445
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1990,
          "fn": 410,
          "accuracy": 0.8291666666666667
        },
        "0.01": {
          "tp": 888,
          "fn": 1512,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9587301215277779
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4263,
          "fn": 537,
          "accuracy": 0.888125
        },
        "0.01": {
          "tp": 1428,
          "fn": 3372,
          "accuracy": 0.2975
        }
      },
      "auroc": 0.9706111111111111
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1124,
          "fn": 1276,
          "accuracy": 0.4683333333333333
        },
        "0.01": {
          "tp": 359,
          "fn": 2041,
          "accuracy": 0.14958333333333335
        }
      },
      "auroc": 0.8739571180555555
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1825,
          "fn": 575,
          "accuracy": 0.7604166666666666
        },
        "0.01": {
          "tp": 629,
          "fn": 1771,
          "accuracy": 0.26208333333333333
        }
      },
      "auroc": 0.9182825520833332
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2949,
          "fn": 1851,
          "accuracy": 0.614375
        },
        "0.01": {
          "tp": 988,
          "fn": 3812,
          "accuracy": 0.20583333333333334
        }
      },
      "auroc": 0.8961198350694445
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3397,
          "fn": 1403,
          "accuracy": 0.7077083333333334
        },
        "0.01": {
          "tp": 899,
          "fn": 3901,
          "accuracy": 0.18729166666666666
        }
      },
      "auroc": 0.9282246093750001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3815,
          "fn": 985,
          "accuracy": 0.7947916666666667
        },
        "0.01": {
          "tp": 1517,
          "fn": 3283,
          "accuracy": 0.31604166666666667
        }
      },
      "auroc": 0.9385063368055556
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7212,
          "fn": 2388,
          "accuracy": 0.75125
        },
        "0.01": {
          "tp": 2416,
          "fn": 7184,
          "accuracy": 0.25166666666666665
        }
      },
      "auroc": 0.9333654730902777
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2215,
          "fn": 185,
          "accuracy": 0.9229166666666667
        },
        "0.01": {
          "tp": 674,
          "fn": 1726,
          "accuracy": 0.2808333333333333
        }
      },
      "auroc": 0.9809152777777778
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2018,
          "fn": 382,
          "accuracy": 0.8408333333333333
        },
        "0.01": {
          "tp": 453,
          "fn": 1947,
          "accuracy": 0.18875
        }
      },
      "auroc": 0.94618359375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4233,
          "fn": 567,
          "accuracy": 0.881875
        },
        "0.01": {
          "tp": 1127,
          "fn": 3673,
          "accuracy": 0.23479166666666668
        }
      },
      "auroc": 0.963549435763889
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 772,
          "fn": 1628,
          "accuracy": 0.32166666666666666
        },
        "0.01": {
          "tp": 259,
          "fn": 2141,
          "accuracy": 0.10791666666666666
        }
      },
      "auroc": 0.816329861111111
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1919,
          "fn": 481,
          "accuracy": 0.7995833333333333
        },
        "0.01": {
          "tp": 414,
          "fn": 1986,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.9375473090277778
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2691,
          "fn": 2109,
          "accuracy": 0.560625
        },
        "0.01": {
          "tp": 673,
          "fn": 4127,
          "accuracy": 0.14020833333333332
        }
      },
      "auroc": 0.8769385850694444
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2987,
          "fn": 1813,
          "accuracy": 0.6222916666666667
        },
        "0.01": {
          "tp": 933,
          "fn": 3867,
          "accuracy": 0.194375
        }
      },
      "auroc": 0.8986225694444445
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3937,
          "fn": 863,
          "accuracy": 0.8202083333333333
        },
        "0.01": {
          "tp": 867,
          "fn": 3933,
          "accuracy": 0.180625
        }
      },
      "auroc": 0.9418654513888889
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6924,
          "fn": 2676,
          "accuracy": 0.72125
        },
        "0.01": {
          "tp": 1800,
          "fn": 7800,
          "accuracy": 0.1875
        }
      },
      "auroc": 0.9202440104166667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1929,
          "fn": 471,
          "accuracy": 0.80375
        },
        "0.01": {
          "tp": 621,
          "fn": 1779,
          "accuracy": 0.25875
        }
      },
      "auroc": 0.94888359375
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1932,
          "fn": 468,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 663,
          "fn": 1737,
          "accuracy": 0.27625
        }
      },
      "auroc": 0.9300561631944445
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3861,
          "fn": 939,
          "accuracy": 0.804375
        },
        "0.01": {
          "tp": 1284,
          "fn": 3516,
          "accuracy": 0.2675
        }
      },
      "auroc": 0.9394698784722223
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1855,
          "fn": 545,
          "accuracy": 0.7729166666666667
        },
        "0.01": {
          "tp": 772,
          "fn": 1628,
          "accuracy": 0.32166666666666666
        }
      },
      "auroc": 0.9378329861111111
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1874,
          "fn": 526,
          "accuracy": 0.7808333333333334
        },
        "0.01": {
          "tp": 837,
          "fn": 1563,
          "accuracy": 0.34875
        }
      },
      "auroc": 0.9193505208333332
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3729,
          "fn": 1071,
          "accuracy": 0.776875
        },
        "0.01": {
          "tp": 1609,
          "fn": 3191,
          "accuracy": 0.33520833333333333
        }
      },
      "auroc": 0.9285917534722222
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3784,
          "fn": 1016,
          "accuracy": 0.7883333333333333
        },
        "0.01": {
          "tp": 1393,
          "fn": 3407,
          "accuracy": 0.29020833333333335
        }
      },
      "auroc": 0.9433582899305556
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3806,
          "fn": 994,
          "accuracy": 0.7929166666666667
        },
        "0.01": {
          "tp": 1500,
          "fn": 3300,
          "accuracy": 0.3125
        }
      },
      "auroc": 0.9247033420138889
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7590,
          "fn": 2010,
          "accuracy": 0.790625
        },
        "0.01": {
          "tp": 2893,
          "fn": 6707,
          "accuracy": 0.30135416666666665
        }
      },
      "auroc": 0.9340308159722222
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1628,
          "fn": 772,
          "accuracy": 0.6783333333333333
        },
        "0.01": {
          "tp": 758,
          "fn": 1642,
          "accuracy": 0.31583333333333335
        }
      },
      "auroc": 0.9254935763888887
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1628,
          "fn": 772,
          "accuracy": 0.6783333333333333
        },
        "0.01": {
          "tp": 758,
          "fn": 1642,
          "accuracy": 0.31583333333333335
        }
      },
      "auroc": 0.9254935763888887
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 802,
          "accuracy": 0.6658333333333334
        },
        "0.01": {
          "tp": 815,
          "fn": 1585,
          "accuracy": 0.33958333333333335
        }
      },
      "auroc": 0.9240828125
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 802,
          "accuracy": 0.6658333333333334
        },
        "0.01": {
          "tp": 815,
          "fn": 1585,
          "accuracy": 0.33958333333333335
        }
      },
      "auroc": 0.9240828125
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3226,
          "fn": 1574,
          "accuracy": 0.6720833333333334
        },
        "0.01": {
          "tp": 1573,
          "fn": 3227,
          "accuracy": 0.3277083333333333
        }
      },
      "auroc": 0.9247881944444443
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3226,
          "fn": 1574,
          "accuracy": 0.6720833333333334
        },
        "0.01": {
          "tp": 1573,
          "fn": 3227,
          "accuracy": 0.3277083333333333
        }
      },
      "auroc": 0.9247881944444443
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 659,
          "fn": 1741,
          "accuracy": 0.27458333333333335
        },
        "0.01": {
          "tp": 241,
          "fn": 2159,
          "accuracy": 0.10041666666666667
        }
      },
      "auroc": 0.7614294270833333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 659,
          "fn": 1741,
          "accuracy": 0.27458333333333335
        },
        "0.01": {
          "tp": 241,
          "fn": 2159,
          "accuracy": 0.10041666666666667
        }
      },
      "auroc": 0.7614294270833333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 537,
          "fn": 1863,
          "accuracy": 0.22375
        },
        "0.01": {
          "tp": 187,
          "fn": 2213,
          "accuracy": 0.07791666666666666
        }
      },
      "auroc": 0.7093800347222221
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 537,
          "fn": 1863,
          "accuracy": 0.22375
        },
        "0.01": {
          "tp": 187,
          "fn": 2213,
          "accuracy": 0.07791666666666666
        }
      },
      "auroc": 0.7093800347222221
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1196,
          "fn": 3604,
          "accuracy": 0.24916666666666668
        },
        "0.01": {
          "tp": 428,
          "fn": 4372,
          "accuracy": 0.08916666666666667
        }
      },
      "auroc": 0.7354047309027778
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1196,
          "fn": 3604,
          "accuracy": 0.24916666666666668
        },
        "0.01": {
          "tp": 428,
          "fn": 4372,
          "accuracy": 0.08916666666666667
        }
      },
      "auroc": 0.7354047309027778
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1921,
          "fn": 479,
          "accuracy": 0.8004166666666667
        },
        "0.01": {
          "tp": 809,
          "fn": 1591,
          "accuracy": 0.33708333333333335
        }
      },
      "auroc": 0.9366979166666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1921,
          "fn": 479,
          "accuracy": 0.8004166666666667
        },
        "0.01": {
          "tp": 809,
          "fn": 1591,
          "accuracy": 0.33708333333333335
        }
      },
      "auroc": 0.9366979166666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1842,
          "fn": 558,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 821,
          "fn": 1579,
          "accuracy": 0.34208333333333335
        }
      },
      "auroc": 0.9231264756944444
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1842,
          "fn": 558,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 821,
          "fn": 1579,
          "accuracy": 0.34208333333333335
        }
      },
      "auroc": 0.9231264756944444
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3763,
          "fn": 1037,
          "accuracy": 0.7839583333333333
        },
        "0.01": {
          "tp": 1630,
          "fn": 3170,
          "accuracy": 0.33958333333333335
        }
      },
      "auroc": 0.9299121961805555
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3763,
          "fn": 1037,
          "accuracy": 0.7839583333333333
        },
        "0.01": {
          "tp": 1630,
          "fn": 3170,
          "accuracy": 0.33958333333333335
        }
      },
      "auroc": 0.9299121961805555
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1853,
          "fn": 547,
          "accuracy": 0.7720833333333333
        },
        "0.01": {
          "tp": 1011,
          "fn": 1389,
          "accuracy": 0.42125
        }
      },
      "auroc": 0.9382380208333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1853,
          "fn": 547,
          "accuracy": 0.7720833333333333
        },
        "0.01": {
          "tp": 1011,
          "fn": 1389,
          "accuracy": 0.42125
        }
      },
      "auroc": 0.9382380208333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 915,
          "fn": 1485,
          "accuracy": 0.38125
        },
        "0.01": {
          "tp": 305,
          "fn": 2095,
          "accuracy": 0.12708333333333333
        }
      },
      "auroc": 0.7993199652777778
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 915,
          "fn": 1485,
          "accuracy": 0.38125
        },
        "0.01": {
          "tp": 305,
          "fn": 2095,
          "accuracy": 0.12708333333333333
        }
      },
      "auroc": 0.7993199652777778
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2768,
          "fn": 2032,
          "accuracy": 0.5766666666666667
        },
        "0.01": {
          "tp": 1316,
          "fn": 3484,
          "accuracy": 0.27416666666666667
        }
      },
      "auroc": 0.8687789930555555
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2768,
          "fn": 2032,
          "accuracy": 0.5766666666666667
        },
        "0.01": {
          "tp": 1316,
          "fn": 3484,
          "accuracy": 0.27416666666666667
        }
      },
      "auroc": 0.8687789930555555
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1072,
          "fn": 1328,
          "accuracy": 0.44666666666666666
        },
        "0.01": {
          "tp": 411,
          "fn": 1989,
          "accuracy": 0.17125
        }
      },
      "auroc": 0.8294622395833333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1072,
          "fn": 1328,
          "accuracy": 0.44666666666666666
        },
        "0.01": {
          "tp": 411,
          "fn": 1989,
          "accuracy": 0.17125
        }
      },
      "auroc": 0.8294622395833333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 936,
          "fn": 1464,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 370,
          "fn": 2030,
          "accuracy": 0.15416666666666667
        }
      },
      "auroc": 0.7803111979166667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 936,
          "fn": 1464,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 370,
          "fn": 2030,
          "accuracy": 0.15416666666666667
        }
      },
      "auroc": 0.7803111979166667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2008,
          "fn": 2792,
          "accuracy": 0.41833333333333333
        },
        "0.01": {
          "tp": 781,
          "fn": 4019,
          "accuracy": 0.16270833333333334
        }
      },
      "auroc": 0.8048867187500001
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2008,
          "fn": 2792,
          "accuracy": 0.41833333333333333
        },
        "0.01": {
          "tp": 781,
          "fn": 4019,
          "accuracy": 0.16270833333333334
        }
      },
      "auroc": 0.8048867187500001
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 19450,
          "fn": 6950,
          "accuracy": 0.7367424242424242
        },
        "0.01": {
          "tp": 7266,
          "fn": 19134,
          "accuracy": 0.2752272727272727
        }
      },
      "auroc": 0.9244005839646464
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 11691,
          "fn": 2709,
          "accuracy": 0.811875
        },
        "0.01": {
          "tp": 4210,
          "fn": 10190,
          "accuracy": 0.2923611111111111
        }
      },
      "auroc": 0.943764771412037
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 31141,
          "fn": 9659,
          "accuracy": 0.7632598039215687
        },
        "0.01": {
          "tp": 11476,
          "fn": 29324,
          "accuracy": 0.28127450980392155
        }
      },
      "auroc": 0.9312350030637253
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14314,
          "fn": 12086,
          "accuracy": 0.5421969696969697
        },
        "0.01": {
          "tp": 5987,
          "fn": 20413,
          "accuracy": 0.22678030303030303
        }
      },
      "auroc": 0.8647013257575759
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 10988,
          "fn": 3412,
          "accuracy": 0.7630555555555556
        },
        "0.01": {
          "tp": 3919,
          "fn": 10481,
          "accuracy": 0.2721527777777778
        }
      },
      "auroc": 0.9241819444444443
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 25302,
          "fn": 15498,
          "accuracy": 0.6201470588235294
        },
        "0.01": {
          "tp": 9906,
          "fn": 30894,
          "accuracy": 0.24279411764705883
        }
      },
      "auroc": 0.8856944852941177
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 33764,
          "fn": 19036,
          "accuracy": 0.639469696969697
        },
        "0.01": {
          "tp": 13253,
          "fn": 39547,
          "accuracy": 0.25100378787878785
        }
      },
      "auroc": 0.894550954861111
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 22679,
          "fn": 6121,
          "accuracy": 0.7874652777777778
        },
        "0.01": {
          "tp": 8129,
          "fn": 20671,
          "accuracy": 0.28225694444444444
        }
      },
      "auroc": 0.9339733579282407
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 56443,
          "fn": 25157,
          "accuracy": 0.691703431372549
        },
        "0.01": {
          "tp": 21382,
          "fn": 60218,
          "accuracy": 0.2620343137254902
        }
      },
      "auroc": 0.9084647441789215
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9803802083333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9841989583333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9822895833333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9812666666666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9689927083333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9751296875000002
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9808234375
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9765958333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": null
      },
      "auroc": 0.9787096354166667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9854135416666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9808656250000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9831395833333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.9525520833333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9637
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": null
      },
      "auroc": 0.9581260416666668
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9689828125000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9722828125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 726,
          "fn": 74,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9706328125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.982621875
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9800406250000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9813312500000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.981384375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9823427083333331
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9818635416666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9820031250000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9811916666666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9815973958333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9857843749999999
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9817895833333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9837869791666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9638270833333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9726947916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9682609375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9748057291666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9772421875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 753,
          "fn": 47,
          "accuracy": 0.94125
        },
        "0.01": null
      },
      "auroc": 0.9760239583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9858010416666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.979553125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9826770833333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.9160822916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.982496875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.9492895833333332
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.9509416666666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9810249999999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 720,
          "fn": 80,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9659833333333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9765770833333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9781145833333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9773458333333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.978984375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9790125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9789984375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9777807291666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9785635416666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9781721354166667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9760427083333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9760427083333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9743843750000001
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9743843750000001
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9752135416666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9752135416666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.9156291666666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.9156291666666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.8181937499999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.8181937499999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.8669114583333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.8669114583333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9765802083333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9765802083333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.97766875
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.97766875
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9771244791666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9771244791666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9777
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9777
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9699625000000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9699625000000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.97383125
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.97383125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9686770833333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9686770833333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9180489583333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9180489583333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.9433630208333332
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.9433630208333332
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2089,
          "fn": 111,
          "accuracy": 0.9495454545454546
        },
        "0.01": null
      },
      "auroc": 0.9737461174242424
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1182,
          "fn": 18,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9807604166666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3271,
          "fn": 129,
          "accuracy": 0.9620588235294117
        },
        "0.01": null
      },
      "auroc": 0.9762217524509804
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1819,
          "fn": 381,
          "accuracy": 0.8268181818181818
        },
        "0.01": null
      },
      "auroc": 0.9483959280303029
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1148,
          "fn": 52,
          "accuracy": 0.9566666666666667
        },
        "0.01": null
      },
      "auroc": 0.9748732638888888
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2967,
          "fn": 433,
          "accuracy": 0.8726470588235294
        },
        "0.01": null
      },
      "auroc": 0.9577408700980392
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3908,
          "fn": 492,
          "accuracy": 0.8881818181818182
        },
        "0.01": null
      },
      "auroc": 0.9610710227272727
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2330,
          "fn": 70,
          "accuracy": 0.9708333333333333
        },
        "0.01": null
      },
      "auroc": 0.9778168402777777
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6238,
          "fn": 562,
          "accuracy": 0.9173529411764706
        },
        "0.01": null
      },
      "auroc": 0.9669813112745099
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9803802083333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9841989583333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9822895833333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9812666666666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9689927083333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9751296875000002
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9808234375
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9765958333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": null
      },
      "auroc": 0.9787096354166667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9854135416666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9808656250000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9831395833333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.9525520833333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9637
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": null
      },
      "auroc": 0.9581260416666668
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9689828125000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9722828125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 726,
          "fn": 74,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9706328125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.982621875
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9800406250000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9813312500000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.981384375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9823427083333331
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9818635416666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9820031250000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9811916666666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9815973958333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9857843749999999
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9817895833333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9837869791666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9638270833333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9726947916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9682609375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9748057291666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9772421875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 753,
          "fn": 47,
          "accuracy": 0.94125
        },
        "0.01": null
      },
      "auroc": 0.9760239583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9858010416666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.979553125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9826770833333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.9160822916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.982496875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.9492895833333332
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.9509416666666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9810249999999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 720,
          "fn": 80,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9659833333333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9765770833333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9781145833333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9773458333333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.978984375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9790125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9789984375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9777807291666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9785635416666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9781721354166667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9760427083333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9760427083333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9743843750000001
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9743843750000001
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9752135416666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9752135416666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.9156177083333332
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.9156177083333332
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.8181937499999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.8181937499999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.8669057291666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.8669057291666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9765802083333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9765802083333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.97766875
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.97766875
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9771244791666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9771244791666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9777
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9777
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9699625000000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9699625000000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.97383125
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.97383125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9686770833333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9686770833333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9180489583333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9180489583333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.9433630208333332
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.9433630208333332
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2089,
          "fn": 111,
          "accuracy": 0.9495454545454546
        },
        "0.01": null
      },
      "auroc": 0.9737450757575757
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1182,
          "fn": 18,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9807604166666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3271,
          "fn": 129,
          "accuracy": 0.9620588235294117
        },
        "0.01": null
      },
      "auroc": 0.9762210784313726
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1819,
          "fn": 381,
          "accuracy": 0.8268181818181818
        },
        "0.01": null
      },
      "auroc": 0.9483959280303029
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1148,
          "fn": 52,
          "accuracy": 0.9566666666666667
        },
        "0.01": null
      },
      "auroc": 0.9748732638888888
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2967,
          "fn": 433,
          "accuracy": 0.8726470588235294
        },
        "0.01": null
      },
      "auroc": 0.9577408700980392
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3908,
          "fn": 492,
          "accuracy": 0.8881818181818182
        },
        "0.01": null
      },
      "auroc": 0.9610705018939394
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2330,
          "fn": 70,
          "accuracy": 0.9708333333333333
        },
        "0.01": null
      },
      "auroc": 0.9778168402777777
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6238,
          "fn": 562,
          "accuracy": 0.9173529411764706
        },
        "0.01": null
      },
      "auroc": 0.966980974264706
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9929791666666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9841395833333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9885593749999999
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9893447916666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9488072916666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9690760416666668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9911619791666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": null
      },
      "auroc": 0.9664734375
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 749,
          "fn": 51,
          "accuracy": 0.93625
        },
        "0.01": null
      },
      "auroc": 0.9788177083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9889052083333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9822229166666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9855640625000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": null
      },
      "auroc": 0.8991270833333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.96510625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9321166666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.9440161458333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9736645833333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 666,
          "fn": 134,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.9588403645833334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9912
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.963059375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9771296875000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9858770833333332
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.970990625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": null
      },
      "auroc": 0.9784338541666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9885385416666668
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.967025
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 726,
          "fn": 74,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9777817708333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9895145833333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9936270833333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9915708333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": null
      },
      "auroc": 0.9072666666666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9735833333333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.940425
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        },
        "0.01": null
      },
      "auroc": 0.948390625
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9836052083333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 670,
          "fn": 130,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.9659979166666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9904124999999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9788770833333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9846447916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": null
      },
      "auroc": 0.8194385416666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9835708333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": null
      },
      "auroc": 0.9015046875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        },
        "0.01": null
      },
      "auroc": 0.9049255208333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9812239583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 636,
          "fn": 164,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9430747395833334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.992303125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9915843750000001
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.99194375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.991384375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9830645833333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9872244791666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.99184375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9873244791666668
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.9895841145833334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.957075
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.957075
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.9512635416666668
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.9512635416666668
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9541692708333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9541692708333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": null
      },
      "auroc": 0.8353979166666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": null
      },
      "auroc": 0.8353979166666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": null
      },
      "auroc": 0.7315781250000001
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": null
      },
      "auroc": 0.7315781250000001
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 281,
          "accuracy": 0.2975
        },
        "0.01": null
      },
      "auroc": 0.7834880208333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 281,
          "accuracy": 0.2975
        },
        "0.01": null
      },
      "auroc": 0.7834880208333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9941979166666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9941979166666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.99285
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.99285
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9935239583333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9935239583333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9929760416666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9929760416666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": null
      },
      "auroc": 0.8500375
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": null
      },
      "auroc": 0.8500375
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9215067708333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9215067708333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.95653125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.95653125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.8655708333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.8655708333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": null
      },
      "auroc": 0.9110510416666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": null
      },
      "auroc": 0.9110510416666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1983,
          "fn": 217,
          "accuracy": 0.9013636363636364
        },
        "0.01": null
      },
      "auroc": 0.9710447916666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1146,
          "fn": 54,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9822517361111112
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3129,
          "fn": 271,
          "accuracy": 0.9202941176470588
        },
        "0.01": null
      },
      "auroc": 0.9750001838235295
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1353,
          "fn": 847,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.9076125946969698
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1091,
          "fn": 109,
          "accuracy": 0.9091666666666667
        },
        "0.01": null
      },
      "auroc": 0.9708538194444444
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2444,
          "fn": 956,
          "accuracy": 0.7188235294117648
        },
        "0.01": null
      },
      "auroc": 0.9299330269607843
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3336,
          "fn": 1064,
          "accuracy": 0.7581818181818182
        },
        "0.01": null
      },
      "auroc": 0.9393286931818181
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2237,
          "fn": 163,
          "accuracy": 0.9320833333333334
        },
        "0.01": null
      },
      "auroc": 0.9765527777777777
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5573,
          "fn": 1227,
          "accuracy": 0.8195588235294118
        },
        "0.01": null
      },
      "auroc": 0.9524666053921569
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9871218749999999
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9891541666666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9881380208333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9858739583333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9528572916666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.969365625
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9864979166666668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9710057291666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9787518229166668
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9863739583333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9811697916666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9837718750000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.9004166666666665
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9640552083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        },
        "0.01": null
      },
      "auroc": 0.9322359375
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        },
        "0.01": null
      },
      "auroc": 0.9433953125000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9726125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 682,
          "fn": 118,
          "accuracy": 0.8525
        },
        "0.01": null
      },
      "auroc": 0.95800390625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9859125000000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9626604166666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9742864583333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9831083333333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.97149375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9773010416666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9845104166666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": null
      },
      "auroc": 0.9670770833333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 729,
          "fn": 71,
          "accuracy": 0.91125
        },
        "0.01": null
      },
      "auroc": 0.9757937500000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9859312499999999
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9906458333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9882885416666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.9099916666666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9670020833333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": null
      },
      "auroc": 0.938496875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.9479614583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9788239583333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 679,
          "fn": 121,
          "accuracy": 0.84875
        },
        "0.01": null
      },
      "auroc": 0.9633927083333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9873541666666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9776979166666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9825260416666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": null
      },
      "auroc": 0.8445906249999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9790875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": null
      },
      "auroc": 0.9118390625
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": null
      },
      "auroc": 0.9159723958333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9783927083333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 657,
          "fn": 143,
          "accuracy": 0.82125
        },
        "0.01": null
      },
      "auroc": 0.9471825520833335
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.981665625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9851322916666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9833989583333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9851479166666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9795645833333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.98235625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9834067708333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9823484375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9828776041666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9607322916666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9607322916666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9542270833333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9542270833333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.9574796875
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.9574796875
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.8510802083333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.8510802083333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        },
        "0.01": null
      },
      "auroc": 0.7404520833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        },
        "0.01": null
      },
      "auroc": 0.7404520833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 245,
          "accuracy": 0.3875
        },
        "0.01": null
      },
      "auroc": 0.7957661458333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 245,
          "accuracy": 0.3875
        },
        "0.01": null
      },
      "auroc": 0.7957661458333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9839927083333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9839927083333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.986521875
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.986521875
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9852572916666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9852572916666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9850083333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9850083333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.9340020833333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.9340020833333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9595052083333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9595052083333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9506468749999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9506468749999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": null
      },
      "auroc": 0.8831635416666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": null
      },
      "auroc": 0.8831635416666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        },
        "0.01": null
      },
      "auroc": 0.9169052083333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        },
        "0.01": null
      },
      "auroc": 0.9169052083333333
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2017,
          "fn": 183,
          "accuracy": 0.9168181818181819
        },
        "0.01": null
      },
      "auroc": 0.9678017992424244
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1148,
          "fn": 52,
          "accuracy": 0.9566666666666667
        },
        "0.01": null
      },
      "auroc": 0.9810767361111111
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3165,
          "fn": 235,
          "accuracy": 0.9308823529411765
        },
        "0.01": null
      },
      "auroc": 0.9724870710784314
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1544,
          "fn": 656,
          "accuracy": 0.7018181818181818
        },
        "0.01": null
      },
      "auroc": 0.9188632575757576
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1098,
          "fn": 102,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9690100694444445
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2642,
          "fn": 758,
          "accuracy": 0.7770588235294118
        },
        "0.01": null
      },
      "auroc": 0.9365621323529413
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3561,
          "fn": 839,
          "accuracy": 0.8093181818181818
        },
        "0.01": null
      },
      "auroc": 0.943332528409091
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2246,
          "fn": 154,
          "accuracy": 0.9358333333333333
        },
        "0.01": null
      },
      "auroc": 0.9750434027777777
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 5807,
          "fn": 993,
          "accuracy": 0.8539705882352941
        },
        "0.01": null
      },
      "auroc": 0.9545246017156862
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9835947916666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9863510416666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9849729166666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9821177083333335
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9610239583333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9715708333333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.98285625
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9736874999999999
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        },
        "0.01": null
      },
      "auroc": 0.978271875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9858947916666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9812291666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9835619791666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.9152364583333332
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9646260416666668
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.93993125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9505656250000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9729276041666668
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 690,
          "fn": 110,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.9617466145833334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9837489583333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9747322916666668
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9792406250000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.981671875
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9763718749999999
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9790218749999999
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9827104166666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9755520833333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 756,
          "fn": 44,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.97913125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9858489583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9876489583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9867489583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.9325989583333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9716322916666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.952115625
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": null
      },
      "auroc": 0.9592239583333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9796406249999999
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 707,
          "fn": 93,
          "accuracy": 0.88375
        },
        "0.01": null
      },
      "auroc": 0.9694322916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.986153125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9753708333333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9807619791666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": null
      },
      "auroc": 0.8562708333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9806364583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": null
      },
      "auroc": 0.9184536458333332
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": null
      },
      "auroc": 0.9212119791666668
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9780036458333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 668,
          "fn": 132,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9496078125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.97783125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.980028125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9789296875
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9806739583333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9796343750000001
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9801541666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9792526041666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9798312499999999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": null
      },
      "auroc": 0.9795419270833333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9524552083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9524552083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9391072916666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9391072916666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": null
      },
      "auroc": 0.94578125
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": null
      },
      "auroc": 0.94578125
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.8587166666666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.8587166666666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        },
        "0.01": null
      },
      "auroc": 0.7507520833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        },
        "0.01": null
      },
      "auroc": 0.7507520833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": null
      },
      "auroc": 0.804734375
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": null
      },
      "auroc": 0.804734375
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9780083333333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9780083333333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.980028125
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.980028125
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9790182291666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9790182291666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.979659375
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.979659375
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.95498125
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.95498125
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9673203125000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9673203125000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9556916666666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9556916666666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.8945302083333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.8945302083333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": null
      },
      "auroc": 0.9251109375000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": null
      },
      "auroc": 0.9251109375000001
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2034,
          "fn": 166,
          "accuracy": 0.9245454545454546
        },
        "0.01": null
      },
      "auroc": 0.9661457386363637
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1162,
          "fn": 38,
          "accuracy": 0.9683333333333334
        },
        "0.01": null
      },
      "auroc": 0.9808934027777778
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3196,
          "fn": 204,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9713507965686274
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1620,
          "fn": 580,
          "accuracy": 0.7363636363636363
        },
        "0.01": null
      },
      "auroc": 0.9243607954545454
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1119,
          "fn": 81,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9723208333333333
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2739,
          "fn": 661,
          "accuracy": 0.8055882352941176
        },
        "0.01": null
      },
      "auroc": 0.9412878676470589
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3654,
          "fn": 746,
          "accuracy": 0.8304545454545454
        },
        "0.01": null
      },
      "auroc": 0.9452532670454545
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2281,
          "fn": 119,
          "accuracy": 0.9504166666666667
        },
        "0.01": null
      },
      "auroc": 0.9766071180555556
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5935,
          "fn": 865,
          "accuracy": 0.8727941176470588
        },
        "0.01": null
      },
      "auroc": 0.9563193321078431
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9827052083333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9833802083333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9830427083333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9827916666666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9434093750000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9631005208333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9827484375
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9633947916666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 725,
          "fn": 75,
          "accuracy": 0.90625
        },
        "0.01": null
      },
      "auroc": 0.9730716145833336
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9797625000000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": null
      },
      "auroc": 0.9048208333333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": null
      },
      "auroc": 0.9422916666666665
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9276572916666668
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": null
      },
      "auroc": 0.7945302083333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": null
      },
      "auroc": 0.86109375
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9537098958333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": null
      },
      "auroc": 0.8496755208333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 522,
          "fn": 278,
          "accuracy": 0.6525
        },
        "0.01": null
      },
      "auroc": 0.9016927083333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.976509375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.93708125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.9567953124999999
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9757489583333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": null
      },
      "auroc": 0.9011114583333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.9384302083333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9761291666666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.9190963541666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 608,
          "fn": 192,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9476127604166666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9783583333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9900864583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9842223958333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.9488385416666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": null
      },
      "auroc": 0.839146875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": null
      },
      "auroc": 0.8939927083333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": null
      },
      "auroc": 0.9635984375000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.9146166666666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 599,
          "fn": 201,
          "accuracy": 0.74875
        },
        "0.01": null
      },
      "auroc": 0.9391075520833334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9842979166666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9434052083333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.9638515624999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.9182760416666665
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": null
      },
      "auroc": 0.7196447916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        },
        "0.01": null
      },
      "auroc": 0.8189604166666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.9512869791666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.8315250000000001
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 499,
          "fn": 301,
          "accuracy": 0.62375
        },
        "0.01": null
      },
      "auroc": 0.8914059895833333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9846822916666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9805291666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9826057291666668
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.985665625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.9625197916666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9740927083333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9851739583333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.9715244791666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 737,
          "fn": 63,
          "accuracy": 0.92125
        },
        "0.01": null
      },
      "auroc": 0.97834921875
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.9487916666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.9487916666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.94725625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.94725625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9480239583333332
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9480239583333332
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.9215583333333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.9215583333333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.8893343749999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.8893343749999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.9054463541666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.9054463541666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.98731875
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.98731875
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9847947916666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9847947916666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9860567708333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9860567708333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9864635416666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9864635416666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9633447916666665
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9633447916666665
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9749041666666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9749041666666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9549375
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9549375
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.92238125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.92238125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        },
        "0.01": null
      },
      "auroc": 0.938659375
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        },
        "0.01": null
      },
      "auroc": 0.938659375
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1964,
          "fn": 236,
          "accuracy": 0.8927272727272727
        },
        "0.01": null
      },
      "auroc": 0.9713986742424241
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 957,
          "fn": 243,
          "accuracy": 0.7975
        },
        "0.01": null
      },
      "auroc": 0.9565505208333334
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2921,
          "fn": 479,
          "accuracy": 0.8591176470588235
        },
        "0.01": null
      },
      "auroc": 0.9661581495098038
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1695,
          "fn": 505,
          "accuracy": 0.7704545454545455
        },
        "0.01": null
      },
      "auroc": 0.9496445075757576
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 634,
          "fn": 566,
          "accuracy": 0.5283333333333333
        },
        "0.01": null
      },
      "auroc": 0.8600604166666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2329,
          "fn": 1071,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.9180265931372549
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3659,
          "fn": 741,
          "accuracy": 0.831590909090909
        },
        "0.01": null
      },
      "auroc": 0.960521590909091
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1591,
          "fn": 809,
          "accuracy": 0.6629166666666667
        },
        "0.01": null
      },
      "auroc": 0.90830546875
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 5250,
          "fn": 1550,
          "accuracy": 0.7720588235294118
        },
        "0.01": null
      },
      "auroc": 0.9420923713235294
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9804947916666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.984315625
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9824052083333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9814802083333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9683010416666668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9748906250000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9809874999999999
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9763083333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": null
      },
      "auroc": 0.9786479166666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9855020833333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.98078125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9831416666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": null
      },
      "auroc": 0.9514864583333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.962803125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9571447916666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9684942708333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9717921875000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 719,
          "fn": 81,
          "accuracy": 0.89875
        },
        "0.01": null
      },
      "auroc": 0.9701432291666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9827718750000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9801302083333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9814510416666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.981659375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9824822916666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9820708333333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.982215625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.98130625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9817609375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9860833333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9819083333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9839958333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9624760416666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9725239583333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9675
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9742796875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9772161458333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 752,
          "fn": 48,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9757479166666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9859291666666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.979703125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9828161458333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": null
      },
      "auroc": 0.9136979166666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9824135416666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9480557291666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9498135416666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9810583333333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 717,
          "fn": 83,
          "accuracy": 0.89625
        },
        "0.01": null
      },
      "auroc": 0.9654359374999999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9766833333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9782489583333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9774661458333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9790416666666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9790625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9790520833333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9778625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9786557291666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9782591145833334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.975309375
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.975309375
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9744447916666668
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9744447916666668
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9748770833333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9748770833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": null
      },
      "auroc": 0.9147562499999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": null
      },
      "auroc": 0.9147562499999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.8169145833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.8169145833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.8658354166666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.8658354166666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9766822916666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9766822916666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.977828125
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.977828125
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9772552083333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9772552083333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9777958333333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9777958333333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9699489583333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9699489583333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9738723958333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9738723958333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9686854166666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9686854166666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9169229166666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9169229166666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.9428041666666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.9428041666666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2089,
          "fn": 111,
          "accuracy": 0.9495454545454546
        },
        "0.01": null
      },
      "auroc": 0.9736994318181819
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1182,
          "fn": 18,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9808479166666666
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3271,
          "fn": 129,
          "accuracy": 0.9620588235294117
        },
        "0.01": null
      },
      "auroc": 0.9762224264705882
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1812,
          "fn": 388,
          "accuracy": 0.8236363636363636
        },
        "0.01": null
      },
      "auroc": 0.9478091856060606
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1145,
          "fn": 55,
          "accuracy": 0.9541666666666667
        },
        "0.01": null
      },
      "auroc": 0.9745977430555556
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2957,
          "fn": 443,
          "accuracy": 0.8697058823529412
        },
        "0.01": null
      },
      "auroc": 0.9572639705882354
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3901,
          "fn": 499,
          "accuracy": 0.8865909090909091
        },
        "0.01": null
      },
      "auroc": 0.9607543087121213
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2327,
          "fn": 73,
          "accuracy": 0.9695833333333334
        },
        "0.01": null
      },
      "auroc": 0.977722829861111
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6228,
          "fn": 572,
          "accuracy": 0.9158823529411765
        },
        "0.01": null
      },
      "auroc": 0.9667431985294117
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9803802083333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9841989583333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9822895833333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9812666666666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9689927083333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9751296875000002
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9808234375
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9765958333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": null
      },
      "auroc": 0.9787096354166667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9854135416666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9808656250000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9831395833333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.9525375
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9636552083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": null
      },
      "auroc": 0.9580963541666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9689755208333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9722604166666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 726,
          "fn": 74,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9706179687500001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.982621875
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9800406250000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9813312500000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.981384375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9824708333333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9819276041666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9820031250000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9812557291666668
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9816294270833332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9857843749999999
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9817895833333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9837869791666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9638270833333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9730197916666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9684234374999999
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9748057291666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9774046875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 754,
          "fn": 46,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9761052083333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9858010416666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.979553125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9826770833333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.9160822916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.982496875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.9492895833333332
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.9509416666666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9810249999999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 720,
          "fn": 80,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9659833333333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9765770833333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9781145833333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9773458333333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.978984375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9790125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9789984375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9777807291666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9785635416666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9781721354166667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9760427083333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9760427083333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9743843750000001
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9743843750000001
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9752135416666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9752135416666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.91555
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.91555
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.8181937499999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.8181937499999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.866871875
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.866871875
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9765802083333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9765802083333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.97766875
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.97766875
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9771244791666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9771244791666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9777
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9777
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9699625000000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9699625000000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.97383125
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.97383125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9686770833333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9686770833333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9180489583333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9180489583333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.9433630208333332
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.9433630208333332
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2089,
          "fn": 111,
          "accuracy": 0.9495454545454546
        },
        "0.01": null
      },
      "auroc": 0.9737389204545455
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1182,
          "fn": 18,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9807604166666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3271,
          "fn": 129,
          "accuracy": 0.9620588235294117
        },
        "0.01": null
      },
      "auroc": 0.9762170955882353
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1819,
          "fn": 381,
          "accuracy": 0.8268181818181818
        },
        "0.01": null
      },
      "auroc": 0.9483946022727273
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1149,
          "fn": 51,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9749413194444445
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2968,
          "fn": 432,
          "accuracy": 0.8729411764705882
        },
        "0.01": null
      },
      "auroc": 0.9577640318627451
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3908,
          "fn": 492,
          "accuracy": 0.8881818181818182
        },
        "0.01": null
      },
      "auroc": 0.9610667613636363
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2331,
          "fn": 69,
          "accuracy": 0.97125
        },
        "0.01": null
      },
      "auroc": 0.9778508680555555
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6239,
          "fn": 561,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9669905637254901
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.7064895833333332
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4813239583333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.5939067708333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.689159375
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.40295625000000007
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.5460578125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.6978244791666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.44214010416666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 799,
          "accuracy": 0.00125
        },
        "0.01": null
      },
      "auroc": 0.5699822916666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9737843749999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9890666666666668
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9814255208333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": null
      },
      "auroc": 0.6639322916666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9548760416666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        },
        "0.01": null
      },
      "auroc": 0.8094041666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": null
      },
      "auroc": 0.8188583333333332
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9719713541666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 549,
          "fn": 251,
          "accuracy": 0.68625
        },
        "0.01": null
      },
      "auroc": 0.8954148437500001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.7083229166666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.5403625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.6243427083333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.6301625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        },
        "0.01": null
      },
      "auroc": 0.579025
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 369,
          "accuracy": 0.0775
        },
        "0.01": null
      },
      "auroc": 0.60459375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        },
        "0.01": null
      },
      "auroc": 0.6692427083333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        },
        "0.01": null
      },
      "auroc": 0.55969375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 759,
          "accuracy": 0.05125
        },
        "0.01": null
      },
      "auroc": 0.6144682291666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9885322916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.6803947916666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.8344635416666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.6269489583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.40374791666666665
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.5153484375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.807740625
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        },
        "0.01": null
      },
      "auroc": 0.5420713541666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 589,
          "accuracy": 0.26375
        },
        "0.01": null
      },
      "auroc": 0.6749059895833334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9601072916666668
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": null
      },
      "auroc": 0.7001270833333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        },
        "0.01": null
      },
      "auroc": 0.8301171875000001
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.5620791666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": null
      },
      "auroc": 0.6370458333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        },
        "0.01": null
      },
      "auroc": 0.5995625
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        },
        "0.01": null
      },
      "auroc": 0.7610932291666668
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        },
        "0.01": null
      },
      "auroc": 0.6685864583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 534,
          "accuracy": 0.3325
        },
        "0.01": null
      },
      "auroc": 0.7148398437499999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.7410406249999999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.535153125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.638096875
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.6697541666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.3576989583333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.5137265625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.7053973958333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4464260416666668
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 792,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.57591171875
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": null
      },
      "auroc": 0.7566625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": null
      },
      "auroc": 0.7566625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": null
      },
      "auroc": 0.7037729166666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": null
      },
      "auroc": 0.7037729166666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        },
        "0.01": null
      },
      "auroc": 0.7302177083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        },
        "0.01": null
      },
      "auroc": 0.7302177083333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.5494479166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.5494479166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4613916666666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4613916666666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.5054197916666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.5054197916666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.6303854166666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.6303854166666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.5675729166666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.5675729166666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.5989791666666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.5989791666666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.6711125
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.6711125
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4242166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4242166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.5476645833333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.5476645833333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.6644416666666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.6644416666666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.5723989583333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.5723989583333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.6184203125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.6184203125
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 555,
          "fn": 1645,
          "accuracy": 0.25227272727272726
        },
        "0.01": null
      },
      "auroc": 0.7591206439393939
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 923,
          "accuracy": 0.23083333333333333
        },
        "0.01": null
      },
      "auroc": 0.6544046875
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 832,
          "fn": 2568,
          "accuracy": 0.2447058823529412
        },
        "0.01": null
      },
      "auroc": 0.7221620710784313
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 2170,
          "accuracy": 0.013636363636363636
        },
        "0.01": null
      },
      "auroc": 0.5973990530303029
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 952,
          "accuracy": 0.20666666666666667
        },
        "0.01": null
      },
      "auroc": 0.5558916666666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 3122,
          "accuracy": 0.08176470588235295
        },
        "0.01": null
      },
      "auroc": 0.582749387254902
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 585,
          "fn": 3815,
          "accuracy": 0.13295454545454546
        },
        "0.01": null
      },
      "auroc": 0.6782598484848484
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 525,
          "fn": 1875,
          "accuracy": 0.21875
        },
        "0.01": null
      },
      "auroc": 0.6051481770833333
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1110,
          "fn": 5690,
          "accuracy": 0.16323529411764706
        },
        "0.01": null
      },
      "auroc": 0.6524557291666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.98751875
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9893802083333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9884494791666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9856010416666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9690927083333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9773468750000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9865598958333335
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9792364583333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 772,
          "fn": 28,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9828981770833333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9866385416666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9808656250000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9837520833333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": null
      },
      "auroc": 0.8469479166666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9637
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        },
        "0.01": null
      },
      "auroc": 0.9053239583333335
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.9167932291666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9722828125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 652,
          "fn": 148,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9445380208333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9866927083333332
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9706458333333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9786692708333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9829531249999999
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.98128125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9821171875
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9848229166666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9759635416666668
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 748,
          "fn": 52,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9803932291666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.988134375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9850291666666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9865817708333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": null
      },
      "auroc": 0.8563072916666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9732145833333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": null
      },
      "auroc": 0.9147609375000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.9222208333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.979121875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 665,
          "fn": 135,
          "accuracy": 0.83125
        },
        "0.01": null
      },
      "auroc": 0.9506713541666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9887010416666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9786458333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9836734375
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": null
      },
      "auroc": 0.77684375
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9821718749999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.8795078125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": null
      },
      "auroc": 0.8827723958333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9804088541666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 642,
          "fn": 158,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.931590625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9823708333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9844739583333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9834223958333332
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9845343750000001
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9818218750000001
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.983178125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9834526041666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9831479166666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9833002604166667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.9494083333333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.9494083333333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": null
      },
      "auroc": 0.943053125
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": null
      },
      "auroc": 0.943053125
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9462307291666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9462307291666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.777484375
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.777484375
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": null
      },
      "auroc": 0.6211802083333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": null
      },
      "auroc": 0.6211802083333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 301,
          "accuracy": 0.2475
        },
        "0.01": null
      },
      "auroc": 0.6993322916666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 301,
          "accuracy": 0.2475
        },
        "0.01": null
      },
      "auroc": 0.6993322916666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9853833333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9853833333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9871979166666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9871979166666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.986290625
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.986290625
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9855145833333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9855145833333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": null
      },
      "auroc": 0.8105968750000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": null
      },
      "auroc": 0.8105968750000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.8980557291666668
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.8980557291666668
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9236968750000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9236968750000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.8161093749999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.8161093749999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.869903125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.869903125
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1947,
          "fn": 253,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9583221590909091
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1166,
          "fn": 34,
          "accuracy": 0.9716666666666667
        },
        "0.01": null
      },
      "auroc": 0.9815067708333334
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3113,
          "fn": 287,
          "accuracy": 0.9155882352941176
        },
        "0.01": null
      },
      "auroc": 0.9665049632352942
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1316,
          "fn": 884,
          "accuracy": 0.5981818181818181
        },
        "0.01": null
      },
      "auroc": 0.8737568181818183
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1133,
          "fn": 67,
          "accuracy": 0.9441666666666667
        },
        "0.01": null
      },
      "auroc": 0.9752137152777778
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2449,
          "fn": 951,
          "accuracy": 0.7202941176470589
        },
        "0.01": null
      },
      "auroc": 0.9095651348039215
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3263,
          "fn": 1137,
          "accuracy": 0.7415909090909091
        },
        "0.01": null
      },
      "auroc": 0.9160394886363636
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2299,
          "fn": 101,
          "accuracy": 0.9579166666666666
        },
        "0.01": null
      },
      "auroc": 0.9783602430555556
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5562,
          "fn": 1238,
          "accuracy": 0.8179411764705883
        },
        "0.01": null
      },
      "auroc": 0.9380350490196079
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.982003125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9865197916666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9842614583333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9835791666666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9696354166666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9766072916666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9827911458333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9780776041666668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.980434375
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.98546875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9808677083333335
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9831682291666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.9474656250000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9638354166666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.9556505208333335
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": null
      },
      "auroc": 0.9664671874999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9723515625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 720,
          "fn": 80,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9694093750000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9846104166666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9766343750000002
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9806223958333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9814083333333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9805166666666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9809625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.983009375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9785755208333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 768,
          "fn": 32,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9807924479166666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.985834375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9831208333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9844776041666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9584479166666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.97244375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9654458333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": null
      },
      "auroc": 0.9721411458333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9777822916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 744,
          "fn": 56,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.97496171875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9860958333333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9798958333333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9829958333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.9045406249999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9823000000000001
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9434203125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9453182291666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9810979166666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 709,
          "fn": 91,
          "accuracy": 0.88625
        },
        "0.01": null
      },
      "auroc": 0.9632080729166668
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9778822916666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.979428125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9786552083333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9807770833333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9803510416666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9805640625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9793296874999999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9798895833333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9796096354166667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9754177083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9754177083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.973840625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.973840625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9746291666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9746291666666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.9079552083333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.9079552083333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.7985645833333332
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.7985645833333332
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.8532598958333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.8532598958333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9781052083333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9781052083333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9796864583333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9796864583333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9788958333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9788958333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9798541666666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9798541666666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.957053125
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.957053125
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9684536458333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9684536458333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9675156250000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9675156250000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.90770625
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.90770625
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9376109374999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9376109374999999
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2082,
          "fn": 118,
          "accuracy": 0.9463636363636364
        },
        "0.01": null
      },
      "auroc": 0.9737038825757576
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1178,
          "fn": 22,
          "accuracy": 0.9816666666666667
        },
        "0.01": null
      },
      "auroc": 0.9810777777777777
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3260,
          "fn": 140,
          "accuracy": 0.9588235294117647
        },
        "0.01": null
      },
      "auroc": 0.9763064338235294
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1761,
          "fn": 439,
          "accuracy": 0.8004545454545454
        },
        "0.01": null
      },
      "auroc": 0.9430063446969696
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1142,
          "fn": 58,
          "accuracy": 0.9516666666666667
        },
        "0.01": null
      },
      "auroc": 0.9748470486111112
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2903,
          "fn": 497,
          "accuracy": 0.8538235294117648
        },
        "0.01": null
      },
      "auroc": 0.9542442401960785
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3843,
          "fn": 557,
          "accuracy": 0.8734090909090909
        },
        "0.01": null
      },
      "auroc": 0.9583551136363637
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2320,
          "fn": 80,
          "accuracy": 0.9666666666666667
        },
        "0.01": null
      },
      "auroc": 0.9779624131944444
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6163,
          "fn": 637,
          "accuracy": 0.9063235294117648
        },
        "0.01": null
      },
      "auroc": 0.9652753370098038
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8086447916666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8084916666666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8085682291666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.802628125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8040052083333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8033166666666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8056364583333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8062484375
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8059424479166667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.8088479166666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.849596875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.8292223958333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8214989583333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8461020833333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8338005208333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.8151734375
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8478494791666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 798,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.8315114583333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.76629375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8195645833333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7929291666666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7820281250000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8209177083333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8014729166666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7741609375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8202411458333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7972010416666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8702072916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.817234375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8437208333333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8218854166666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.82683125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8243583333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8460463541666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8220328125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8340395833333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.8291499999999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8090218749999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.8190859375
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8237260416666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8265239583333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.825125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.8264380208333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8177729166666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 799,
          "accuracy": 0.00125
        },
        "0.01": null
      },
      "auroc": 0.82210546875
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7797239583333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7666875000000001
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7732057291666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7805052083333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7798937499999999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7801994791666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7801145833333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.773290625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7767026041666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.74853125
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.74853125
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7714052083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7714052083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7599682291666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7599682291666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8072145833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8072145833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8042979166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8042979166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.80575625
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.80575625
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7598666666666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7598666666666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7739572916666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7739572916666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7669119791666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7669119791666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.782909375
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.782909375
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7984687500000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7984687500000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7906890625
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7906890625
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7863812499999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7863812499999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.78243125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.78243125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.78440625
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.78440625
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 2197,
          "accuracy": 0.0013636363636363637
        },
        "0.01": null
      },
      "auroc": 0.7952518939393939
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8117661458333334
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 3397,
          "accuracy": 0.0008823529411764706
        },
        "0.01": null
      },
      "auroc": 0.8010804534313726
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 2200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.7966211174242425
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8173789930555556
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 3400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8039474264705883
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 4397,
          "accuracy": 0.0006818181818181819
        },
        "0.01": null
      },
      "auroc": 0.7959365056818182
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 2400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.8145725694444443
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 6797,
          "accuracy": 0.0004411764705882353
        },
        "0.01": null
      },
      "auroc": 0.8025139399509803
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1990,
          "fn": 410,
          "accuracy": 0.8291666666666667
        },
        "0.01": null
      },
      "auroc": 0.9460577256944445
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1968,
          "fn": 432,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9288044270833333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3958,
          "fn": 842,
          "accuracy": 0.8245833333333333
        },
        "0.01": null
      },
      "auroc": 0.9374310763888889
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1970,
          "fn": 430,
          "accuracy": 0.8208333333333333
        },
        "0.01": null
      },
      "auroc": 0.9438646701388889
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1804,
          "fn": 596,
          "accuracy": 0.7516666666666667
        },
        "0.01": null
      },
      "auroc": 0.9022555555555556
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3774,
          "fn": 1026,
          "accuracy": 0.78625
        },
        "0.01": null
      },
      "auroc": 0.9230601128472222
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3960,
          "fn": 840,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9449611979166668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3772,
          "fn": 1028,
          "accuracy": 0.7858333333333334
        },
        "0.01": null
      },
      "auroc": 0.9155299913194445
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7732,
          "fn": 1868,
          "accuracy": 0.8054166666666667
        },
        "0.01": null
      },
      "auroc": 0.9302455946180557
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2156,
          "fn": 244,
          "accuracy": 0.8983333333333333
        },
        "0.01": null
      },
      "auroc": 0.9697848958333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2095,
          "fn": 305,
          "accuracy": 0.8729166666666667
        },
        "0.01": null
      },
      "auroc": 0.9644348090277778
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4251,
          "fn": 549,
          "accuracy": 0.885625
        },
        "0.01": null
      },
      "auroc": 0.9671098524305556
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1215,
          "fn": 1185,
          "accuracy": 0.50625
        },
        "0.01": null
      },
      "auroc": 0.8942842013888889
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1914,
          "fn": 486,
          "accuracy": 0.7975
        },
        "0.01": null
      },
      "auroc": 0.9392241319444444
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3129,
          "fn": 1671,
          "accuracy": 0.651875
        },
        "0.01": null
      },
      "auroc": 0.9167541666666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3371,
          "fn": 1429,
          "accuracy": 0.7022916666666666
        },
        "0.01": null
      },
      "auroc": 0.9320345486111111
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4009,
          "fn": 791,
          "accuracy": 0.8352083333333333
        },
        "0.01": null
      },
      "auroc": 0.9518294704861111
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7380,
          "fn": 2220,
          "accuracy": 0.76875
        },
        "0.01": null
      },
      "auroc": 0.941932009548611
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1976,
          "fn": 424,
          "accuracy": 0.8233333333333334
        },
        "0.01": null
      },
      "auroc": 0.9428273437499999
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1761,
          "fn": 639,
          "accuracy": 0.73375
        },
        "0.01": null
      },
      "auroc": 0.9220827256944444
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3737,
          "fn": 1063,
          "accuracy": 0.7785416666666667
        },
        "0.01": null
      },
      "auroc": 0.9324550347222222
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1935,
          "fn": 465,
          "accuracy": 0.80625
        },
        "0.01": null
      },
      "auroc": 0.9357309027777778
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1800,
          "fn": 600,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.9259455729166666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3735,
          "fn": 1065,
          "accuracy": 0.778125
        },
        "0.01": null
      },
      "auroc": 0.9308382378472222
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3911,
          "fn": 889,
          "accuracy": 0.8147916666666667
        },
        "0.01": null
      },
      "auroc": 0.9392791232638888
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3561,
          "fn": 1239,
          "accuracy": 0.741875
        },
        "0.01": null
      },
      "auroc": 0.9240141493055556
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7472,
          "fn": 2128,
          "accuracy": 0.7783333333333333
        },
        "0.01": null
      },
      "auroc": 0.9316466362847222
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2193,
          "fn": 207,
          "accuracy": 0.91375
        },
        "0.01": null
      },
      "auroc": 0.9763164930555555
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1997,
          "fn": 403,
          "accuracy": 0.8320833333333333
        },
        "0.01": null
      },
      "auroc": 0.9462553819444444
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4190,
          "fn": 610,
          "accuracy": 0.8729166666666667
        },
        "0.01": null
      },
      "auroc": 0.9612859375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1344,
          "fn": 1056,
          "accuracy": 0.56
        },
        "0.01": null
      },
      "auroc": 0.9013535590277779
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1753,
          "fn": 647,
          "accuracy": 0.7304166666666667
        },
        "0.01": null
      },
      "auroc": 0.9015446180555555
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3097,
          "fn": 1703,
          "accuracy": 0.6452083333333334
        },
        "0.01": null
      },
      "auroc": 0.9014490885416667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3537,
          "fn": 1263,
          "accuracy": 0.736875
        },
        "0.01": null
      },
      "auroc": 0.9388350260416667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3750,
          "fn": 1050,
          "accuracy": 0.78125
        },
        "0.01": null
      },
      "auroc": 0.9238999999999999
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7287,
          "fn": 2313,
          "accuracy": 0.7590625
        },
        "0.01": null
      },
      "auroc": 0.9313675130208333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2144,
          "fn": 256,
          "accuracy": 0.8933333333333333
        },
        "0.01": null
      },
      "auroc": 0.9713003472222221
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1958,
          "fn": 442,
          "accuracy": 0.8158333333333333
        },
        "0.01": null
      },
      "auroc": 0.9384503472222221
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4102,
          "fn": 698,
          "accuracy": 0.8545833333333334
        },
        "0.01": null
      },
      "auroc": 0.9548753472222222
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 997,
          "fn": 1403,
          "accuracy": 0.41541666666666666
        },
        "0.01": null
      },
      "auroc": 0.8473092013888889
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1856,
          "fn": 544,
          "accuracy": 0.7733333333333333
        },
        "0.01": null
      },
      "auroc": 0.9184071180555555
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2853,
          "fn": 1947,
          "accuracy": 0.594375
        },
        "0.01": null
      },
      "auroc": 0.8828581597222223
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3141,
          "fn": 1659,
          "accuracy": 0.654375
        },
        "0.01": null
      },
      "auroc": 0.9093047743055556
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3814,
          "fn": 986,
          "accuracy": 0.7945833333333333
        },
        "0.01": null
      },
      "auroc": 0.9284287326388889
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6955,
          "fn": 2645,
          "accuracy": 0.7244791666666667
        },
        "0.01": null
      },
      "auroc": 0.9188667534722222
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1995,
          "fn": 405,
          "accuracy": 0.83125
        },
        "0.01": null
      },
      "auroc": 0.9436595486111112
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1983,
          "fn": 417,
          "accuracy": 0.82625
        },
        "0.01": null
      },
      "auroc": 0.92630078125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3978,
          "fn": 822,
          "accuracy": 0.82875
        },
        "0.01": null
      },
      "auroc": 0.9349801649305556
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1990,
          "fn": 410,
          "accuracy": 0.8291666666666667
        },
        "0.01": null
      },
      "auroc": 0.9395364583333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1928,
          "fn": 472,
          "accuracy": 0.8033333333333333
        },
        "0.01": null
      },
      "auroc": 0.9100540798611111
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3918,
          "fn": 882,
          "accuracy": 0.81625
        },
        "0.01": null
      },
      "auroc": 0.9247952690972223
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3985,
          "fn": 815,
          "accuracy": 0.8302083333333333
        },
        "0.01": null
      },
      "auroc": 0.9415980034722223
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3911,
          "fn": 889,
          "accuracy": 0.8147916666666667
        },
        "0.01": null
      },
      "auroc": 0.9181774305555556
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7896,
          "fn": 1704,
          "accuracy": 0.8225
        },
        "0.01": null
      },
      "auroc": 0.9298877170138888
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1651,
          "fn": 749,
          "accuracy": 0.6879166666666666
        },
        "0.01": null
      },
      "auroc": 0.9293759548611112
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1651,
          "fn": 749,
          "accuracy": 0.6879166666666666
        },
        "0.01": null
      },
      "auroc": 0.9293759548611112
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1632,
          "fn": 768,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.923460329861111
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1632,
          "fn": 768,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.923460329861111
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3283,
          "fn": 1517,
          "accuracy": 0.6839583333333333
        },
        "0.01": null
      },
      "auroc": 0.9264181423611111
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3283,
          "fn": 1517,
          "accuracy": 0.6839583333333333
        },
        "0.01": null
      },
      "auroc": 0.9264181423611111
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1138,
          "fn": 1262,
          "accuracy": 0.4741666666666667
        },
        "0.01": null
      },
      "auroc": 0.8475340277777778
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1138,
          "fn": 1262,
          "accuracy": 0.4741666666666667
        },
        "0.01": null
      },
      "auroc": 0.8475340277777778
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 750,
          "fn": 1650,
          "accuracy": 0.3125
        },
        "0.01": null
      },
      "auroc": 0.75575390625
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 750,
          "fn": 1650,
          "accuracy": 0.3125
        },
        "0.01": null
      },
      "auroc": 0.75575390625
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1888,
          "fn": 2912,
          "accuracy": 0.3933333333333333
        },
        "0.01": null
      },
      "auroc": 0.8016439670138888
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1888,
          "fn": 2912,
          "accuracy": 0.3933333333333333
        },
        "0.01": null
      },
      "auroc": 0.8016439670138888
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1998,
          "fn": 402,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.9336401041666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1998,
          "fn": 402,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.9336401041666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1992,
          "fn": 408,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9302869791666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1992,
          "fn": 408,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9302869791666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3990,
          "fn": 810,
          "accuracy": 0.83125
        },
        "0.01": null
      },
      "auroc": 0.9319635416666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3990,
          "fn": 810,
          "accuracy": 0.83125
        },
        "0.01": null
      },
      "auroc": 0.9319635416666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1990,
          "fn": 410,
          "accuracy": 0.8291666666666667
        },
        "0.01": null
      },
      "auroc": 0.9395328125000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1990,
          "fn": 410,
          "accuracy": 0.8291666666666667
        },
        "0.01": null
      },
      "auroc": 0.9395328125000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1395,
          "fn": 1005,
          "accuracy": 0.58125
        },
        "0.01": null
      },
      "auroc": 0.8810447916666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1395,
          "fn": 1005,
          "accuracy": 0.58125
        },
        "0.01": null
      },
      "auroc": 0.8810447916666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3385,
          "fn": 1415,
          "accuracy": 0.7052083333333333
        },
        "0.01": null
      },
      "auroc": 0.9102888020833333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3385,
          "fn": 1415,
          "accuracy": 0.7052083333333333
        },
        "0.01": null
      },
      "auroc": 0.9102888020833333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1710,
          "fn": 690,
          "accuracy": 0.7125
        },
        "0.01": null
      },
      "auroc": 0.9195466145833334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1710,
          "fn": 690,
          "accuracy": 0.7125
        },
        "0.01": null
      },
      "auroc": 0.9195466145833334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1368,
          "fn": 1032,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.8596134548611112
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1368,
          "fn": 1032,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.8596134548611112
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3078,
          "fn": 1722,
          "accuracy": 0.64125
        },
        "0.01": null
      },
      "auroc": 0.8895800347222222
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3078,
          "fn": 1722,
          "accuracy": 0.64125
        },
        "0.01": null
      },
      "auroc": 0.8895800347222222
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 20941,
          "fn": 5459,
          "accuracy": 0.793219696969697
        },
        "0.01": null
      },
      "auroc": 0.9381432607323231
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 11762,
          "fn": 2638,
          "accuracy": 0.8168055555555556
        },
        "0.01": null
      },
      "auroc": 0.937721412037037
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 32703,
          "fn": 8097,
          "accuracy": 0.8015441176470588
        },
        "0.01": null
      },
      "auroc": 0.9379943729575164
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 16588,
          "fn": 9812,
          "accuracy": 0.6283333333333333
        },
        "0.01": null
      },
      "auroc": 0.8920216777146465
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 11055,
          "fn": 3345,
          "accuracy": 0.7677083333333333
        },
        "0.01": null
      },
      "auroc": 0.9162385127314814
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 27643,
          "fn": 13157,
          "accuracy": 0.6775245098039215
        },
        "0.01": null
      },
      "auroc": 0.9005687959558822
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37529,
          "fn": 15271,
          "accuracy": 0.7107765151515152
        },
        "0.01": null
      },
      "auroc": 0.9150824692234849
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 22817,
          "fn": 5983,
          "accuracy": 0.7922569444444445
        },
        "0.01": null
      },
      "auroc": 0.9269799623842593
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 60346,
          "fn": 21254,
          "accuracy": 0.7395343137254902
        },
        "0.01": null
      },
      "auroc": 0.9192815844566994
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.996946875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9973739583333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9971604166666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9973239583333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9901916666666668
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9937578125000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9971354166666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9937828125
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 695,
          "fn": 105,
          "accuracy": 0.86875
        }
      },
      "auroc": 0.9954591145833334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9965708333333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9917791666666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        }
      },
      "auroc": 0.994175
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9727552083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9723614583333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9725583333333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9846630208333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9820703125000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 746,
          "fn": 54,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 610,
          "fn": 190,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.9833666666666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9932885416666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9946177083333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9939531250000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.994821875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9957166666666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9952692708333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.9940552083333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9951671875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 572,
          "fn": 228,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9946111979166666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.995465625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.99629375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9958796875
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9430104166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9821520833333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9625812499999999
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        }
      },
      "auroc": 0.9692380208333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9892229166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 713,
          "fn": 87,
          "accuracy": 0.89125
        },
        "0.01": {
          "tp": 567,
          "fn": 233,
          "accuracy": 0.70875
        }
      },
      "auroc": 0.9792304687500001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9953208333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9818041666666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9885625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.9129770833333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9772614583333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        }
      },
      "auroc": 0.9451192708333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        }
      },
      "auroc": 0.9541489583333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9795328125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 681,
          "fn": 119,
          "accuracy": 0.85125
        },
        "0.01": {
          "tp": 551,
          "fn": 249,
          "accuracy": 0.68875
        }
      },
      "auroc": 0.9668408854166667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9913427083333335
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9954104166666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.9933765625000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9868541666666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.96625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9765520833333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        }
      },
      "auroc": 0.9890984375000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9808302083333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 759,
          "fn": 41,
          "accuracy": 0.94875
        },
        "0.01": {
          "tp": 565,
          "fn": 235,
          "accuracy": 0.70625
        }
      },
      "auroc": 0.9849643229166667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9591489583333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9591489583333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9166718749999999
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9166718749999999
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        }
      },
      "auroc": 0.9379104166666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        }
      },
      "auroc": 0.9379104166666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9686177083333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9686177083333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.93499375
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.93499375
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9518057291666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9518057291666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9930729166666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9930729166666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9944395833333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9944395833333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.99375625
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.99375625
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9966333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9966333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9864020833333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9864020833333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9915177083333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9915177083333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9677864583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9677864583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9501291666666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9501291666666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9589578125
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9589578125
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2082,
          "fn": 118,
          "accuracy": 0.9463636363636364
        },
        "0.01": {
          "tp": 1488,
          "fn": 712,
          "accuracy": 0.6763636363636364
        }
      },
      "auroc": 0.9867449810606062
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1164,
          "fn": 36,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 986,
          "fn": 214,
          "accuracy": 0.8216666666666667
        }
      },
      "auroc": 0.9928798611111112
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3246,
          "fn": 154,
          "accuracy": 0.9547058823529412
        },
        "0.01": {
          "tp": 2474,
          "fn": 926,
          "accuracy": 0.7276470588235294
        }
      },
      "auroc": 0.9889102328431374
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1831,
          "fn": 369,
          "accuracy": 0.8322727272727273
        },
        "0.01": {
          "tp": 1280,
          "fn": 920,
          "accuracy": 0.5818181818181818
        }
      },
      "auroc": 0.9627617424242423
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1127,
          "fn": 73,
          "accuracy": 0.9391666666666667
        },
        "0.01": {
          "tp": 955,
          "fn": 245,
          "accuracy": 0.7958333333333333
        }
      },
      "auroc": 0.9806555555555555
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2958,
          "fn": 442,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 2235,
          "fn": 1165,
          "accuracy": 0.6573529411764706
        }
      },
      "auroc": 0.9690772058823529
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3913,
          "fn": 487,
          "accuracy": 0.8893181818181818
        },
        "0.01": {
          "tp": 2768,
          "fn": 1632,
          "accuracy": 0.6290909090909091
        }
      },
      "auroc": 0.9747533617424242
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2291,
          "fn": 109,
          "accuracy": 0.9545833333333333
        },
        "0.01": {
          "tp": 1941,
          "fn": 459,
          "accuracy": 0.80875
        }
      },
      "auroc": 0.9867677083333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6204,
          "fn": 596,
          "accuracy": 0.9123529411764706
        },
        "0.01": {
          "tp": 4709,
          "fn": 2091,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.9789937193627449
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.996946875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9973739583333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9971604166666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9973239583333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9901916666666668
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9937578125000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9971354166666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9937828125
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 695,
          "fn": 105,
          "accuracy": 0.86875
        }
      },
      "auroc": 0.9954591145833334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9965708333333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9917791666666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        }
      },
      "auroc": 0.994175
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9727552083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9723614583333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9725583333333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9846630208333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9820703125000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 746,
          "fn": 54,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 610,
          "fn": 190,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.9833666666666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9932885416666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9946177083333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9939531250000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.994821875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9957166666666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9952692708333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.9940552083333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9951671875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 572,
          "fn": 228,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9946111979166666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.995465625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.99629375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9958796875
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9430104166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9821520833333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9625812499999999
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        }
      },
      "auroc": 0.9692380208333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9892229166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 713,
          "fn": 87,
          "accuracy": 0.89125
        },
        "0.01": {
          "tp": 567,
          "fn": 233,
          "accuracy": 0.70875
        }
      },
      "auroc": 0.9792304687500001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9953208333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9818041666666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9885625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.9129770833333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9772614583333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        }
      },
      "auroc": 0.9451192708333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        }
      },
      "auroc": 0.9541489583333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9795328125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 681,
          "fn": 119,
          "accuracy": 0.85125
        },
        "0.01": {
          "tp": 551,
          "fn": 249,
          "accuracy": 0.68875
        }
      },
      "auroc": 0.9668408854166667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9913427083333335
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9954104166666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.9933765625000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9868541666666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.96625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9765520833333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        }
      },
      "auroc": 0.9890984375000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9808302083333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 759,
          "fn": 41,
          "accuracy": 0.94875
        },
        "0.01": {
          "tp": 565,
          "fn": 235,
          "accuracy": 0.70625
        }
      },
      "auroc": 0.9849643229166667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9591489583333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9591489583333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9166718749999999
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9166718749999999
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        }
      },
      "auroc": 0.9379104166666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        }
      },
      "auroc": 0.9379104166666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9686177083333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9686177083333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.93499375
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.93499375
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9518057291666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9518057291666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9930729166666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9930729166666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9944395833333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9944395833333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.99375625
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.99375625
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9966333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9966333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9864020833333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9864020833333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9915177083333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9915177083333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9677864583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9677864583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9501291666666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9501291666666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9589578125
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9589578125
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2082,
          "fn": 118,
          "accuracy": 0.9463636363636364
        },
        "0.01": {
          "tp": 1488,
          "fn": 712,
          "accuracy": 0.6763636363636364
        }
      },
      "auroc": 0.9867449810606062
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1164,
          "fn": 36,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 986,
          "fn": 214,
          "accuracy": 0.8216666666666667
        }
      },
      "auroc": 0.9928798611111112
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3246,
          "fn": 154,
          "accuracy": 0.9547058823529412
        },
        "0.01": {
          "tp": 2474,
          "fn": 926,
          "accuracy": 0.7276470588235294
        }
      },
      "auroc": 0.9889102328431374
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1831,
          "fn": 369,
          "accuracy": 0.8322727272727273
        },
        "0.01": {
          "tp": 1280,
          "fn": 920,
          "accuracy": 0.5818181818181818
        }
      },
      "auroc": 0.9627617424242423
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1127,
          "fn": 73,
          "accuracy": 0.9391666666666667
        },
        "0.01": {
          "tp": 955,
          "fn": 245,
          "accuracy": 0.7958333333333333
        }
      },
      "auroc": 0.9806555555555555
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2958,
          "fn": 442,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 2235,
          "fn": 1165,
          "accuracy": 0.6573529411764706
        }
      },
      "auroc": 0.9690772058823529
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3913,
          "fn": 487,
          "accuracy": 0.8893181818181818
        },
        "0.01": {
          "tp": 2768,
          "fn": 1632,
          "accuracy": 0.6290909090909091
        }
      },
      "auroc": 0.9747533617424242
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2291,
          "fn": 109,
          "accuracy": 0.9545833333333333
        },
        "0.01": {
          "tp": 1941,
          "fn": 459,
          "accuracy": 0.80875
        }
      },
      "auroc": 0.9867677083333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6204,
          "fn": 596,
          "accuracy": 0.9123529411764706
        },
        "0.01": {
          "tp": 4709,
          "fn": 2091,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.9789937193627449
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993364583333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9990864583333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9992114583333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9991687499999999
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.988575
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9938718750000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9992526041666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9938307291666668
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": {
          "tp": 776,
          "fn": 24,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9965416666666668
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9972677083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9924083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9948380208333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.9420249999999999
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.971178125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        }
      },
      "auroc": 0.9566015625000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9696463541666668
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9817932291666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 676,
          "fn": 124,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 586,
          "fn": 214,
          "accuracy": 0.7325
        }
      },
      "auroc": 0.9757197916666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9886791666666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9912322916666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9899557291666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9833906250000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9951145833333332
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        }
      },
      "auroc": 0.9892526041666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9860348958333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9931734375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 747,
          "fn": 53,
          "accuracy": 0.93375
        },
        "0.01": {
          "tp": 656,
          "fn": 144,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9896041666666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9970364583333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.990909375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9939729166666668
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.8988239583333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9777489583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        },
        "0.01": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9382864583333335
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9479302083333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9843291666666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 661,
          "fn": 139,
          "accuracy": 0.82625
        },
        "0.01": {
          "tp": 553,
          "fn": 247,
          "accuracy": 0.69125
        }
      },
      "auroc": 0.9661296875000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9946635416666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9695052083333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        }
      },
      "auroc": 0.9820843750000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.8560791666666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9746333333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        },
        "0.01": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        }
      },
      "auroc": 0.91535625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        }
      },
      "auroc": 0.9253713541666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9720692708333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 625,
          "fn": 175,
          "accuracy": 0.78125
        },
        "0.01": {
          "tp": 522,
          "fn": 278,
          "accuracy": 0.6525
        }
      },
      "auroc": 0.9487203125000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9925604166666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9901864583333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9913734375000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9804177083333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.9422979166666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9613578125
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9864890625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.9662421875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 709,
          "fn": 91,
          "accuracy": 0.88625
        },
        "0.01": {
          "tp": 621,
          "fn": 179,
          "accuracy": 0.77625
        }
      },
      "auroc": 0.9763656249999999
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.9111760416666668
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.9111760416666668
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.85565625
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.85565625
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        }
      },
      "auroc": 0.8834161458333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        }
      },
      "auroc": 0.8834161458333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.8973687499999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.8973687499999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.8661583333333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.8661583333333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        }
      },
      "auroc": 0.8817635416666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        }
      },
      "auroc": 0.8817635416666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9990791666666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9990791666666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9981562500000001
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9981562500000001
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9986177083333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9986177083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9966947916666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9966947916666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.9477697916666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.9477697916666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9722322916666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9722322916666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.93191875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.93191875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9092270833333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9092270833333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.9205729166666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.9205729166666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1914,
          "fn": 286,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 1698,
          "fn": 502,
          "accuracy": 0.7718181818181818
        }
      },
      "auroc": 0.9732528409090908
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1130,
          "fn": 70,
          "accuracy": 0.9416666666666667
        },
        "0.01": {
          "tp": 1018,
          "fn": 182,
          "accuracy": 0.8483333333333334
        }
      },
      "auroc": 0.9888880208333334
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3044,
          "fn": 356,
          "accuracy": 0.8952941176470588
        },
        "0.01": {
          "tp": 2716,
          "fn": 684,
          "accuracy": 0.7988235294117647
        }
      },
      "auroc": 0.9787711397058824
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1486,
          "fn": 714,
          "accuracy": 0.6754545454545454
        },
        "0.01": {
          "tp": 1063,
          "fn": 1137,
          "accuracy": 0.48318181818181816
        }
      },
      "auroc": 0.9306248106060606
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1094,
          "fn": 106,
          "accuracy": 0.9116666666666666
        },
        "0.01": {
          "tp": 1002,
          "fn": 198,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9749246527777777
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2580,
          "fn": 820,
          "accuracy": 0.7588235294117647
        },
        "0.01": {
          "tp": 2065,
          "fn": 1335,
          "accuracy": 0.6073529411764705
        }
      },
      "auroc": 0.9462600490196078
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3400,
          "fn": 1000,
          "accuracy": 0.7727272727272727
        },
        "0.01": {
          "tp": 2761,
          "fn": 1639,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.9519388257575758
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2224,
          "fn": 176,
          "accuracy": 0.9266666666666666
        },
        "0.01": {
          "tp": 2020,
          "fn": 380,
          "accuracy": 0.8416666666666667
        }
      },
      "auroc": 0.9819063368055554
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5624,
          "fn": 1176,
          "accuracy": 0.8270588235294117
        },
        "0.01": {
          "tp": 4781,
          "fn": 2019,
          "accuracy": 0.7030882352941177
        }
      },
      "auroc": 0.962515594362745
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9986927083333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9986843750000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9986885416666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9988479166666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9893697916666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9941088541666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9987703125
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9940270833333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        }
      },
      "auroc": 0.9963986979166667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9959604166666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9916562500000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9938083333333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.9510229166666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9711322916666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.9610776041666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.9734916666666668
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9813942708333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 692,
          "fn": 108,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 580,
          "fn": 220,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9774429687499999
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.98940625
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9866104166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.9880083333333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9853020833333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.99391875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9896104166666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.9873541666666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9902645833333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 759,
          "fn": 41,
          "accuracy": 0.94875
        },
        "0.01": {
          "tp": 625,
          "fn": 175,
          "accuracy": 0.78125
        }
      },
      "auroc": 0.988809375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9955364583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9903218750000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        }
      },
      "auroc": 0.9929291666666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.8826489583333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9708124999999999
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9267307291666665
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9390927083333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        }
      },
      "auroc": 0.9805671874999999
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 663,
          "fn": 137,
          "accuracy": 0.82875
        },
        "0.01": {
          "tp": 517,
          "fn": 283,
          "accuracy": 0.64625
        }
      },
      "auroc": 0.9598299479166666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9924958333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9696322916666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9810640625000002
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.8619583333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.971809375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.9168838541666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9272270833333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9707208333333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 642,
          "fn": 158,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 497,
          "fn": 303,
          "accuracy": 0.62125
        }
      },
      "auroc": 0.9489739583333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9903916666666668
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9919218750000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        }
      },
      "auroc": 0.9911567708333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9789520833333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.9331979166666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        }
      },
      "auroc": 0.956075
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9846718750000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9625598958333332
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 717,
          "fn": 83,
          "accuracy": 0.89625
        },
        "0.01": {
          "tp": 589,
          "fn": 211,
          "accuracy": 0.73625
        }
      },
      "auroc": 0.9736158854166668
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.9151656250000001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.9151656250000001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.8619177083333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.8619177083333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        },
        "0.01": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        }
      },
      "auroc": 0.8885416666666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        },
        "0.01": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        }
      },
      "auroc": 0.8885416666666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9216250000000001
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9216250000000001
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.88955
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.88955
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9055875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9055875
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.996765625
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.996765625
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.997490625
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.997490625
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.997128125
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.997128125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.99593125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.99593125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9668135416666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9668135416666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.9813723958333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.9813723958333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9361562499999998
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9361562499999998
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9171937499999999
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9171937499999999
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9266749999999999
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9266749999999999
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1949,
          "fn": 251,
          "accuracy": 0.8859090909090909
        },
        "0.01": {
          "tp": 1594,
          "fn": 606,
          "accuracy": 0.7245454545454545
        }
      },
      "auroc": 0.9752842803030303
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1135,
          "fn": 65,
          "accuracy": 0.9458333333333333
        },
        "0.01": {
          "tp": 1003,
          "fn": 197,
          "accuracy": 0.8358333333333333
        }
      },
      "auroc": 0.9881378472222222
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3084,
          "fn": 316,
          "accuracy": 0.9070588235294118
        },
        "0.01": {
          "tp": 2597,
          "fn": 803,
          "accuracy": 0.7638235294117647
        }
      },
      "auroc": 0.9798208333333334
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1587,
          "fn": 613,
          "accuracy": 0.7213636363636363
        },
        "0.01": {
          "tp": 1115,
          "fn": 1085,
          "accuracy": 0.5068181818181818
        }
      },
      "auroc": 0.9356089015151515
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1084,
          "fn": 116,
          "accuracy": 0.9033333333333333
        },
        "0.01": {
          "tp": 970,
          "fn": 230,
          "accuracy": 0.8083333333333333
        }
      },
      "auroc": 0.9717067708333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2671,
          "fn": 729,
          "accuracy": 0.7855882352941177
        },
        "0.01": {
          "tp": 2085,
          "fn": 1315,
          "accuracy": 0.6132352941176471
        }
      },
      "auroc": 0.9483493259803921
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3536,
          "fn": 864,
          "accuracy": 0.8036363636363636
        },
        "0.01": {
          "tp": 2709,
          "fn": 1691,
          "accuracy": 0.6156818181818182
        }
      },
      "auroc": 0.9554465909090908
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2219,
          "fn": 181,
          "accuracy": 0.9245833333333333
        },
        "0.01": {
          "tp": 1973,
          "fn": 427,
          "accuracy": 0.8220833333333334
        }
      },
      "auroc": 0.9799223090277779
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 5755,
          "fn": 1045,
          "accuracy": 0.8463235294117647
        },
        "0.01": {
          "tp": 4682,
          "fn": 2118,
          "accuracy": 0.6885294117647058
        }
      },
      "auroc": 0.9640850796568627
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.997459375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9979218750000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9976906249999999
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9977114583333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.990003125
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9938572916666668
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9975854166666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9939625000000002
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 722,
          "fn": 78,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9957739583333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9964354166666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9915854166666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9940104166666668
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.96370625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9722427083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        }
      },
      "auroc": 0.9679744791666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        }
      },
      "auroc": 0.9800708333333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.9819140625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 720,
          "fn": 80,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 593,
          "fn": 207,
          "accuracy": 0.74125
        }
      },
      "auroc": 0.9809924479166666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9914875000000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9935927083333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9925401041666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        }
      },
      "auroc": 0.994121875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9956499999999999
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9948859375000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        }
      },
      "auroc": 0.9928046875000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9946213541666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 782,
          "fn": 18,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 603,
          "fn": 197,
          "accuracy": 0.75375
        }
      },
      "auroc": 0.9937130208333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9954979166666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9940895833333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        }
      },
      "auroc": 0.99479375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.9176781250000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9805812500000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.9491296874999999
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        }
      },
      "auroc": 0.9565880208333335
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9873354166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 693,
          "fn": 107,
          "accuracy": 0.86625
        },
        "0.01": {
          "tp": 551,
          "fn": 249,
          "accuracy": 0.68875
        }
      },
      "auroc": 0.9719617187499999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.99475625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9751864583333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9849713541666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.8925000000000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9717125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        }
      },
      "auroc": 0.9321062500000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        },
        "0.01": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9436281250000002
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9734494791666668
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 659,
          "fn": 141,
          "accuracy": 0.82375
        },
        "0.01": {
          "tp": 527,
          "fn": 273,
          "accuracy": 0.65875
        }
      },
      "auroc": 0.9585388020833334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.9919062500000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.99484375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        }
      },
      "auroc": 0.9933750000000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9861947916666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9551875000000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9706911458333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9890505208333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        }
      },
      "auroc": 0.9750156249999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 749,
          "fn": 51,
          "accuracy": 0.93625
        },
        "0.01": {
          "tp": 591,
          "fn": 209,
          "accuracy": 0.73875
        }
      },
      "auroc": 0.9820330729166666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9516833333333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9516833333333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9052541666666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9052541666666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9284687500000001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9284687500000001
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.9549322916666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.9549322916666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.915221875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.915221875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.9350770833333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.9350770833333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9943770833333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9943770833333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.9956322916666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.9956322916666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.9950046875
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.9950046875
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9968927083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9968927083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9804385416666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9804385416666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9886656250000001
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9886656250000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.9613322916666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.9613322916666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9326697916666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9326697916666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        }
      },
      "auroc": 0.9470010416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        }
      },
      "auroc": 0.9470010416666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2046,
          "fn": 154,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 1538,
          "fn": 662,
          "accuracy": 0.6990909090909091
        }
      },
      "auroc": 0.984250946969697
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1155,
          "fn": 45,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 996,
          "fn": 204,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9912032986111111
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3201,
          "fn": 199,
          "accuracy": 0.9414705882352942
        },
        "0.01": {
          "tp": 2534,
          "fn": 866,
          "accuracy": 0.7452941176470588
        }
      },
      "auroc": 0.986704718137255
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1742,
          "fn": 458,
          "accuracy": 0.7918181818181819
        },
        "0.01": {
          "tp": 1222,
          "fn": 978,
          "accuracy": 0.5554545454545454
        }
      },
      "auroc": 0.9528299242424243
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1114,
          "fn": 86,
          "accuracy": 0.9283333333333333
        },
        "0.01": {
          "tp": 979,
          "fn": 221,
          "accuracy": 0.8158333333333333
        }
      },
      "auroc": 0.9775628472222222
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2856,
          "fn": 544,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 2201,
          "fn": 1199,
          "accuracy": 0.6473529411764706
        }
      },
      "auroc": 0.9615591911764706
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3788,
          "fn": 612,
          "accuracy": 0.860909090909091
        },
        "0.01": {
          "tp": 2760,
          "fn": 1640,
          "accuracy": 0.6272727272727273
        }
      },
      "auroc": 0.9685404356060605
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2269,
          "fn": 131,
          "accuracy": 0.9454166666666667
        },
        "0.01": {
          "tp": 1975,
          "fn": 425,
          "accuracy": 0.8229166666666666
        }
      },
      "auroc": 0.9843830729166667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6057,
          "fn": 743,
          "accuracy": 0.8907352941176471
        },
        "0.01": {
          "tp": 4735,
          "fn": 2065,
          "accuracy": 0.6963235294117647
        }
      },
      "auroc": 0.9741319546568628
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9935406250000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.987621875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        }
      },
      "auroc": 0.9905812500000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9939197916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9660770833333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9799984375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9937302083333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.9768494791666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 750,
          "fn": 50,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 629,
          "fn": 171,
          "accuracy": 0.78625
        }
      },
      "auroc": 0.98528984375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9862562500000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.80965
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.8979531249999999
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.91121875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.746375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 70,
          "fn": 330,
          "accuracy": 0.175
        }
      },
      "auroc": 0.8287968750000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9487375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        },
        "0.01": {
          "tp": 72,
          "fn": 328,
          "accuracy": 0.18
        }
      },
      "auroc": 0.7780125000000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 423,
          "fn": 377,
          "accuracy": 0.52875
        },
        "0.01": {
          "tp": 260,
          "fn": 540,
          "accuracy": 0.325
        }
      },
      "auroc": 0.863375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9818625000000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.9147875000000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9483250000000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9716010416666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.7819197916666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.8767604166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        }
      },
      "auroc": 0.9767317708333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 123,
          "fn": 277,
          "accuracy": 0.3075
        }
      },
      "auroc": 0.8483536458333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 561,
          "fn": 239,
          "accuracy": 0.70125
        },
        "0.01": {
          "tp": 398,
          "fn": 402,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.9125427083333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9913364583333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.9652166666666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        }
      },
      "auroc": 0.9782765625000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.9105989583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.8327604166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        }
      },
      "auroc": 0.8716796874999999
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.9509677083333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        }
      },
      "auroc": 0.8989885416666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 542,
          "fn": 258,
          "accuracy": 0.6775
        },
        "0.01": {
          "tp": 373,
          "fn": 427,
          "accuracy": 0.46625
        }
      },
      "auroc": 0.924978125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.98989375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.8741760416666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.9320348958333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.8454697916666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.6816979166666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 281,
          "accuracy": 0.2975
        },
        "0.01": {
          "tp": 44,
          "fn": 356,
          "accuracy": 0.11
        }
      },
      "auroc": 0.7635838541666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.9176817708333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 262,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        }
      },
      "auroc": 0.7779369791666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 402,
          "fn": 398,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 245,
          "fn": 555,
          "accuracy": 0.30625
        }
      },
      "auroc": 0.847809375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9853895833333335
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.9766583333333332
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        }
      },
      "auroc": 0.9810239583333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9683666666666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.862196875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.9152817708333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        }
      },
      "auroc": 0.9768781250000002
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.9194276041666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 615,
          "fn": 185,
          "accuracy": 0.76875
        },
        "0.01": {
          "tp": 450,
          "fn": 350,
          "accuracy": 0.5625
        }
      },
      "auroc": 0.9481528645833333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.901184375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.901184375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.8661
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.8661
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.8836421875
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.8836421875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.9284291666666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.9284291666666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9115208333333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9115208333333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        }
      },
      "auroc": 0.919975
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        }
      },
      "auroc": 0.919975
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9957791666666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9957791666666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.994890625
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.994890625
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9953348958333335
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9953348958333335
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9912864583333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9912864583333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9864864583333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9864864583333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.9888864583333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.9888864583333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9390489583333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9390489583333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.92061875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.92061875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9298338541666668
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9298338541666668
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1881,
          "fn": 319,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 1429,
          "fn": 771,
          "accuracy": 0.6495454545454545
        }
      },
      "auroc": 0.9712733901515151
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 825,
          "fn": 375,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 584,
          "fn": 616,
          "accuracy": 0.4866666666666667
        }
      },
      "auroc": 0.9213517361111111
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2706,
          "fn": 694,
          "accuracy": 0.7958823529411765
        },
        "0.01": {
          "tp": 2013,
          "fn": 1387,
          "accuracy": 0.5920588235294117
        }
      },
      "auroc": 0.9536539828431372
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1567,
          "fn": 633,
          "accuracy": 0.7122727272727273
        },
        "0.01": {
          "tp": 1070,
          "fn": 1130,
          "accuracy": 0.4863636363636364
        }
      },
      "auroc": 0.9346174242424242
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 496,
          "fn": 704,
          "accuracy": 0.41333333333333333
        },
        "0.01": {
          "tp": 309,
          "fn": 891,
          "accuracy": 0.2575
        }
      },
      "auroc": 0.8118378472222223
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2063,
          "fn": 1337,
          "accuracy": 0.606764705882353
        },
        "0.01": {
          "tp": 1379,
          "fn": 2021,
          "accuracy": 0.40558823529411764
        }
      },
      "auroc": 0.8912834558823529
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3448,
          "fn": 952,
          "accuracy": 0.7836363636363637
        },
        "0.01": {
          "tp": 2499,
          "fn": 1901,
          "accuracy": 0.5679545454545455
        }
      },
      "auroc": 0.9529454071969699
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1321,
          "fn": 1079,
          "accuracy": 0.5504166666666667
        },
        "0.01": {
          "tp": 893,
          "fn": 1507,
          "accuracy": 0.3720833333333333
        }
      },
      "auroc": 0.8665947916666666
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4769,
          "fn": 2031,
          "accuracy": 0.7013235294117647
        },
        "0.01": {
          "tp": 3392,
          "fn": 3408,
          "accuracy": 0.4988235294117647
        }
      },
      "auroc": 0.9224687193627451
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9973041666666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9977645833333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.997534375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.997665625
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9905395833333335
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9941026041666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9974848958333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.9941520833333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 709,
          "fn": 91,
          "accuracy": 0.88625
        }
      },
      "auroc": 0.9958184895833334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9966322916666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9922927083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9944625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.966928125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9722010416666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        }
      },
      "auroc": 0.9695645833333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        }
      },
      "auroc": 0.9817802083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.982246875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 737,
          "fn": 63,
          "accuracy": 0.92125
        },
        "0.01": {
          "tp": 605,
          "fn": 195,
          "accuracy": 0.75625
        }
      },
      "auroc": 0.9820135416666668
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9931281249999999
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9938802083333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        }
      },
      "auroc": 0.9935041666666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9939635416666668
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9946854166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9943244791666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        }
      },
      "auroc": 0.9935458333333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.9942828125000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 584,
          "fn": 216,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9939143229166666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9955135416666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9948239583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        }
      },
      "auroc": 0.99516875
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9363572916666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9822520833333335
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.9593046875000002
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        }
      },
      "auroc": 0.9659354166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9885380208333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 708,
          "fn": 92,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 552,
          "fn": 248,
          "accuracy": 0.69
        }
      },
      "auroc": 0.97723671875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.993790625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9773197916666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9855552083333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.8969072916666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9775499999999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9372286458333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9453489583333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9774348958333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 664,
          "fn": 136,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 536,
          "fn": 264,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9613919270833333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9912395833333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9951916666666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        }
      },
      "auroc": 0.993215625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9852104166666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9593989583333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.9723046875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9882250000000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        }
      },
      "auroc": 0.9772953124999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 755,
          "fn": 45,
          "accuracy": 0.94375
        },
        "0.01": {
          "tp": 571,
          "fn": 229,
          "accuracy": 0.71375
        }
      },
      "auroc": 0.9827601562500001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        }
      },
      "auroc": 0.9427864583333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        }
      },
      "auroc": 0.9427864583333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.8990833333333332
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.8990833333333332
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9209348958333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9209348958333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.9526052083333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.9526052083333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.9194458333333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.9194458333333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.9360255208333335
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.9360255208333335
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.99348125
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.99348125
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9947072916666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9947072916666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        }
      },
      "auroc": 0.9940942708333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        }
      },
      "auroc": 0.9940942708333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9966854166666668
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9966854166666668
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9840760416666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9840760416666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.9903807291666668
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.9903807291666668
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9584531249999999
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9584531249999999
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9413687500000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9413687500000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        }
      },
      "auroc": 0.9499109374999999
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        }
      },
      "auroc": 0.9499109374999999
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2037,
          "fn": 163,
          "accuracy": 0.9259090909090909
        },
        "0.01": {
          "tp": 1481,
          "fn": 719,
          "accuracy": 0.6731818181818182
        }
      },
      "auroc": 0.9828745265151515
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1160,
          "fn": 40,
          "accuracy": 0.9666666666666667
        },
        "0.01": {
          "tp": 988,
          "fn": 212,
          "accuracy": 0.8233333333333334
        }
      },
      "auroc": 0.9918788194444444
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 203,
          "accuracy": 0.9402941176470588
        },
        "0.01": {
          "tp": 2469,
          "fn": 931,
          "accuracy": 0.7261764705882353
        }
      },
      "auroc": 0.986052512254902
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1773,
          "fn": 427,
          "accuracy": 0.8059090909090909
        },
        "0.01": {
          "tp": 1209,
          "fn": 991,
          "accuracy": 0.5495454545454546
        }
      },
      "auroc": 0.9559739583333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1124,
          "fn": 76,
          "accuracy": 0.9366666666666666
        },
        "0.01": {
          "tp": 957,
          "fn": 243,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9794378472222222
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2897,
          "fn": 503,
          "accuracy": 0.8520588235294118
        },
        "0.01": {
          "tp": 2166,
          "fn": 1234,
          "accuracy": 0.6370588235294118
        }
      },
      "auroc": 0.964255330882353
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3810,
          "fn": 590,
          "accuracy": 0.865909090909091
        },
        "0.01": {
          "tp": 2690,
          "fn": 1710,
          "accuracy": 0.6113636363636363
        }
      },
      "auroc": 0.9694242424242425
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2284,
          "fn": 116,
          "accuracy": 0.9516666666666667
        },
        "0.01": {
          "tp": 1945,
          "fn": 455,
          "accuracy": 0.8104166666666667
        }
      },
      "auroc": 0.9856583333333334
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6094,
          "fn": 706,
          "accuracy": 0.8961764705882352
        },
        "0.01": {
          "tp": 4635,
          "fn": 2165,
          "accuracy": 0.6816176470588236
        }
      },
      "auroc": 0.9751539215686273
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.996946875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9973739583333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9971604166666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9973239583333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9901843750000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9937541666666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9971354166666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9937791666666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 695,
          "fn": 105,
          "accuracy": 0.86875
        }
      },
      "auroc": 0.9954572916666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9965708333333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9917229166666668
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.994146875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9723385416666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9723520833333335
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9723453125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        }
      },
      "auroc": 0.9844546875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9820375000000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 744,
          "fn": 56,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 607,
          "fn": 193,
          "accuracy": 0.75875
        }
      },
      "auroc": 0.98324609375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9932885416666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9944229166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        }
      },
      "auroc": 0.9938557291666668
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.994821875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9957458333333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9952838541666668
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.9940552083333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.995084375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        },
        "0.01": {
          "tp": 577,
          "fn": 223,
          "accuracy": 0.72125
        }
      },
      "auroc": 0.9945697916666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.995465625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.99635625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        }
      },
      "auroc": 0.9959109375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9425041666666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.982140625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        }
      },
      "auroc": 0.9623223958333335
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        }
      },
      "auroc": 0.9689848958333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9892484375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 712,
          "fn": 88,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 570,
          "fn": 230,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.9791166666666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9953208333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9820739583333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9886973958333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.9136947916666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9761124999999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        }
      },
      "auroc": 0.9449036458333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        }
      },
      "auroc": 0.9545078125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9790932291666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 679,
          "fn": 121,
          "accuracy": 0.84875
        },
        "0.01": {
          "tp": 555,
          "fn": 245,
          "accuracy": 0.69375
        }
      },
      "auroc": 0.9668005208333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9913427083333335
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9954104166666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.9933765625000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9868541666666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9658958333333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        }
      },
      "auroc": 0.976375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        }
      },
      "auroc": 0.9890984375000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9806531250000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 759,
          "fn": 41,
          "accuracy": 0.94875
        },
        "0.01": {
          "tp": 565,
          "fn": 235,
          "accuracy": 0.70625
        }
      },
      "auroc": 0.9848757812500002
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9591489583333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9591489583333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9166833333333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9166833333333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        }
      },
      "auroc": 0.9379161458333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        }
      },
      "auroc": 0.9379161458333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9685979166666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9685979166666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9350520833333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9350520833333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.951825
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.951825
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9930729166666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9930729166666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9944395833333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9944395833333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.99375625
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.99375625
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9966333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9966333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9864020833333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9864020833333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9915177083333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9915177083333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9677864583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9677864583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9501291666666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9501291666666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9589578125
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9589578125
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2082,
          "fn": 118,
          "accuracy": 0.9463636363636364
        },
        "0.01": {
          "tp": 1488,
          "fn": 712,
          "accuracy": 0.6763636363636364
        }
      },
      "auroc": 0.9867431818181818
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1163,
          "fn": 37,
          "accuracy": 0.9691666666666666
        },
        "0.01": {
          "tp": 992,
          "fn": 208,
          "accuracy": 0.8266666666666667
        }
      },
      "auroc": 0.9928934027777778
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3245,
          "fn": 155,
          "accuracy": 0.9544117647058824
        },
        "0.01": {
          "tp": 2480,
          "fn": 920,
          "accuracy": 0.7294117647058823
        }
      },
      "auroc": 0.9889138480392157
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1827,
          "fn": 373,
          "accuracy": 0.8304545454545454
        },
        "0.01": {
          "tp": 1275,
          "fn": 925,
          "accuracy": 0.5795454545454546
        }
      },
      "auroc": 0.9627494318181817
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1126,
          "fn": 74,
          "accuracy": 0.9383333333333334
        },
        "0.01": {
          "tp": 963,
          "fn": 237,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9804052083333334
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2953,
          "fn": 447,
          "accuracy": 0.8685294117647059
        },
        "0.01": {
          "tp": 2238,
          "fn": 1162,
          "accuracy": 0.658235294117647
        }
      },
      "auroc": 0.9689808823529411
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3909,
          "fn": 491,
          "accuracy": 0.8884090909090909
        },
        "0.01": {
          "tp": 2763,
          "fn": 1637,
          "accuracy": 0.6279545454545454
        }
      },
      "auroc": 0.9747463068181818
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2289,
          "fn": 111,
          "accuracy": 0.95375
        },
        "0.01": {
          "tp": 1955,
          "fn": 445,
          "accuracy": 0.8145833333333333
        }
      },
      "auroc": 0.9866493055555555
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6198,
          "fn": 602,
          "accuracy": 0.9114705882352941
        },
        "0.01": {
          "tp": 4718,
          "fn": 2082,
          "accuracy": 0.6938235294117647
        }
      },
      "auroc": 0.9789473651960785
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8455010416666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.683109375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7643052083333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8270947916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6198270833333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7234609375000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8362979166666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6514682291666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 789,
          "accuracy": 0.01375
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7438830729166667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9659614583333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9901135416666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9780375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6538843750000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.964884375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 229,
          "accuracy": 0.4275
        },
        "0.01": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        }
      },
      "auroc": 0.8093843749999999
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        },
        "0.01": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        }
      },
      "auroc": 0.8099229166666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9774989583333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 508,
          "fn": 292,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 435,
          "fn": 365,
          "accuracy": 0.54375
        }
      },
      "auroc": 0.8937109375000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5771104166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.5275166666666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 363,
          "accuracy": 0.0925
        },
        "0.01": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        }
      },
      "auroc": 0.5523135416666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5581135416666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.7756416666666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 306,
          "accuracy": 0.235
        },
        "0.01": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        }
      },
      "auroc": 0.6668776041666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5676119791666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        },
        "0.01": {
          "tp": 88,
          "fn": 312,
          "accuracy": 0.22
        }
      },
      "auroc": 0.6515791666666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 669,
          "accuracy": 0.16375
        },
        "0.01": {
          "tp": 88,
          "fn": 712,
          "accuracy": 0.11
        }
      },
      "auroc": 0.6095955729166667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9962260416666668
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.69565
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        },
        "0.01": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        }
      },
      "auroc": 0.8459380208333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6153989583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.413759375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5145791666666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        }
      },
      "auroc": 0.8058124999999999
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        },
        "0.01": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        }
      },
      "auroc": 0.5547046875
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 222,
          "fn": 578,
          "accuracy": 0.2775
        },
        "0.01": {
          "tp": 189,
          "fn": 611,
          "accuracy": 0.23625
        }
      },
      "auroc": 0.6802585937500001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9525333333333332
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.6127947916666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 261,
          "accuracy": 0.3475
        },
        "0.01": {
          "tp": 88,
          "fn": 312,
          "accuracy": 0.22
        }
      },
      "auroc": 0.7826640625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6019916666666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.608465625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 362,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 21,
          "fn": 379,
          "accuracy": 0.0525
        }
      },
      "auroc": 0.6052286458333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        }
      },
      "auroc": 0.7772625000000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.6106302083333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 623,
          "accuracy": 0.22125
        },
        "0.01": {
          "tp": 109,
          "fn": 691,
          "accuracy": 0.13625
        }
      },
      "auroc": 0.6939463541666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7790052083333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4624083333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.6207067708333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.66315625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.34971145833333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.5064338541666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7210807291666668
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.40605989583333335
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 775,
          "accuracy": 0.03125
        },
        "0.01": {
          "tp": 2,
          "fn": 798,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.5635703125
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.5862416666666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.5862416666666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.5394270833333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.5394270833333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.5628343750000001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.5628343750000001
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6072562499999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6072562499999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5838447916666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5838447916666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5955505208333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5955505208333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6788479166666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6788479166666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5996958333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5996958333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.639271875
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.639271875
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7273572916666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7273572916666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5254895833333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5254895833333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6264234375000001
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6264234375000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6408270833333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6408270833333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6136104166666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6136104166666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6272187499999999
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6272187499999999
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 519,
          "fn": 1681,
          "accuracy": 0.2359090909090909
        },
        "0.01": {
          "tp": 363,
          "fn": 1837,
          "accuracy": 0.165
        }
      },
      "auroc": 0.759715246212121
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 260,
          "fn": 940,
          "accuracy": 0.21666666666666667
        },
        "0.01": {
          "tp": 223,
          "fn": 977,
          "accuracy": 0.18583333333333332
        }
      },
      "auroc": 0.6619321180555555
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 779,
          "fn": 2621,
          "accuracy": 0.22911764705882354
        },
        "0.01": {
          "tp": 586,
          "fn": 2814,
          "accuracy": 0.1723529411764706
        }
      },
      "auroc": 0.7252035539215685
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 2178,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 3,
          "fn": 2197,
          "accuracy": 0.0013636363636363637
        }
      },
      "auroc": 0.6165188446969697
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 900,
          "accuracy": 0.25
        },
        "0.01": {
          "tp": 240,
          "fn": 960,
          "accuracy": 0.2
        }
      },
      "auroc": 0.6220482638888889
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 3078,
          "accuracy": 0.09470588235294118
        },
        "0.01": {
          "tp": 243,
          "fn": 3157,
          "accuracy": 0.07147058823529412
        }
      },
      "auroc": 0.6184704044117646
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 541,
          "fn": 3859,
          "accuracy": 0.12295454545454546
        },
        "0.01": {
          "tp": 366,
          "fn": 4034,
          "accuracy": 0.08318181818181818
        }
      },
      "auroc": 0.6881170454545454
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 560,
          "fn": 1840,
          "accuracy": 0.23333333333333334
        },
        "0.01": {
          "tp": 463,
          "fn": 1937,
          "accuracy": 0.19291666666666665
        }
      },
      "auroc": 0.6419901909722223
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1101,
          "fn": 5699,
          "accuracy": 0.16191176470588237
        },
        "0.01": {
          "tp": 829,
          "fn": 5971,
          "accuracy": 0.12191176470588236
        }
      },
      "auroc": 0.6718369791666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.998209375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9982833333333332
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9982463541666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9984510416666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.990675
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9945630208333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9983302083333332
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9944791666666668
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": {
          "tp": 748,
          "fn": 52,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9964046875000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.996221875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9917791666666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9940005208333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9425291666666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9723614583333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.9574453125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.9693755208333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9820703125000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 681,
          "fn": 119,
          "accuracy": 0.85125
        },
        "0.01": {
          "tp": 571,
          "fn": 229,
          "accuracy": 0.71375
        }
      },
      "auroc": 0.9757229166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9892927083333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9934135416666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9913531250000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9862364583333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.995915625
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9910760416666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9877645833333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9946645833333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 604,
          "fn": 196,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9912145833333332
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.995896875
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9910958333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9934963541666668
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.8626947916666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.979978125
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        }
      },
      "auroc": 0.9213364583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.9292958333333332
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        }
      },
      "auroc": 0.9855369791666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 659,
          "fn": 141,
          "accuracy": 0.82375
        },
        "0.01": {
          "tp": 508,
          "fn": 292,
          "accuracy": 0.635
        }
      },
      "auroc": 0.95741640625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9943468750000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9777104166666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9860286458333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.8315458333333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9795604166666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.9055531250000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.9129463541666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9786354166666668
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 635,
          "fn": 165,
          "accuracy": 0.79375
        },
        "0.01": {
          "tp": 515,
          "fn": 285,
          "accuracy": 0.64375
        }
      },
      "auroc": 0.9457908854166667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9913562499999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9923677083333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9918619791666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.9763927083333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9590395833333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        }
      },
      "auroc": 0.9677161458333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9838744791666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9757036458333332
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 741,
          "fn": 59,
          "accuracy": 0.92625
        },
        "0.01": {
          "tp": 584,
          "fn": 216,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9797890624999999
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.8967583333333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.8967583333333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.8264624999999999
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.8264624999999999
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        },
        "0.01": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        }
      },
      "auroc": 0.8616104166666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        },
        "0.01": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        }
      },
      "auroc": 0.8616104166666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.8722260416666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.8722260416666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.8573927083333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.8573927083333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        }
      },
      "auroc": 0.8648093750000001
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        }
      },
      "auroc": 0.8648093750000001
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9953822916666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9953822916666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9956572916666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9956572916666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        }
      },
      "auroc": 0.9955197916666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        }
      },
      "auroc": 0.9955197916666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.995940625
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.995940625
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9508885416666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9508885416666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        }
      },
      "auroc": 0.9734145833333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        }
      },
      "auroc": 0.9734145833333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9244364583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9244364583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.8856697916666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.8856697916666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        }
      },
      "auroc": 0.905053125
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        }
      },
      "auroc": 0.905053125
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1913,
          "fn": 287,
          "accuracy": 0.8695454545454545
        },
        "0.01": {
          "tp": 1498,
          "fn": 702,
          "accuracy": 0.6809090909090909
        }
      },
      "auroc": 0.9681879734848485
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1154,
          "fn": 46,
          "accuracy": 0.9616666666666667
        },
        "0.01": {
          "tp": 990,
          "fn": 210,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9907750000000001
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3067,
          "fn": 333,
          "accuracy": 0.9020588235294118
        },
        "0.01": {
          "tp": 2488,
          "fn": 912,
          "accuracy": 0.731764705882353
        }
      },
      "auroc": 0.9761598651960786
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1488,
          "fn": 712,
          "accuracy": 0.6763636363636364
        },
        "0.01": {
          "tp": 1017,
          "fn": 1183,
          "accuracy": 0.4622727272727273
        }
      },
      "auroc": 0.9194473484848485
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1122,
          "fn": 78,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 965,
          "fn": 235,
          "accuracy": 0.8041666666666667
        }
      },
      "auroc": 0.9795883680555555
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2610,
          "fn": 790,
          "accuracy": 0.7676470588235295
        },
        "0.01": {
          "tp": 1982,
          "fn": 1418,
          "accuracy": 0.5829411764705882
        }
      },
      "auroc": 0.9406735906862744
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3401,
          "fn": 999,
          "accuracy": 0.7729545454545454
        },
        "0.01": {
          "tp": 2515,
          "fn": 1885,
          "accuracy": 0.571590909090909
        }
      },
      "auroc": 0.9438176609848485
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2276,
          "fn": 124,
          "accuracy": 0.9483333333333334
        },
        "0.01": {
          "tp": 1955,
          "fn": 445,
          "accuracy": 0.8145833333333333
        }
      },
      "auroc": 0.9851816840277777
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5677,
          "fn": 1123,
          "accuracy": 0.8348529411764706
        },
        "0.01": {
          "tp": 4470,
          "fn": 2330,
          "accuracy": 0.6573529411764706
        }
      },
      "auroc": 0.9584167279411765
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9972822916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.997775
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9975286458333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9976729166666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9901843749999999
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9939286458333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9974776041666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9939796875000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 721,
          "fn": 79,
          "accuracy": 0.90125
        }
      },
      "auroc": 0.9957286458333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9963270833333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9917989583333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9940630208333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9697583333333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9723770833333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9710677083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9830427083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9820880208333335
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 735,
          "fn": 65,
          "accuracy": 0.91875
        },
        "0.01": {
          "tp": 602,
          "fn": 198,
          "accuracy": 0.7525
        }
      },
      "auroc": 0.9825653645833333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.992309375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.994359375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.993334375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9938687500000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.995746875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9948078125
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.9930890625000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.995053125
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": {
          "tp": 595,
          "fn": 205,
          "accuracy": 0.74375
        }
      },
      "auroc": 0.99407109375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9954989583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9955322916666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.995515625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9376166666666668
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9808458333333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        }
      },
      "auroc": 0.95923125
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        }
      },
      "auroc": 0.9665578125000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9881890625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 710,
          "fn": 90,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 558,
          "fn": 242,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9773734374999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9950687500000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9812041666666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9881364583333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9044645833333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9767197916666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.9405921875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.9497666666666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9789619791666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 675,
          "fn": 125,
          "accuracy": 0.84375
        },
        "0.01": {
          "tp": 546,
          "fn": 254,
          "accuracy": 0.6825
        }
      },
      "auroc": 0.9643643229166666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9917458333333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.99516875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9934572916666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.985928125
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9611791666666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9735536458333335
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9888369791666668
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9781739583333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 754,
          "fn": 46,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 588,
          "fn": 212,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9835054687500002
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9507104166666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9507104166666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.9053520833333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.9053520833333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        }
      },
      "auroc": 0.9280312500000001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        }
      },
      "auroc": 0.9280312500000001
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.963209375
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.963209375
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        }
      },
      "auroc": 0.9300843750000001
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        }
      },
      "auroc": 0.9300843750000001
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9466468749999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9466468749999999
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9939489583333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9939489583333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.995309375
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.995309375
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.9946291666666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.9946291666666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9969583333333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9969583333333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.979171875
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.979171875
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        }
      },
      "auroc": 0.9880651041666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        }
      },
      "auroc": 0.9880651041666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9657250000000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9657250000000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.9453666666666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.9453666666666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9555458333333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9555458333333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2058,
          "fn": 142,
          "accuracy": 0.9354545454545454
        },
        "0.01": {
          "tp": 1533,
          "fn": 667,
          "accuracy": 0.6968181818181818
        }
      },
      "auroc": 0.9853440340909091
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1159,
          "fn": 41,
          "accuracy": 0.9658333333333333
        },
        "0.01": {
          "tp": 1003,
          "fn": 197,
          "accuracy": 0.8358333333333333
        }
      },
      "auroc": 0.9926397569444445
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3217,
          "fn": 183,
          "accuracy": 0.9461764705882353
        },
        "0.01": {
          "tp": 2536,
          "fn": 864,
          "accuracy": 0.7458823529411764
        }
      },
      "auroc": 0.9879189950980394
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1798,
          "fn": 402,
          "accuracy": 0.8172727272727273
        },
        "0.01": {
          "tp": 1273,
          "fn": 927,
          "accuracy": 0.5786363636363636
        }
      },
      "auroc": 0.9585994318181817
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1121,
          "fn": 79,
          "accuracy": 0.9341666666666667
        },
        "0.01": {
          "tp": 965,
          "fn": 235,
          "accuracy": 0.8041666666666667
        }
      },
      "auroc": 0.9795088541666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2919,
          "fn": 481,
          "accuracy": 0.8585294117647059
        },
        "0.01": {
          "tp": 2238,
          "fn": 1162,
          "accuracy": 0.658235294117647
        }
      },
      "auroc": 0.9659792279411765
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3856,
          "fn": 544,
          "accuracy": 0.8763636363636363
        },
        "0.01": {
          "tp": 2806,
          "fn": 1594,
          "accuracy": 0.6377272727272727
        }
      },
      "auroc": 0.9719717329545454
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2280,
          "fn": 120,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 1968,
          "fn": 432,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9860743055555555
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6136,
          "fn": 664,
          "accuracy": 0.9023529411764706
        },
        "0.01": {
          "tp": 4774,
          "fn": 2026,
          "accuracy": 0.7020588235294117
        }
      },
      "auroc": 0.976949111519608
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7493583333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7418052083333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7455817708333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.744365625
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7344416666666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7394036458333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7468619791666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7381234375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7424927083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7412260416666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.803259375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7722427083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.745865625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.80235
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7741078124999999
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7435458333333335
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8028046875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 798,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7731752604166666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7352479166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7431114583333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7391796875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7321010416666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7642291666666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7481651041666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7336744791666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7536703125
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7436723958333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7906708333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7619625000000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7763166666666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7835270833333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7900041666666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.786765625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7870989583333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7759833333333332
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 799,
          "accuracy": 0.00125
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7815411458333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.720715625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7484333333333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7345744791666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7479479166666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8024447916666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7751963541666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7343317708333335
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7754390624999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 799,
          "accuracy": 0.00125
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7548854166666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7218302083333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.715771875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7188010416666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7368916666666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7326427083333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7347671874999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7293609375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7242072916666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7267841145833333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7113072916666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7113072916666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.709
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.709
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7101536458333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7101536458333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.717671875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.717671875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7300031250000001
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7300031250000001
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7238374999999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7238374999999999
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7373520833333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7373520833333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7338958333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7338958333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7356239583333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7356239583333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.726453125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.726453125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7438239583333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7438239583333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7351385416666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7351385416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7341708333333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7341708333333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.73239375
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.73239375
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7332822916666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7332822916666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 2196,
          "accuracy": 0.0018181818181818182
        },
        "0.01": {
          "tp": 0,
          "fn": 2200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.735091287878788
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.752390625
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 3396,
          "accuracy": 0.001176470588235294
        },
        "0.01": {
          "tp": 0,
          "fn": 3400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7411969362745098
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 2200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 2200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7399832386363636
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7710187500000001
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 3400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 3400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7509369485294117
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 4396,
          "accuracy": 0.0009090909090909091
        },
        "0.01": {
          "tp": 0,
          "fn": 4400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7375372632575756
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 2400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 2400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7617046874999999
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 6796,
          "accuracy": 0.000588235294117647
        },
        "0.01": {
          "tp": 0,
          "fn": 6800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7460669424019608
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2003,
          "fn": 397,
          "accuracy": 0.8345833333333333
        },
        "0.01": {
          "tp": 1789,
          "fn": 611,
          "accuracy": 0.7454166666666666
        }
      },
      "auroc": 0.9639604166666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1987,
          "fn": 413,
          "accuracy": 0.8279166666666666
        },
        "0.01": {
          "tp": 1823,
          "fn": 577,
          "accuracy": 0.7595833333333334
        }
      },
      "auroc": 0.9495144965277778
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3990,
          "fn": 810,
          "accuracy": 0.83125
        },
        "0.01": {
          "tp": 3612,
          "fn": 1188,
          "accuracy": 0.7525
        }
      },
      "auroc": 0.9567374565972222
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1998,
          "fn": 402,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 1815,
          "fn": 585,
          "accuracy": 0.75625
        }
      },
      "auroc": 0.9622391493055557
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1937,
          "fn": 463,
          "accuracy": 0.8070833333333334
        },
        "0.01": {
          "tp": 1738,
          "fn": 662,
          "accuracy": 0.7241666666666666
        }
      },
      "auroc": 0.9358550347222223
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3935,
          "fn": 865,
          "accuracy": 0.8197916666666667
        },
        "0.01": {
          "tp": 3553,
          "fn": 1247,
          "accuracy": 0.7402083333333334
        }
      },
      "auroc": 0.9490470920138888
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4001,
          "fn": 799,
          "accuracy": 0.8335416666666666
        },
        "0.01": {
          "tp": 3604,
          "fn": 1196,
          "accuracy": 0.7508333333333334
        }
      },
      "auroc": 0.9630997829861111
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3924,
          "fn": 876,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 3561,
          "fn": 1239,
          "accuracy": 0.741875
        }
      },
      "auroc": 0.942684765625
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7925,
          "fn": 1675,
          "accuracy": 0.8255208333333334
        },
        "0.01": {
          "tp": 7165,
          "fn": 2435,
          "accuracy": 0.7463541666666667
        }
      },
      "auroc": 0.9528922743055555
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2113,
          "fn": 287,
          "accuracy": 0.8804166666666666
        },
        "0.01": {
          "tp": 1745,
          "fn": 655,
          "accuracy": 0.7270833333333333
        }
      },
      "auroc": 0.971833420138889
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2025,
          "fn": 375,
          "accuracy": 0.84375
        },
        "0.01": {
          "tp": 1926,
          "fn": 474,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.96081875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4138,
          "fn": 662,
          "accuracy": 0.8620833333333333
        },
        "0.01": {
          "tp": 3671,
          "fn": 1129,
          "accuracy": 0.7647916666666666
        }
      },
      "auroc": 0.9663260850694445
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1405,
          "fn": 995,
          "accuracy": 0.5854166666666667
        },
        "0.01": {
          "tp": 682,
          "fn": 1718,
          "accuracy": 0.2841666666666667
        }
      },
      "auroc": 0.9137322916666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1867,
          "fn": 533,
          "accuracy": 0.7779166666666667
        },
        "0.01": {
          "tp": 1706,
          "fn": 694,
          "accuracy": 0.7108333333333333
        }
      },
      "auroc": 0.9385147569444444
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3272,
          "fn": 1528,
          "accuracy": 0.6816666666666666
        },
        "0.01": {
          "tp": 2388,
          "fn": 2412,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.9261235243055554
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3518,
          "fn": 1282,
          "accuracy": 0.7329166666666667
        },
        "0.01": {
          "tp": 2427,
          "fn": 2373,
          "accuracy": 0.505625
        }
      },
      "auroc": 0.9427828559027778
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3892,
          "fn": 908,
          "accuracy": 0.8108333333333333
        },
        "0.01": {
          "tp": 3632,
          "fn": 1168,
          "accuracy": 0.7566666666666667
        }
      },
      "auroc": 0.9496667534722221
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7410,
          "fn": 2190,
          "accuracy": 0.771875
        },
        "0.01": {
          "tp": 6059,
          "fn": 3541,
          "accuracy": 0.6311458333333333
        }
      },
      "auroc": 0.9462248046875001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1931,
          "fn": 469,
          "accuracy": 0.8045833333333333
        },
        "0.01": {
          "tp": 1379,
          "fn": 1021,
          "accuracy": 0.5745833333333333
        }
      },
      "auroc": 0.9348657986111112
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1911,
          "fn": 489,
          "accuracy": 0.79625
        },
        "0.01": {
          "tp": 1409,
          "fn": 991,
          "accuracy": 0.5870833333333333
        }
      },
      "auroc": 0.9268468750000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3842,
          "fn": 958,
          "accuracy": 0.8004166666666667
        },
        "0.01": {
          "tp": 2788,
          "fn": 2012,
          "accuracy": 0.5808333333333333
        }
      },
      "auroc": 0.9308563368055556
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1906,
          "fn": 494,
          "accuracy": 0.7941666666666667
        },
        "0.01": {
          "tp": 1437,
          "fn": 963,
          "accuracy": 0.59875
        }
      },
      "auroc": 0.9319303819444444
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1940,
          "fn": 460,
          "accuracy": 0.8083333333333333
        },
        "0.01": {
          "tp": 1649,
          "fn": 751,
          "accuracy": 0.6870833333333334
        }
      },
      "auroc": 0.9400000868055556
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3846,
          "fn": 954,
          "accuracy": 0.80125
        },
        "0.01": {
          "tp": 3086,
          "fn": 1714,
          "accuracy": 0.6429166666666667
        }
      },
      "auroc": 0.935965234375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3837,
          "fn": 963,
          "accuracy": 0.799375
        },
        "0.01": {
          "tp": 2816,
          "fn": 1984,
          "accuracy": 0.5866666666666667
        }
      },
      "auroc": 0.9333980902777778
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3851,
          "fn": 949,
          "accuracy": 0.8022916666666666
        },
        "0.01": {
          "tp": 3058,
          "fn": 1742,
          "accuracy": 0.6370833333333333
        }
      },
      "auroc": 0.9334234809027777
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7688,
          "fn": 1912,
          "accuracy": 0.8008333333333333
        },
        "0.01": {
          "tp": 5874,
          "fn": 3726,
          "accuracy": 0.611875
        }
      },
      "auroc": 0.9334107855902778
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2172,
          "fn": 228,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 1803,
          "fn": 597,
          "accuracy": 0.75125
        }
      },
      "auroc": 0.9783008680555556
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1929,
          "fn": 471,
          "accuracy": 0.80375
        },
        "0.01": {
          "tp": 1623,
          "fn": 777,
          "accuracy": 0.67625
        }
      },
      "auroc": 0.9473788194444445
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4101,
          "fn": 699,
          "accuracy": 0.854375
        },
        "0.01": {
          "tp": 3426,
          "fn": 1374,
          "accuracy": 0.71375
        }
      },
      "auroc": 0.9628398437500001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1134,
          "fn": 1266,
          "accuracy": 0.4725
        },
        "0.01": {
          "tp": 647,
          "fn": 1753,
          "accuracy": 0.26958333333333334
        }
      },
      "auroc": 0.8811558159722223
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1762,
          "fn": 638,
          "accuracy": 0.7341666666666666
        },
        "0.01": {
          "tp": 1432,
          "fn": 968,
          "accuracy": 0.5966666666666667
        }
      },
      "auroc": 0.9045989583333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2896,
          "fn": 1904,
          "accuracy": 0.6033333333333334
        },
        "0.01": {
          "tp": 2079,
          "fn": 2721,
          "accuracy": 0.433125
        }
      },
      "auroc": 0.8928773871527778
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3306,
          "fn": 1494,
          "accuracy": 0.68875
        },
        "0.01": {
          "tp": 2450,
          "fn": 2350,
          "accuracy": 0.5104166666666666
        }
      },
      "auroc": 0.929728342013889
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3691,
          "fn": 1109,
          "accuracy": 0.7689583333333333
        },
        "0.01": {
          "tp": 3055,
          "fn": 1745,
          "accuracy": 0.6364583333333333
        }
      },
      "auroc": 0.9259888888888889
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6997,
          "fn": 2603,
          "accuracy": 0.7288541666666667
        },
        "0.01": {
          "tp": 5505,
          "fn": 4095,
          "accuracy": 0.5734375
        }
      },
      "auroc": 0.9278586154513889
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2068,
          "fn": 332,
          "accuracy": 0.8616666666666667
        },
        "0.01": {
          "tp": 1741,
          "fn": 659,
          "accuracy": 0.7254166666666667
        }
      },
      "auroc": 0.9678522569444444
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1688,
          "fn": 712,
          "accuracy": 0.7033333333333334
        },
        "0.01": {
          "tp": 1401,
          "fn": 999,
          "accuracy": 0.58375
        }
      },
      "auroc": 0.9193037326388889
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3756,
          "fn": 1044,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 3142,
          "fn": 1658,
          "accuracy": 0.6545833333333333
        }
      },
      "auroc": 0.9435779947916666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 999,
          "fn": 1401,
          "accuracy": 0.41625
        },
        "0.01": {
          "tp": 483,
          "fn": 1917,
          "accuracy": 0.20125
        }
      },
      "auroc": 0.8482094618055555
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1766,
          "fn": 634,
          "accuracy": 0.7358333333333333
        },
        "0.01": {
          "tp": 1529,
          "fn": 871,
          "accuracy": 0.6370833333333333
        }
      },
      "auroc": 0.9062690972222222
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2765,
          "fn": 2035,
          "accuracy": 0.5760416666666667
        },
        "0.01": {
          "tp": 2012,
          "fn": 2788,
          "accuracy": 0.4191666666666667
        }
      },
      "auroc": 0.8772392795138889
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3067,
          "fn": 1733,
          "accuracy": 0.6389583333333333
        },
        "0.01": {
          "tp": 2224,
          "fn": 2576,
          "accuracy": 0.4633333333333333
        }
      },
      "auroc": 0.9080308593749999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3454,
          "fn": 1346,
          "accuracy": 0.7195833333333334
        },
        "0.01": {
          "tp": 2930,
          "fn": 1870,
          "accuracy": 0.6104166666666667
        }
      },
      "auroc": 0.9127864149305555
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6521,
          "fn": 3079,
          "accuracy": 0.6792708333333334
        },
        "0.01": {
          "tp": 5154,
          "fn": 4446,
          "accuracy": 0.536875
        }
      },
      "auroc": 0.9104086371527778
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1977,
          "fn": 423,
          "accuracy": 0.82375
        },
        "0.01": {
          "tp": 1416,
          "fn": 984,
          "accuracy": 0.59
        }
      },
      "auroc": 0.9507877604166668
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1929,
          "fn": 471,
          "accuracy": 0.80375
        },
        "0.01": {
          "tp": 1587,
          "fn": 813,
          "accuracy": 0.66125
        }
      },
      "auroc": 0.9250624999999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3906,
          "fn": 894,
          "accuracy": 0.81375
        },
        "0.01": {
          "tp": 3003,
          "fn": 1797,
          "accuracy": 0.625625
        }
      },
      "auroc": 0.9379251302083333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1873,
          "fn": 527,
          "accuracy": 0.7804166666666666
        },
        "0.01": {
          "tp": 1482,
          "fn": 918,
          "accuracy": 0.6175
        }
      },
      "auroc": 0.9351727430555555
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1563,
          "fn": 837,
          "accuracy": 0.65125
        },
        "0.01": {
          "tp": 1206,
          "fn": 1194,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.8794373263888889
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3436,
          "fn": 1364,
          "accuracy": 0.7158333333333333
        },
        "0.01": {
          "tp": 2688,
          "fn": 2112,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9073050347222222
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3850,
          "fn": 950,
          "accuracy": 0.8020833333333334
        },
        "0.01": {
          "tp": 2898,
          "fn": 1902,
          "accuracy": 0.60375
        }
      },
      "auroc": 0.942980251736111
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3492,
          "fn": 1308,
          "accuracy": 0.7275
        },
        "0.01": {
          "tp": 2793,
          "fn": 2007,
          "accuracy": 0.581875
        }
      },
      "auroc": 0.9022499131944445
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7342,
          "fn": 2258,
          "accuracy": 0.7647916666666666
        },
        "0.01": {
          "tp": 5691,
          "fn": 3909,
          "accuracy": 0.5928125
        }
      },
      "auroc": 0.9226150824652778
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1339,
          "fn": 1061,
          "accuracy": 0.5579166666666666
        },
        "0.01": {
          "tp": 889,
          "fn": 1511,
          "accuracy": 0.37041666666666667
        }
      },
      "auroc": 0.8870383680555556
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1339,
          "fn": 1061,
          "accuracy": 0.5579166666666666
        },
        "0.01": {
          "tp": 889,
          "fn": 1511,
          "accuracy": 0.37041666666666667
        }
      },
      "auroc": 0.8870383680555556
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1139,
          "fn": 1261,
          "accuracy": 0.47458333333333336
        },
        "0.01": {
          "tp": 738,
          "fn": 1662,
          "accuracy": 0.3075
        }
      },
      "auroc": 0.8431900173611111
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1139,
          "fn": 1261,
          "accuracy": 0.47458333333333336
        },
        "0.01": {
          "tp": 738,
          "fn": 1662,
          "accuracy": 0.3075
        }
      },
      "auroc": 0.8431900173611111
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2478,
          "fn": 2322,
          "accuracy": 0.51625
        },
        "0.01": {
          "tp": 1627,
          "fn": 3173,
          "accuracy": 0.3389583333333333
        }
      },
      "auroc": 0.8651141927083333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2478,
          "fn": 2322,
          "accuracy": 0.51625
        },
        "0.01": {
          "tp": 1627,
          "fn": 3173,
          "accuracy": 0.3389583333333333
        }
      },
      "auroc": 0.8651141927083333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1374,
          "fn": 1026,
          "accuracy": 0.5725
        },
        "0.01": {
          "tp": 913,
          "fn": 1487,
          "accuracy": 0.3804166666666667
        }
      },
      "auroc": 0.8934297743055556
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1374,
          "fn": 1026,
          "accuracy": 0.5725
        },
        "0.01": {
          "tp": 913,
          "fn": 1487,
          "accuracy": 0.3804166666666667
        }
      },
      "auroc": 0.8934297743055556
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1265,
          "fn": 1135,
          "accuracy": 0.5270833333333333
        },
        "0.01": {
          "tp": 793,
          "fn": 1607,
          "accuracy": 0.3304166666666667
        }
      },
      "auroc": 0.8673551215277778
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1265,
          "fn": 1135,
          "accuracy": 0.5270833333333333
        },
        "0.01": {
          "tp": 793,
          "fn": 1607,
          "accuracy": 0.3304166666666667
        }
      },
      "auroc": 0.8673551215277778
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2639,
          "fn": 2161,
          "accuracy": 0.5497916666666667
        },
        "0.01": {
          "tp": 1706,
          "fn": 3094,
          "accuracy": 0.35541666666666666
        }
      },
      "auroc": 0.8803924479166666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2639,
          "fn": 2161,
          "accuracy": 0.5497916666666667
        },
        "0.01": {
          "tp": 1706,
          "fn": 3094,
          "accuracy": 0.35541666666666666
        }
      },
      "auroc": 0.8803924479166666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1999,
          "fn": 401,
          "accuracy": 0.8329166666666666
        },
        "0.01": {
          "tp": 1107,
          "fn": 1293,
          "accuracy": 0.46125
        }
      },
      "auroc": 0.9470193576388888
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1999,
          "fn": 401,
          "accuracy": 0.8329166666666666
        },
        "0.01": {
          "tp": 1107,
          "fn": 1293,
          "accuracy": 0.46125
        }
      },
      "auroc": 0.9470193576388888
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1994,
          "fn": 406,
          "accuracy": 0.8308333333333333
        },
        "0.01": {
          "tp": 1343,
          "fn": 1057,
          "accuracy": 0.5595833333333333
        }
      },
      "auroc": 0.9407295138888889
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1994,
          "fn": 406,
          "accuracy": 0.8308333333333333
        },
        "0.01": {
          "tp": 1343,
          "fn": 1057,
          "accuracy": 0.5595833333333333
        }
      },
      "auroc": 0.9407295138888889
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3993,
          "fn": 807,
          "accuracy": 0.831875
        },
        "0.01": {
          "tp": 2450,
          "fn": 2350,
          "accuracy": 0.5104166666666666
        }
      },
      "auroc": 0.9438744357638889
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3993,
          "fn": 807,
          "accuracy": 0.831875
        },
        "0.01": {
          "tp": 2450,
          "fn": 2350,
          "accuracy": 0.5104166666666666
        }
      },
      "auroc": 0.9438744357638889
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1977,
          "fn": 423,
          "accuracy": 0.82375
        },
        "0.01": {
          "tp": 1670,
          "fn": 730,
          "accuracy": 0.6958333333333333
        }
      },
      "auroc": 0.9511750000000001
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1977,
          "fn": 423,
          "accuracy": 0.82375
        },
        "0.01": {
          "tp": 1670,
          "fn": 730,
          "accuracy": 0.6958333333333333
        }
      },
      "auroc": 0.9511750000000001
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1728,
          "fn": 672,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 1341,
          "fn": 1059,
          "accuracy": 0.55875
        }
      },
      "auroc": 0.9186803819444445
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1728,
          "fn": 672,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 1341,
          "fn": 1059,
          "accuracy": 0.55875
        }
      },
      "auroc": 0.9186803819444445
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3705,
          "fn": 1095,
          "accuracy": 0.771875
        },
        "0.01": {
          "tp": 3011,
          "fn": 1789,
          "accuracy": 0.6272916666666667
        }
      },
      "auroc": 0.9349276909722222
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3705,
          "fn": 1095,
          "accuracy": 0.771875
        },
        "0.01": {
          "tp": 3011,
          "fn": 1789,
          "accuracy": 0.6272916666666667
        }
      },
      "auroc": 0.9349276909722222
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1614,
          "fn": 786,
          "accuracy": 0.6725
        },
        "0.01": {
          "tp": 1146,
          "fn": 1254,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.90795234375
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1614,
          "fn": 786,
          "accuracy": 0.6725
        },
        "0.01": {
          "tp": 1146,
          "fn": 1254,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.90795234375
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1511,
          "fn": 889,
          "accuracy": 0.6295833333333334
        },
        "0.01": {
          "tp": 1046,
          "fn": 1354,
          "accuracy": 0.43583333333333335
        }
      },
      "auroc": 0.8873755208333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1511,
          "fn": 889,
          "accuracy": 0.6295833333333334
        },
        "0.01": {
          "tp": 1046,
          "fn": 1354,
          "accuracy": 0.43583333333333335
        }
      },
      "auroc": 0.8873755208333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3125,
          "fn": 1675,
          "accuracy": 0.6510416666666666
        },
        "0.01": {
          "tp": 2192,
          "fn": 2608,
          "accuracy": 0.45666666666666667
        }
      },
      "auroc": 0.8976639322916666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3125,
          "fn": 1675,
          "accuracy": 0.6510416666666666
        },
        "0.01": {
          "tp": 2192,
          "fn": 2608,
          "accuracy": 0.45666666666666667
        }
      },
      "auroc": 0.8976639322916666
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 20567,
          "fn": 5833,
          "accuracy": 0.7790530303030303
        },
        "0.01": {
          "tp": 15598,
          "fn": 10802,
          "accuracy": 0.5908333333333333
        }
      },
      "auroc": 0.9412923058712122
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 11469,
          "fn": 2931,
          "accuracy": 0.7964583333333334
        },
        "0.01": {
          "tp": 9769,
          "fn": 4631,
          "accuracy": 0.6784027777777778
        }
      },
      "auroc": 0.9381541956018519
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 32036,
          "fn": 8764,
          "accuracy": 0.7851960784313725
        },
        "0.01": {
          "tp": 25367,
          "fn": 15433,
          "accuracy": 0.6217401960784313
        }
      },
      "auroc": 0.9401847375408496
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 16952,
          "fn": 9448,
          "accuracy": 0.6421212121212121
        },
        "0.01": {
          "tp": 11807,
          "fn": 14593,
          "accuracy": 0.44723484848484846
        }
      },
      "auroc": 0.9027063999368687
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 10835,
          "fn": 3565,
          "accuracy": 0.7524305555555556
        },
        "0.01": {
          "tp": 9260,
          "fn": 5140,
          "accuracy": 0.6430555555555556
        }
      },
      "auroc": 0.9174458767361111
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 27787,
          "fn": 13013,
          "accuracy": 0.6810539215686274
        },
        "0.01": {
          "tp": 21067,
          "fn": 19733,
          "accuracy": 0.5163480392156863
        }
      },
      "auroc": 0.9079085682189543
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37519,
          "fn": 15281,
          "accuracy": 0.7105871212121212
        },
        "0.01": {
          "tp": 27405,
          "fn": 25395,
          "accuracy": 0.5190340909090909
        }
      },
      "auroc": 0.9219993529040403
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 22304,
          "fn": 6496,
          "accuracy": 0.7744444444444445
        },
        "0.01": {
          "tp": 19029,
          "fn": 9771,
          "accuracy": 0.6607291666666667
        }
      },
      "auroc": 0.9278000361689815
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 59823,
          "fn": 21777,
          "accuracy": 0.733125
        },
        "0.01": {
          "tp": 46434,
          "fn": 35166,
          "accuracy": 0.5690441176470589
        }
      },
      "auroc": 0.9240466528799021
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9474947428385416
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9435244791666666
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9455096110026041
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9472399739583334
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9309165690104166
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.939078271484375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9473673583984376
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9372205240885417
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9422939412434896
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9491723470052084
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.94061728515625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9448948160807291
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9112212565104167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9331882975260417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9222047770182291
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9301968017578126
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9369027913411458
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9335497965494791
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9481734375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9439841959635417
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9460788167317707
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9446885416666666
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9404024251302082
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9425454833984375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9464309895833334
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.942193310546875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9443121500651042
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9476603352864584
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9540227864583333
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9508415608723959
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9145688313802084
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9409238444010417
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.927746337890625
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9311145833333334
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9474733154296875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9392939493815105
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9466562174479166
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9391267415364584
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9428914794921875
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.856700439453125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9405069010416667
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8986036702473957
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9016783284505208
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9398168212890624
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9207475748697916
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9470365071614584
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9455030924479166
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9462697998046875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9470576334635417
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9343689453124999
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9407132893880208
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9470470703125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9399360188802084
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9434915445963542
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9282279947916667
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9282279947916667
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.91602919921875
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.91602919921875
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9221285970052084
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9221285970052084
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8646147135416666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8646147135416666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8034993977864583
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8034993977864583
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8340570556640625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8340570556640625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9450884928385417
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9450884928385417
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9375253580729166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9375253580729166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9413069254557291
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9413069254557291
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9463579427083334
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9463579427083334
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8604760579427083
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8604760579427083
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9034170003255207
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9034170003255207
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9151567708333334
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9151567708333334
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8756985188802083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8756985188802083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8954276448567708
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8954276448567708
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9350581365411932
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9444630967881944
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9383775342754289
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9013368371212122
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9367178304036459
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9138242465150123
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9181974868312027
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9405904635959201
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9261008903952206
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9474947428385416
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9435244791666666
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9455096110026041
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9472399739583334
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9309165690104166
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.939078271484375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9473673583984376
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9372205240885417
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9422939412434896
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9491723470052084
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.94061728515625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9448948160807291
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9112212565104167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9331882975260417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9222047770182291
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9301968017578126
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9369027913411458
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9335497965494791
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9481734375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9439841959635417
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9460788167317707
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9446885416666666
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9404024251302082
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9425454833984375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9464309895833334
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.942193310546875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9443121500651042
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9476603352864584
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9540227864583333
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9508415608723959
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9145688313802084
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9409238444010417
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.927746337890625
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9311145833333334
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9474733154296875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9392939493815105
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9466562174479166
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9391267415364584
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9428914794921875
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.856700439453125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9405069010416667
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8986036702473957
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9016783284505208
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9398168212890624
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9207475748697916
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9470365071614584
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9455030924479166
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9462697998046875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9470576334635417
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9343689453124999
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9407132893880208
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9470470703125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9399360188802084
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9434915445963542
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9282279947916667
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9282279947916667
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.91602919921875
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.91602919921875
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9221285970052084
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9221285970052084
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8646118977864583
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8646118977864583
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8034993977864583
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8034993977864583
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8340556477864582
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8340556477864582
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9450884928385417
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9450884928385417
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9375253580729166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9375253580729166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9413069254557291
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9413069254557291
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9463579427083334
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9463579427083334
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8604760579427083
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8604760579427083
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9034170003255207
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9034170003255207
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9151567708333334
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9151567708333334
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8756985188802083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8756985188802083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8954276448567708
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8954276448567708
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.935057880563447
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9444630967881944
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9383773686427697
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9013368371212122
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9367178304036459
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9138242465150123
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9181973588423297
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9405904635959201
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9261008075788909
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9603084798177083
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9449416341145833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.952625056966146
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9562929361979167
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.918959423828125
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9376261800130208
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9583007080078125
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9319505289713541
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9451256184895833
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9575683105468751
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.942497314453125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9500328124999999
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8501940917968749
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9349518880208334
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8925729899088541
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9038812011718751
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9387246012369791
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.921302901204427
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.950930419921875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9422868977864584
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9466086588541667
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9410529459635417
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9380386881510417
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9395458170572917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9459916829427083
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9401627929687499
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9430772379557292
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9519310221354167
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9649417317708333
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.958436376953125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8466594075520834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9452803059895833
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8959698567708334
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.89929521484375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9551110188802083
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9272031168619792
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.95193896484375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.93386650390625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9429027343749999
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.776139599609375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9416183430989583
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8588789713541666
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8640392822265626
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9377424235026042
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9008908528645834
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9590829264322918
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9534382649739583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.956260595703125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9505731933593751
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9314558268229166
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9410145100911458
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9548280598958334
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9424470458984375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9486375528971354
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.90525087890625
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.90525087890625
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8829880045572918
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8829880045572918
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8941194417317708
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8941194417317708
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7905323567708333
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7905323567708333
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7269129557291667
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7269129557291667
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.75872265625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.75872265625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9567694986979167
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9567694986979167
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.941347509765625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.941347509765625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9490585042317708
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9490585042317708
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9477550130208334
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9477550130208334
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7679355143229167
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7679355143229167
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.857845263671875
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.857845263671875
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8822312174479167
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8822312174479167
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8320576660156249
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8320576660156249
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8571444417317708
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8571444417317708
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9285726444128788
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9469953911675348
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9350747903262867
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.861104893169981
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9350507459852431
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8872034294577206
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8948387687914301
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.941023068576389
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9111391098920036
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9528962727864584
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9429468098958335
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9479215413411457
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.950957568359375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.920329541015625
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9356435546874999
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9519269205729166
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.931638175455729
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9417825480143229
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.951205126953125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9401755696614583
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9456903483072917
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8583404134114584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9334365397135416
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8958884765625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9047727701822916
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9368060546875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9207894124348959
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9426976888020833
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9336279296875001
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9381628092447917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9330212565104168
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9324942057291666
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9327577311197917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.93785947265625
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9330610677083333
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9354602701822916
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9471409830729167
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.95923779296875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9531893880208333
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8450729166666666
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9340472981770833
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8895601074218751
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8961069498697916
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9466425455729166
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9213747477213541
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9478915364583334
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9260745279947917
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9369830322265625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.783172607421875
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.935083056640625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.85912783203125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8655320719401043
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9305787923177083
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8980554321289063
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9501318522135417
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9456838378906249
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9479078450520833
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.943522265625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.922295654296875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9329089599609375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9468270589192709
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9339897460937501
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9404084025065104
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.89760791015625
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.89760791015625
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8737892252604167
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8737892252604167
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8856985677083333
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8856985677083333
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7990387532552083
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7990387532552083
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7312174153645834
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7312174153645834
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7651280843098958
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7651280843098958
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9453741048177082
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9453741048177082
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9342792154947918
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9342792154947918
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9398266601562499
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9398266601562499
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9433214192708332
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9433214192708332
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7956523600260417
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7956523600260417
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8694868896484376
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8694868896484376
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8765598632812499
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8765598632812499
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8316307779947916
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8316307779947916
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8540953206380208
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8540953206380208
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9230786828243371
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.941291078016493
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9295065870098038
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8618778201941288
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9296143825954861
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8857848422181372
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8924782515092329
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9354527303059896
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9076457146139706
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.949734130859375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9447011230468749
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.947217626953125
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9483241373697917
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9268078125
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9375659749348959
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9490291341145833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9357544677734375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9423918009440104
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9499039713541667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9405784667968751
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9452412190755208
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8828703776041666
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9333468098958333
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.90810859375
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9163871744791666
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9369626383463542
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9266749064127604
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.945822412109375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9405517903645834
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9431871012369792
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9403158203124999
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9375125162760416
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9389141682942708
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9430691162109375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9390321533203125
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9410506347656251
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9478450520833334
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.95705146484375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9524482584635416
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8797357747395833
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.940160009765625
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9099478922526041
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9137904134114583
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9486057373046876
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.931198075358073
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9476398274739584
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9311287272135418
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.93938427734375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8175927083333334
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9389151692708334
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8782539388020834
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8826162679036458
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9350219482421875
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9088191080729167
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9470464192708333
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9454380696614584
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9462422444661458
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9454185709635416
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.932099560546875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9387590657552083
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9462324951171874
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9387688151041667
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9425006551106769
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.913838720703125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.913838720703125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8981057291666668
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8981057291666668
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9059722249348958
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9059722249348958
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8298291666666666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8298291666666666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.76275791015625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.76275791015625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7962935384114583
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7962935384114583
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9456697102864583
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9456697102864583
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9373091145833334
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9373091145833334
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9414894124348958
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9414894124348958
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9466014160156249
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9466014160156249
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8312556640625
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8312556640625
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8889285400390625
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8889285400390625
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.896918115234375
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.896918115234375
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8524006998697917
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8524006998697917
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8746594075520834
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8746594075520834
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.929168085641572
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9432416069878472
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9341352108226104
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8814624097419508
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9348069797092013
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9002899050245099
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9053152476917614
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9390242933485242
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.91721255792356
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9503779785156251
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9426561360677084
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9465170572916667
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9492213053385417
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9211231119791667
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9351722086588542
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9497996419270833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9318896240234376
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9408446329752604
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9348054850260417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8496528157552083
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.892229150390625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8780608398437499
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7643284016927083
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8211946207682291
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9064331624348959
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8069906087239582
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8567118855794271
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9370860351562501
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8863392415364583
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9117126383463542
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9337846354166666
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8314796223958333
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.88263212890625
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9354353352864584
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8589094319661458
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8971723836263021
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9173789876302083
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9443559895833333
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9308674886067708
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8923825358072917
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8021166015625
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8472495686848959
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.90488076171875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8732362955729167
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8890585286458333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9346001790364584
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8840910644531249
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9093456217447917
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8515482910156249
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7214859700520834
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7865171305338542
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8930742350260418
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8027885172526041
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8479313761393229
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9469145182291667
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9413721516927084
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9441433349609375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9427395182291668
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9056606933593749
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9242001057942708
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9448270182291667
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9235164225260417
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9341717203776043
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8977642740885416
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8977642740885416
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8900814615885417
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8900814615885417
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8939228678385417
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8939228678385417
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.865169775390625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.865169775390625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8417174967447916
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8417174967447916
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8534436360677082
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8534436360677082
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9459590169270833
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9459590169270833
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9386484374999999
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9386484374999999
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9423037272135417
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9423037272135417
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9429255045572916
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9429255045572916
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.900573291015625
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.900573291015625
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9217493977864584
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9217493977864584
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9038265462239583
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9038265462239583
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8827410319010416
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8827410319010416
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8932837890625
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8932837890625
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9251643909801136
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9080778998480903
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.919133864698223
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9001362585819128
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8243657335069445
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8733937203201594
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9126503247810132
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8662218166775174
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8962637925091912
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.948504736328125
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9447135904947916
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9466091634114583
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9483004231770833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9317262044270834
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9400133138020834
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9484025797526041
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9382198974609376
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9433112386067708
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9498790201822916
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9407035319010417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9452912760416666
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9079360188802084
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9330007486979166
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9204683837890626
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.92890751953125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9368521402994792
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9328798299153646
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9481838704427084
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9441000651041668
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9461419677734375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9442984049479167
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9403492350260418
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9423238199869791
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9462411376953125
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9422246500651041
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9442328938802083
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.94500126953125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9539099772135416
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9494556233723959
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9111720703125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9408829264322918
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.926027498372396
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.928086669921875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9473964518229168
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9377415608723958
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.94318662109375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9383390950520833
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9407628580729166
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8513197265625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9405633789062501
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.895941552734375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8972531738281251
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9394512369791668
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9183522054036458
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9474364908854167
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9463582845052083
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9468973876953125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9471155924479168
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9337708658854167
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9404432291666667
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9472760416666666
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9400645751953125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9436703084309896
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9229585123697917
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9229585123697917
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.91100146484375
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.91100146484375
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9169799886067709
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9169799886067709
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8589986979166666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8589986979166666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7975141438802082
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7975141438802082
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8282564208984375
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8282564208984375
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9458671875
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9458671875
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9381361816406251
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9381361816406251
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9420016845703125
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9420016845703125
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9473591308593751
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9473591308593751
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8597107584635416
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8597107584635416
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9035349446614582
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9035349446614582
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.91138662109375
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.91138662109375
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8714957356770834
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8714957356770834
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8914411783854166
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8914411783854166
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9335238325639206
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9446874240451388
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9374639236749387
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8989091382575758
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9367155598958333
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9122525811887254
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9162164854107482
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9407014919704861
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9248582524318321
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9474947428385416
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9435244791666666
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9455096110026041
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9472399739583334
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9309119303385418
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9390759521484374
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9473673583984376
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9372182047526042
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9422927815755209
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.949167138671875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9404605631510417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9448138509114583
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9110707845052084
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9331059895833334
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9220883870442709
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9301189615885417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9367832763671875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9334511189778646
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9481734375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9440162434895834
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9460948404947916
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9446885416666666
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9404928710937499
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9425907063802083
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9464309895833334
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9422545572916665
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9443427734375001
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9476603352864584
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9540559082031249
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9508581217447917
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9144911295572917
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9412432291666668
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9278671793619792
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.931075732421875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9476495686848959
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9393626505533854
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9466562174479166
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9391313313802083
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9428937744140624
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8567774088541666
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9402965169270834
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.898536962890625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9017168131510417
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9397139241536459
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9207153686523437
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9470393229166667
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9455030924479166
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9462712076822917
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9470490071614582
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9343388346354167
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9406939208984375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9470441650390625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9399209635416667
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9434825642903645
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9282279947916667
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9282279947916667
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9160300618489583
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9160300618489583
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9221290283203125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9221290283203125
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.86459912109375
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.86459912109375
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8035145833333334
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8035145833333334
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8340568522135416
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8340568522135416
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9450884928385417
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9450884928385417
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9375253580729166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9375253580729166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9413069254557291
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9413069254557291
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9463579427083334
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9463579427083334
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8604760579427083
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8604760579427083
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9034170003255207
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9034170003255207
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9151644205729167
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9151644205729167
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8756694986979167
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8756694986979167
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8954169596354167
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8954169596354167
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.935057196969697
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9444486029730903
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9383718108532476
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9013211277817235
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9367315619574652
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9138189280790442
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9181891623757102
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9405900824652778
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9260953694661458
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7114545247395834
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.533827099609375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6226408121744792
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6951040852864583
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.4671075520833333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5811058186848959
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7032793050130208
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5004673258463542
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6018733154296876
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9361746419270833
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9457457845052083
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9409602132161459
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.60087626953125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9295632486979167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7652197591145833
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7685254557291668
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9376545166015625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8530899861653646
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.62874091796875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.522654150390625
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5756975341796875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5833013020833333
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6013030598958333
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5923021809895833
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6060211100260416
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5619786051432292
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5839998575846355
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9428012044270834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.67736962890625
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8100854166666667
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5779461751302084
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.40707535807291667
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.49251076660156257
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7603736897786458
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5422224934895834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6512980916341147
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.919788330078125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5691313639322917
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7444598470052083
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5365136555989583
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.605090673828125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5708021647135417
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7281509928385417
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5871110188802084
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6576310058593751
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7113215169270832
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.4898136555989583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6005675862630209
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6150152018229167
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.3757864420572917
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.49540082194010415
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.663168359375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.432800048828125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5479842041015626
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6997966796874999
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6997966796874999
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.643573876953125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.643573876953125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6716852783203124
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6716852783203124
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5423994303385417
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5423994303385417
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5047889811197916
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5047889811197916
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5235942057291667
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5235942057291667
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6036701009114582
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6036701009114582
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.540501953125
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.540501953125
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5720860270182292
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5720860270182292
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6248222330729167
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6248222330729167
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.4300702799479167
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.4300702799479167
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5274462565104167
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5274462565104167
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6073452311197917
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6073452311197917
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5663279947916666
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5663279947916666
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5868366129557292
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5868366129557292
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7207558919270833
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6230902804904515
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6862856761259191
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5721836159446022
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5643210557725694
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5694085947074142
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6464697539358428
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.5937056681315105
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6278471354166667
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9499624999999999
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9422231445312499
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.946092822265625
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9477036295572917
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9252847819010417
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9364942057291668
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9488330647786458
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9337539632161459
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9412935139973958
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.952972705078125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9406201009114583
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9467964029947917
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8364551757812501
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9331882975260417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8848217366536458
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8947139404296874
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.93690419921875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9158090698242187
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9446583984375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9420663574218751
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9433623779296875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9357490885416667
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9398916829427083
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9378203857421875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9402037434895832
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9409790201822916
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9405913818359375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.95043544921875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9559122558593749
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9531738525390625
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8223338053385417
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9398216959635417
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8810777506510417
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8863846272786459
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9478669759114584
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9171258015950521
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9493549479166665
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.93528232421875
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9423186360677083
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7520552897135417
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9410172526041667
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8465362711588542
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8507051188151042
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9381497884114582
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8944274536132814
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9495860026041667
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.94694599609375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9482659993489583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9428800455729167
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9323259765624999
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9376030110677083
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9462330240885417
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9396359863281251
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9429345052083334
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8929573893229168
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8929573893229168
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8707628255208333
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8707628255208333
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.881860107421875
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.881860107421875
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7599575846354167
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7599575846354167
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6869692057291666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.6869692057291666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7234633951822917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7234633951822917
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9441311848958333
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9441311848958333
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9323207356770833
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9323207356770833
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9382259602864584
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9382259602864584
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9333765950520834
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9333765950520834
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7548337565104166
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7548337565104166
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.84410517578125
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.84410517578125
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8656395182291666
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8656395182291666
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8104366048177083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8104366048177083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8380380615234375
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8380380615234375
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9175483886718749
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9438416965060764
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9268283796721813
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8447727420691288
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9352549479166667
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8767076382506128
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8811605653705019
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9395483222113715
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9017680089613971
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9486966471354167
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9446447916666666
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9466707194010416
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9485840657552083
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9303426595052083
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9394633626302084
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9486403564453125
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9374937255859375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9430670410156251
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.949362060546875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.940249658203125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.944805859375
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9047452636718749
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9333249186197918
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9190350911458334
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9270536621093751
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9367872884114584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9319204752604167
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.949052783203125
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9434784993489583
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9462656412760416
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9446181803385417
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9399188639322917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9422685221354167
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9468354817708333
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.941698681640625
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9442670817057291
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9476214680989584
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9551404134114584
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9513809407552083
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9073688639322917
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9409293294270834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9241490966796875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.927495166015625
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9480348714192708
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9377650187174478
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9470320963541667
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9387158203125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9428739583333333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8475220540364583
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.940429345703125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8939756998697916
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8972770751953124
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9395725830078124
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9184248291015624
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9478723958333335
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9466102864583333
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9472413411458334
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9475028971354167
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9336615234375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9405822102864583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.947687646484375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9401359049479165
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9439117757161459
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9257855305989584
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9257855305989584
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.913063818359375
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.913063818359375
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9194246744791668
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9194246744791668
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8564142578125
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8564142578125
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7934631673177084
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7934631673177084
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8249387125651042
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8249387125651042
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9462200520833334
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9462200520833334
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9380659993489583
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9380659993489583
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9421430257161458
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9421430257161458
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.947046484375
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.947046484375
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8465475748697917
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8465475748697917
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8967970296223958
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8967970296223958
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.91108291015625
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.91108291015625
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8700495279947916
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8700495279947916
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8905662190755208
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8905662190755208
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.934198789654356
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9448065782335069
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9379427150352329
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8965028557054924
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9364344401041667
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9105963560814951
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9153508226799243
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9406205091688368
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.924269535558364
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7542553873697917
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7525071289062499
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7533812581380208
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7531543294270833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7492766927083334
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7512155110677083
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7537048583984375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7508919108072917
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7522983846028646
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7677429850260417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.78175205078125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7747475179036459
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7488349283854167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7783876790364583
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7636113037109376
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7582889567057292
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7800698649088542
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7691794108072917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7393022135416667
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7490597005208334
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7441809570312501
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7411984537760417
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.754796728515625
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7479975911458334
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7402503336588542
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7519282145182292
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7460892740885418
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7921222819010416
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7563838216145833
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7742530517578126
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7570514322916666
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7600712890625001
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7585613606770832
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7745868570963542
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7582275553385416
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.766407206217448
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.762239501953125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7350032877604167
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7486213948567708
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7472000325520833
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7597572916666666
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.753478662109375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7547197672526041
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7473802897135418
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7510500284830728
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.744351708984375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7346675944010417
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7395096516927083
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7432122395833334
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7392421549479167
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.741227197265625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7437819742838541
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7369548746744792
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7403684244791667
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7284887369791666
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7284887369791666
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7287475423177083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7287475423177083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7286181396484375
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7286181396484375
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7321579264322917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7321579264322917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7346049641927083
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7346049641927083
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7333814453124999
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7333814453124999
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7368447265625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7368447265625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7351649739583334
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7351649739583334
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7360048502604167
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7360048502604167
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.74302490234375
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.74302490234375
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.741253955078125
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.741253955078125
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7421394287109374
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7421394287109374
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7420241048177083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7420241048177083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7392296549479167
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7392296549479167
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7406268798828125
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7406268798828125
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.749323134173769
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7515622639973958
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7501134152879902
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7426956824100379
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.75692197265625
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7477167260263481
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7460094082919034
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7542421183268229
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7489150706571691
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9140562405056424
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8936445746527777
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9038504075792101
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9116135335286458
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8736419040256077
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8926277187771268
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9128348870171441
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8836432393391926
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8982390631781685
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9330938449435764
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9203058688693577
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9266998569064671
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8501522230360242
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9060842597113714
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.878118241373698
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8916230339898004
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9131950642903647
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9024090491400825
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9025829210069445
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.886345772298177
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8944643466525608
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.894283809407552
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8864235270182292
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8903536682128905
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8984333652072483
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8863846496582033
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8924090074327257
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9321048936631945
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9155337131076389
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9238193033854167
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8486126478407117
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8694563110351563
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.859034479437934
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8903587707519531
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8924950120713975
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8914268914116754
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9286367214626736
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.884084794108073
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9063607577853733
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7944368543836806
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8787725667317708
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8366047105577257
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.861536787923177
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8814286804199218
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8714827341715495
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9120713473849826
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8905697848849827
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9013205661349826
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9015953165690104
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8674479519314235
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.884521634250217
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9068333319769967
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8790088684082032
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8929211001925998
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8807610514322917
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8807610514322917
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8633502007378472
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8633502007378472
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8720556260850695
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8720556260850695
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8023603068033855
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8023603068033855
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7492049682617188
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7492049682617188
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.775782637532552
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.775782637532552
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9004809217664931
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9004809217664931
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8873625162760417
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8873625162760417
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8939217190212674
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8939217190212674
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9012755438910591
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.9012755438910591
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7924384440104166
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.7924384440104166
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8468569939507378
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8468569939507378
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8618743408203126
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8618743408203126
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8236196858723958
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8236196858723958
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8427470133463542
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8427470133463542
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8972089212436869
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8984140846535011
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.897634273035386
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8469700181749131
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8803044200755932
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8587351011986826
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8720894697093001
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8893592523645472
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": null,
        "0.01": null
      },
      "auroc": 0.8781846871170343
    }
  ],
  "thresholds": {
    "0.05": {
      "abstracts": 0.9995340420750001,
      "books": 0.9999866659686769,
      "news": 0.9999859277911622,
      "poetry": 0.9999205114265624,
      "recipes": 0.9999877199568286,
      "reddit": 0.999980149655078,
      "reviews": 0.999984492794507,
      "wiki": 0.9999837188654297
    },
    "0.01": {
      "abstracts": 0.9999722554233887,
      "books": 0.9999876196429933,
      "news": 0.9999877199468341,
      "poetry": 0.9999873878380003,
      "recipes": 0.9999877199568286,
      "reddit": 0.9999875406310301,
      "reviews": 0.9999874797788344,
      "wiki": 0.9999872951441162
    }
  },
  "fpr": {
    "0.05": {
      "abstracts": 0.050000000000000044,
      "books": 0.050000000000000044,
      "news": 0.050000000000000044,
      "poetry": 0.050000000000000044,
      "recipes": 0.345,
      "reddit": 0.050000000000000044,
      "reviews": 0.050000000000000044,
      "wiki": 0.050000000000000044
    },
    "0.01": {
      "abstracts": 0.010000000000000009,
      "books": 0.010000000000000009,
      "news": 0.015000000000000013,
      "poetry": 0.010000000000000009,
      "recipes": 0.345,
      "reddit": 0.010000000000000009,
      "reviews": 0.020000000000000018,
      "wiki": 0.010000000000000009
    }
  }
}